{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Idea \n",
    "The proposed project will focus on utilizing deep learning to detect facial expressions in humans. While there have been many projects in the past that dealt with expression classification: (https://github.com/atulapra/Emotion-detection) and (https://tinyurl.com/1m4km78w), one motif we noticed throughout examining these projects is that the training accuracy on the expression classifier is often much higher than the validation accuracy. In other words, it may be the case that these models have a hard time generalizing expressions among different faces. This is perhaps due to the large variety of different faces present in the dataset.\n",
    "\n",
    "While we will not attempt to improve the ability of deep learning models to generalize facial expression in this project, we will attempt to improve model performance for specific users. The overall idea is to train a model jointly, using both a general dataset of facial expressions as well as a dataset of a particular user’s facial expressions.\n",
    "\n",
    "## Pipeline Overview\n",
    "There will be three main parts to the pipline we want to create. Facial detection, general expression classification, and specific expression classification. Images containing (or not) human faces will be fed into the pipeline. Now, the first step in the pipeline will be to detect faces (or lack thereof). The most convenient way to do this is to use a pre-trained model provided by OpenCV. The facial detection step of the pipeline will output cropped images of faces, which is then fed into the next (and final) step of the pipeline to perform expression classification. Now, to train the expression classification step of the pipeline, we will first train a general CNN (same architecture as here: https://github.com/atulapra/Emotion-detection) with a general facial expression dataset (FER-13). Next, we will perform transfer learning on a specific CNN. We will freeze the convolutional layers of the previously trained general CNN and use that as the convolutional layers of the specific CNN. The fully-connected layers of the original CNN will then be trained by passing a specific dataset of user’s facial expressions.\n",
    "\n",
    "## Dataset\n",
    "The general dataset we are using is FER-13 with 7 classes (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set contains 28,709 examples. The public test set contains 3,589 examples, and the private test set contains another 3,589 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import pipeline\n",
    "from src.pipeline import Pipeline\n",
    "p = Pipeline()\n",
    "\n",
    "p.ingest_fer13_data(\"data/icml_face_data.csv\")\n",
    "p.ingest_specific_data(\"data/specific_dataset\", train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Rundown\n",
    "## General model\n",
    "The general model architecture is given by this GitHub repo - https://github.com/atulapra/Emotion-detection. Using that model architecture, we first train the general model using the FER13 face data. Note that we are now training our general model and this may take quite some time if you're not using a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 loss: 1.9499114751815796, accuracy: 0.078125\n",
      "Batch 1 loss: 1.8471424579620361, accuracy: 0.1796875\n",
      "Batch 2 loss: 2.2261767387390137, accuracy: 0.21875\n",
      "Batch 3 loss: 1.8242533206939697, accuracy: 0.21875\n",
      "Batch 4 loss: 1.8614542484283447, accuracy: 0.20937499403953552\n",
      "Batch 5 loss: 1.8961968421936035, accuracy: 0.1953125\n",
      "Batch 6 loss: 1.901938557624817, accuracy: 0.1919642835855484\n",
      "Batch 7 loss: 1.9291903972625732, accuracy: 0.185546875\n",
      "Batch 8 loss: 1.8992499113082886, accuracy: 0.1822916716337204\n",
      "Batch 9 loss: 1.9148668050765991, accuracy: 0.17812499403953552\n",
      "Batch 10 loss: 1.890213131904602, accuracy: 0.17684659361839294\n",
      "Batch 11 loss: 1.8897367715835571, accuracy: 0.1783854216337204\n",
      "Batch 12 loss: 1.870928168296814, accuracy: 0.17908653616905212\n",
      "Batch 13 loss: 1.874949336051941, accuracy: 0.17578125\n",
      "Batch 14 loss: 1.883000135421753, accuracy: 0.17343750596046448\n",
      "Batch 15 loss: 1.842260479927063, accuracy: 0.1748046875\n",
      "Batch 16 loss: 1.8391897678375244, accuracy: 0.1741727888584137\n",
      "Batch 17 loss: 1.7919845581054688, accuracy: 0.1766493022441864\n",
      "Batch 18 loss: 1.8439984321594238, accuracy: 0.1772203892469406\n",
      "Batch 19 loss: 1.8008756637573242, accuracy: 0.17812499403953552\n",
      "Batch 20 loss: 1.802476167678833, accuracy: 0.181547611951828\n",
      "Batch 21 loss: 1.796366572380066, accuracy: 0.18607954680919647\n",
      "Batch 22 loss: 1.812407374382019, accuracy: 0.18716032803058624\n",
      "Batch 23 loss: 1.7446627616882324, accuracy: 0.1943359375\n",
      "Batch 24 loss: 1.8164349794387817, accuracy: 0.19687500596046448\n",
      "Batch 25 loss: 1.7951513528823853, accuracy: 0.19801682233810425\n",
      "Batch 26 loss: 1.7537528276443481, accuracy: 0.19994212687015533\n",
      "Batch 27 loss: 1.8062235116958618, accuracy: 0.2014508992433548\n",
      "Batch 28 loss: 1.8045297861099243, accuracy: 0.20150862634181976\n",
      "Batch 29 loss: 1.8988454341888428, accuracy: 0.2005208283662796\n",
      "Batch 30 loss: 1.8031742572784424, accuracy: 0.20337702333927155\n",
      "Batch 31 loss: 1.8130451440811157, accuracy: 0.203125\n",
      "Batch 32 loss: 1.7919012308120728, accuracy: 0.2052556872367859\n",
      "Batch 33 loss: 1.8049349784851074, accuracy: 0.20818014442920685\n",
      "Batch 34 loss: 1.7849071025848389, accuracy: 0.20781250298023224\n",
      "Batch 35 loss: 1.8760240077972412, accuracy: 0.2078993022441864\n",
      "Batch 36 loss: 1.7723777294158936, accuracy: 0.20882602035999298\n",
      "Batch 37 loss: 1.8227986097335815, accuracy: 0.2094983607530594\n",
      "Batch 38 loss: 1.8478636741638184, accuracy: 0.20993590354919434\n",
      "Batch 39 loss: 1.8146857023239136, accuracy: 0.2119140625\n",
      "Batch 40 loss: 1.7606815099716187, accuracy: 0.21341463923454285\n",
      "Batch 41 loss: 1.8107229471206665, accuracy: 0.21484375\n",
      "Batch 42 loss: 1.7677806615829468, accuracy: 0.21711482107639313\n",
      "Batch 43 loss: 1.7675901651382446, accuracy: 0.21803976595401764\n",
      "Batch 44 loss: 1.826599359512329, accuracy: 0.21770833432674408\n",
      "Batch 45 loss: 1.8053665161132812, accuracy: 0.21773098409175873\n",
      "Batch 46 loss: 1.7604961395263672, accuracy: 0.2192486673593521\n",
      "Batch 47 loss: 1.7873866558074951, accuracy: 0.2197265625\n",
      "Batch 48 loss: 1.824334740638733, accuracy: 0.2205038219690323\n",
      "Batch 49 loss: 1.7266818284988403, accuracy: 0.22218750417232513\n",
      "Batch 50 loss: 1.7897711992263794, accuracy: 0.22288602590560913\n",
      "Batch 51 loss: 1.829123854637146, accuracy: 0.2231069654226303\n",
      "Batch 52 loss: 1.7576444149017334, accuracy: 0.22346697747707367\n",
      "Batch 53 loss: 1.7459057569503784, accuracy: 0.22424767911434174\n",
      "Batch 54 loss: 1.754598617553711, accuracy: 0.2244318127632141\n",
      "Batch 55 loss: 1.7899062633514404, accuracy: 0.224609375\n",
      "Batch 56 loss: 1.7540091276168823, accuracy: 0.22491776943206787\n",
      "Batch 57 loss: 1.8097132444381714, accuracy: 0.22629310190677643\n",
      "Batch 58 loss: 1.74448823928833, accuracy: 0.22576801478862762\n",
      "Batch 59 loss: 1.7757232189178467, accuracy: 0.22643229365348816\n",
      "Batch 60 loss: 1.750015377998352, accuracy: 0.22707480192184448\n",
      "Batch 61 loss: 1.7436978816986084, accuracy: 0.22769656777381897\n",
      "Batch 62 loss: 1.8505252599716187, accuracy: 0.2276785671710968\n",
      "Batch 63 loss: 1.817948818206787, accuracy: 0.2269287109375\n",
      "Batch 64 loss: 1.7876046895980835, accuracy: 0.22728365659713745\n",
      "Batch 65 loss: 1.789544939994812, accuracy: 0.2286931872367859\n",
      "Batch 66 loss: 1.7661402225494385, accuracy: 0.22912779450416565\n",
      "Batch 67 loss: 1.7763869762420654, accuracy: 0.22874540090560913\n",
      "Batch 68 loss: 1.8245782852172852, accuracy: 0.2289402186870575\n",
      "Batch 69 loss: 1.7253332138061523, accuracy: 0.23002232611179352\n",
      "Batch 70 loss: 1.7859629392623901, accuracy: 0.23041373491287231\n",
      "Batch 71 loss: 1.746915340423584, accuracy: 0.2305772602558136\n",
      "Batch 72 loss: 1.7512202262878418, accuracy: 0.23223458230495453\n",
      "Batch 73 loss: 1.8360627889633179, accuracy: 0.23279137909412384\n",
      "Batch 74 loss: 1.8172727823257446, accuracy: 0.23281249403953552\n",
      "Batch 75 loss: 1.860784649848938, accuracy: 0.23221628367900848\n",
      "Batch 76 loss: 1.7581918239593506, accuracy: 0.23285308480262756\n",
      "Batch 77 loss: 1.7527121305465698, accuracy: 0.23397435247898102\n",
      "Batch 78 loss: 1.80165433883667, accuracy: 0.23496834933757782\n",
      "Batch 79 loss: 1.8008582592010498, accuracy: 0.23505859076976776\n",
      "Batch 80 loss: 1.7829245328903198, accuracy: 0.23533950746059418\n",
      "Batch 81 loss: 1.819654107093811, accuracy: 0.23532775044441223\n",
      "Batch 82 loss: 1.791446566581726, accuracy: 0.23569276928901672\n",
      "Batch 83 loss: 1.7671419382095337, accuracy: 0.2358630895614624\n",
      "Batch 84 loss: 1.759356141090393, accuracy: 0.2363051474094391\n",
      "Batch 85 loss: 1.7771902084350586, accuracy: 0.23610101640224457\n",
      "Batch 86 loss: 1.7927758693695068, accuracy: 0.2359015792608261\n",
      "Batch 87 loss: 1.734579086303711, accuracy: 0.23668323457241058\n",
      "Batch 88 loss: 1.7877298593521118, accuracy: 0.2360428422689438\n",
      "Batch 89 loss: 1.7782424688339233, accuracy: 0.23628471791744232\n",
      "Batch 90 loss: 1.8179682493209839, accuracy: 0.23592032492160797\n",
      "Batch 91 loss: 1.758925199508667, accuracy: 0.23598845303058624\n",
      "Batch 92 loss: 1.789818286895752, accuracy: 0.2359711080789566\n",
      "Batch 93 loss: 1.7477012872695923, accuracy: 0.23562167584896088\n",
      "Batch 94 loss: 1.8058828115463257, accuracy: 0.23626644909381866\n",
      "Batch 95 loss: 1.7777491807937622, accuracy: 0.2364095002412796\n",
      "Batch 96 loss: 1.7396645545959473, accuracy: 0.2366301566362381\n",
      "Batch 97 loss: 1.7590805292129517, accuracy: 0.2365274280309677\n",
      "Batch 98 loss: 1.6970196962356567, accuracy: 0.23721590638160706\n",
      "Batch 99 loss: 1.8317062854766846, accuracy: 0.23726563155651093\n",
      "Batch 100 loss: 1.7592495679855347, accuracy: 0.2379331737756729\n",
      "Batch 101 loss: 1.8654155731201172, accuracy: 0.23743872344493866\n",
      "Batch 102 loss: 1.7744202613830566, accuracy: 0.23771238327026367\n",
      "Batch 103 loss: 1.7444536685943604, accuracy: 0.2378305345773697\n",
      "Batch 104 loss: 1.777688980102539, accuracy: 0.23876488208770752\n",
      "Batch 105 loss: 1.839806318283081, accuracy: 0.23828125\n",
      "Batch 106 loss: 1.8100265264511108, accuracy: 0.23860980570316315\n",
      "Batch 107 loss: 1.7960206270217896, accuracy: 0.2387152761220932\n",
      "Batch 108 loss: 1.816247820854187, accuracy: 0.23846043646335602\n",
      "Batch 109 loss: 1.75738525390625, accuracy: 0.23849432170391083\n",
      "Batch 110 loss: 1.7810615301132202, accuracy: 0.23859797418117523\n",
      "Batch 111 loss: 1.771240472793579, accuracy: 0.23876953125\n",
      "Batch 112 loss: 1.7612557411193848, accuracy: 0.23859237134456635\n",
      "Batch 113 loss: 1.87598717212677, accuracy: 0.23862390220165253\n",
      "Batch 114 loss: 1.8150100708007812, accuracy: 0.23797553777694702\n",
      "Batch 115 loss: 1.7732242345809937, accuracy: 0.23848329484462738\n",
      "Batch 116 loss: 1.7973495721817017, accuracy: 0.2382478564977646\n",
      "Batch 117 loss: 1.766348958015442, accuracy: 0.23861229419708252\n",
      "Batch 118 loss: 1.745862603187561, accuracy: 0.23910188674926758\n",
      "Batch 119 loss: 1.8078768253326416, accuracy: 0.23964843153953552\n",
      "Batch 120 loss: 1.7648967504501343, accuracy: 0.23966942727565765\n",
      "Batch 121 loss: 1.6812690496444702, accuracy: 0.24077868461608887\n",
      "Batch 122 loss: 1.7852091789245605, accuracy: 0.24085365235805511\n",
      "Batch 123 loss: 1.7568899393081665, accuracy: 0.24086441099643707\n",
      "Batch 124 loss: 1.8160282373428345, accuracy: 0.2406875044107437\n",
      "Batch 125 loss: 1.741121530532837, accuracy: 0.2408854216337204\n",
      "Batch 126 loss: 1.836464285850525, accuracy: 0.24126476049423218\n",
      "Batch 127 loss: 1.7989544868469238, accuracy: 0.24188232421875\n",
      "Batch 128 loss: 1.8091011047363281, accuracy: 0.24194525182247162\n",
      "Batch 129 loss: 1.8462263345718384, accuracy: 0.24146634340286255\n",
      "Batch 130 loss: 1.8620625734329224, accuracy: 0.24153149127960205\n",
      "Batch 131 loss: 1.814033031463623, accuracy: 0.24165482819080353\n",
      "Batch 132 loss: 1.772976040840149, accuracy: 0.24160009622573853\n",
      "Batch 133 loss: 1.787506103515625, accuracy: 0.24183768033981323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 134 loss: 1.8419195413589478, accuracy: 0.24172453582286835\n",
      "Batch 135 loss: 1.7769129276275635, accuracy: 0.2418428361415863\n",
      "Batch 136 loss: 1.7084155082702637, accuracy: 0.24235858023166656\n",
      "Batch 137 loss: 1.8019944429397583, accuracy: 0.24224410951137543\n",
      "Batch 138 loss: 1.7633687257766724, accuracy: 0.24280574917793274\n",
      "Batch 139 loss: 1.7692217826843262, accuracy: 0.24313616752624512\n",
      "Batch 140 loss: 1.891778588294983, accuracy: 0.24296320974826813\n",
      "Batch 141 loss: 1.8152902126312256, accuracy: 0.2429027259349823\n",
      "Batch 142 loss: 1.7742040157318115, accuracy: 0.24338942766189575\n",
      "Batch 143 loss: 1.7503262758255005, accuracy: 0.2434353232383728\n",
      "Batch 144 loss: 1.786777138710022, accuracy: 0.2432650923728943\n",
      "Batch 145 loss: 1.7841728925704956, accuracy: 0.2436322718858719\n",
      "Batch 146 loss: 1.7895101308822632, accuracy: 0.24362245202064514\n",
      "Batch 147 loss: 1.7913539409637451, accuracy: 0.24350717663764954\n",
      "Batch 148 loss: 1.8154687881469727, accuracy: 0.2433934509754181\n",
      "Batch 149 loss: 1.7580478191375732, accuracy: 0.2434374988079071\n",
      "Batch 150 loss: 1.7619361877441406, accuracy: 0.2437913864850998\n",
      "Batch 151 loss: 1.733124017715454, accuracy: 0.24424342811107635\n",
      "Batch 152 loss: 1.8031035661697388, accuracy: 0.24397467076778412\n",
      "Batch 153 loss: 1.7551876306533813, accuracy: 0.24416598677635193\n",
      "Batch 154 loss: 1.748747706413269, accuracy: 0.24450604617595673\n",
      "Batch 155 loss: 1.7545994520187378, accuracy: 0.24439102411270142\n",
      "Batch 156 loss: 1.8043935298919678, accuracy: 0.24442675709724426\n",
      "Batch 157 loss: 1.8349875211715698, accuracy: 0.24401700496673584\n",
      "Batch 158 loss: 1.7093634605407715, accuracy: 0.24459512531757355\n",
      "Batch 159 loss: 1.749047040939331, accuracy: 0.24477538466453552\n",
      "Batch 160 loss: 1.7473394870758057, accuracy: 0.24485637247562408\n",
      "Batch 161 loss: 1.6903789043426514, accuracy: 0.24537037312984467\n",
      "Batch 162 loss: 1.7169923782348633, accuracy: 0.2456384152173996\n",
      "Batch 163 loss: 1.8476122617721558, accuracy: 0.24566501379013062\n",
      "Batch 164 loss: 1.794072151184082, accuracy: 0.24569128453731537\n",
      "Batch 165 loss: 1.712414026260376, accuracy: 0.24552899599075317\n",
      "Batch 166 loss: 1.7263221740722656, accuracy: 0.245930016040802\n",
      "Batch 167 loss: 1.7913566827774048, accuracy: 0.2458612322807312\n",
      "Batch 168 loss: 1.7284296751022339, accuracy: 0.24616308510303497\n",
      "Batch 169 loss: 1.7044665813446045, accuracy: 0.24692095816135406\n",
      "Batch 170 loss: 1.7892897129058838, accuracy: 0.24712170660495758\n",
      "Batch 171 loss: 1.7488988637924194, accuracy: 0.24750182032585144\n",
      "Batch 172 loss: 1.7904118299484253, accuracy: 0.2476968914270401\n",
      "Batch 173 loss: 1.7733632326126099, accuracy: 0.24779993295669556\n",
      "Batch 174 loss: 1.8044054508209229, accuracy: 0.2481696456670761\n",
      "Batch 175 loss: 1.768976092338562, accuracy: 0.2483575940132141\n",
      "Batch 176 loss: 1.7117769718170166, accuracy: 0.24863171577453613\n",
      "Batch 177 loss: 1.7583421468734741, accuracy: 0.24872717261314392\n",
      "Batch 178 loss: 1.7210801839828491, accuracy: 0.24882157146930695\n",
      "Batch 179 loss: 1.681488037109375, accuracy: 0.24908854067325592\n",
      "Batch 180 loss: 1.8075505495071411, accuracy: 0.2487482726573944\n",
      "Batch 181 loss: 1.8035959005355835, accuracy: 0.24884100258350372\n",
      "Batch 182 loss: 1.7160037755966187, accuracy: 0.24880464375019073\n",
      "Batch 183 loss: 1.646752953529358, accuracy: 0.2494480311870575\n",
      "Batch 184 loss: 1.7434159517288208, accuracy: 0.24974662065505981\n",
      "Batch 185 loss: 1.7793763875961304, accuracy: 0.2499159872531891\n",
      "Batch 186 loss: 1.7458205223083496, accuracy: 0.25004178285598755\n",
      "Batch 187 loss: 1.719539999961853, accuracy: 0.2503739893436432\n",
      "Batch 188 loss: 1.7603950500488281, accuracy: 0.25070270895957947\n",
      "Batch 189 loss: 1.8240069150924683, accuracy: 0.2508634924888611\n",
      "Batch 190 loss: 1.6475194692611694, accuracy: 0.2514316141605377\n",
      "Batch 191 loss: 1.808061122894287, accuracy: 0.25146484375\n",
      "Batch 192 loss: 1.7728792428970337, accuracy: 0.25137630105018616\n",
      "Batch 193 loss: 1.7695395946502686, accuracy: 0.25173163414001465\n",
      "Batch 194 loss: 1.7777289152145386, accuracy: 0.25208333134651184\n",
      "Batch 195 loss: 1.7299445867538452, accuracy: 0.25191327929496765\n",
      "Batch 196 loss: 1.689212441444397, accuracy: 0.2522604763507843\n",
      "Batch 197 loss: 1.7739830017089844, accuracy: 0.2522490620613098\n",
      "Batch 198 loss: 1.7310230731964111, accuracy: 0.25243404507637024\n",
      "Batch 199 loss: 1.7883448600769043, accuracy: 0.2522265613079071\n",
      "Batch 200 loss: 1.7124353647232056, accuracy: 0.2522543668746948\n",
      "Batch 201 loss: 1.7653796672821045, accuracy: 0.2523205578327179\n",
      "Batch 202 loss: 1.7276567220687866, accuracy: 0.2525784969329834\n",
      "Batch 203 loss: 1.7378169298171997, accuracy: 0.252565860748291\n",
      "Batch 204 loss: 1.8105437755584717, accuracy: 0.2525533437728882\n",
      "Batch 205 loss: 1.6976416110992432, accuracy: 0.2530719041824341\n",
      "Batch 206 loss: 1.7093470096588135, accuracy: 0.25335898995399475\n",
      "Batch 207 loss: 1.7565817832946777, accuracy: 0.25356820225715637\n",
      "Batch 208 loss: 1.744614601135254, accuracy: 0.25340160727500916\n",
      "Batch 209 loss: 1.8236620426177979, accuracy: 0.25338542461395264\n",
      "Batch 210 loss: 1.802625298500061, accuracy: 0.25388774275779724\n",
      "Batch 211 loss: 1.63858962059021, accuracy: 0.2544221580028534\n",
      "Batch 212 loss: 1.7231286764144897, accuracy: 0.2544380724430084\n",
      "Batch 213 loss: 1.7048819065093994, accuracy: 0.254709392786026\n",
      "Batch 214 loss: 1.7464455366134644, accuracy: 0.2550508677959442\n",
      "Batch 215 loss: 1.6556658744812012, accuracy: 0.2556423544883728\n",
      "Batch 216 loss: 1.766110897064209, accuracy: 0.25565236806869507\n",
      "Batch 217 loss: 1.669824242591858, accuracy: 0.2556622624397278\n",
      "Batch 218 loss: 1.800504207611084, accuracy: 0.25549373030662537\n",
      "Batch 219 loss: 1.721137523651123, accuracy: 0.255752831697464\n",
      "Batch 220 loss: 1.7896194458007812, accuracy: 0.2556561231613159\n",
      "Batch 221 loss: 1.709365725517273, accuracy: 0.2559473514556885\n",
      "Batch 222 loss: 1.723412275314331, accuracy: 0.25588566064834595\n",
      "Batch 223 loss: 1.6391407251358032, accuracy: 0.2562081515789032\n",
      "Batch 224 loss: 1.8038233518600464, accuracy: 0.2561565935611725\n",
      "Training epoch: 1, train accuracy: 25.615659713745117, train loss: 1.783966941833496, valid accuracy: 31.345779418945312, valid loss: 1.6954665636194164 \n",
      "Batch 0 loss: 1.7145339250564575, accuracy: 0.328125\n",
      "Batch 1 loss: 1.8138558864593506, accuracy: 0.30078125\n",
      "Batch 2 loss: 1.7317862510681152, accuracy: 0.2942708432674408\n",
      "Batch 3 loss: 1.7782152891159058, accuracy: 0.28125\n",
      "Batch 4 loss: 1.728879690170288, accuracy: 0.2890625\n",
      "Batch 5 loss: 1.69382643699646, accuracy: 0.2877604067325592\n",
      "Batch 6 loss: 1.6436524391174316, accuracy: 0.2901785671710968\n",
      "Batch 7 loss: 1.6437596082687378, accuracy: 0.291015625\n",
      "Batch 8 loss: 1.7644351720809937, accuracy: 0.2960069477558136\n",
      "Batch 9 loss: 1.7358243465423584, accuracy: 0.2984375059604645\n",
      "Batch 10 loss: 1.6843535900115967, accuracy: 0.3046875\n",
      "Batch 11 loss: 1.7038861513137817, accuracy: 0.3040364682674408\n",
      "Batch 12 loss: 1.6809836626052856, accuracy: 0.30288460850715637\n",
      "Batch 13 loss: 1.7467930316925049, accuracy: 0.3030133843421936\n",
      "Batch 14 loss: 1.7923847436904907, accuracy: 0.30260416865348816\n",
      "Batch 15 loss: 1.6630831956863403, accuracy: 0.30712890625\n",
      "Batch 16 loss: 1.6968803405761719, accuracy: 0.30514705181121826\n",
      "Batch 17 loss: 1.6327707767486572, accuracy: 0.30859375\n",
      "Batch 18 loss: 1.7224570512771606, accuracy: 0.30633223056793213\n",
      "Batch 19 loss: 1.6175097227096558, accuracy: 0.30585938692092896\n",
      "Batch 20 loss: 1.8151663541793823, accuracy: 0.3061755895614624\n",
      "Batch 21 loss: 1.7659167051315308, accuracy: 0.3061079680919647\n",
      "Batch 22 loss: 1.818534255027771, accuracy: 0.30298912525177\n",
      "Batch 23 loss: 1.67409348487854, accuracy: 0.3017578125\n",
      "Batch 24 loss: 1.6911994218826294, accuracy: 0.3018749952316284\n",
      "Batch 25 loss: 1.7080020904541016, accuracy: 0.3022836446762085\n",
      "Batch 26 loss: 1.7299473285675049, accuracy: 0.30324074625968933\n",
      "Batch 27 loss: 1.716686487197876, accuracy: 0.3060825765132904\n",
      "Batch 28 loss: 1.7443244457244873, accuracy: 0.30334052443504333\n",
      "Batch 29 loss: 1.7286384105682373, accuracy: 0.30286458134651184\n",
      "Batch 30 loss: 1.6703615188598633, accuracy: 0.30141130089759827\n",
      "Batch 31 loss: 1.715947151184082, accuracy: 0.299560546875\n",
      "Batch 32 loss: 1.641967535018921, accuracy: 0.2999526560306549\n",
      "Batch 33 loss: 1.6437783241271973, accuracy: 0.30193015933036804\n",
      "Batch 34 loss: 1.6812968254089355, accuracy: 0.3042410612106323\n",
      "Batch 35 loss: 1.651587724685669, accuracy: 0.3053385317325592\n",
      "Batch 36 loss: 1.7194503545761108, accuracy: 0.3059543967247009\n",
      "Batch 37 loss: 1.742299199104309, accuracy: 0.30386513471603394\n",
      "Batch 38 loss: 1.6428343057632446, accuracy: 0.30548879504203796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39 loss: 1.6860020160675049, accuracy: 0.3070312440395355\n",
      "Batch 40 loss: 1.6520559787750244, accuracy: 0.3075457215309143\n",
      "Batch 41 loss: 1.7321512699127197, accuracy: 0.3074776828289032\n",
      "Batch 42 loss: 1.6521397829055786, accuracy: 0.3088662922382355\n",
      "Batch 43 loss: 1.7048146724700928, accuracy: 0.3087713122367859\n",
      "Batch 44 loss: 1.6959437131881714, accuracy: 0.3092013895511627\n",
      "Batch 45 loss: 1.683875322341919, accuracy: 0.3087635934352875\n",
      "Batch 46 loss: 1.6581319570541382, accuracy: 0.3086768686771393\n",
      "Batch 47 loss: 1.6361500024795532, accuracy: 0.3092447817325592\n",
      "Batch 48 loss: 1.5753625631332397, accuracy: 0.3102678656578064\n",
      "Batch 49 loss: 1.7863444089889526, accuracy: 0.3109374940395355\n",
      "Batch 50 loss: 1.7448548078536987, accuracy: 0.31004902720451355\n",
      "Batch 51 loss: 1.642702341079712, accuracy: 0.31084734201431274\n",
      "Batch 52 loss: 1.6970164775848389, accuracy: 0.3105837404727936\n",
      "Batch 53 loss: 1.6319572925567627, accuracy: 0.3111979067325592\n",
      "Batch 54 loss: 1.608675479888916, accuracy: 0.31207385659217834\n",
      "Batch 55 loss: 1.654826283454895, accuracy: 0.3126395046710968\n",
      "Batch 56 loss: 1.6593494415283203, accuracy: 0.31304824352264404\n",
      "Batch 57 loss: 1.6676433086395264, accuracy: 0.3143857717514038\n",
      "Batch 58 loss: 1.7729524374008179, accuracy: 0.31408897042274475\n",
      "Batch 59 loss: 1.7217391729354858, accuracy: 0.3135416805744171\n",
      "Batch 60 loss: 1.6865651607513428, accuracy: 0.31275615096092224\n",
      "Batch 61 loss: 1.697820782661438, accuracy: 0.3128780126571655\n",
      "Batch 62 loss: 1.8181257247924805, accuracy: 0.3110119104385376\n",
      "Batch 63 loss: 1.676845908164978, accuracy: 0.311279296875\n",
      "Batch 64 loss: 1.662980079650879, accuracy: 0.31177884340286255\n",
      "Batch 65 loss: 1.626299262046814, accuracy: 0.31273674964904785\n",
      "Batch 66 loss: 1.6533797979354858, accuracy: 0.3133162260055542\n",
      "Batch 67 loss: 1.5838706493377686, accuracy: 0.3138786852359772\n",
      "Batch 68 loss: 1.7616409063339233, accuracy: 0.3134058117866516\n",
      "Batch 69 loss: 1.6723320484161377, accuracy: 0.3136160671710968\n",
      "Batch 70 loss: 1.7170101404190063, accuracy: 0.31382042169570923\n",
      "Batch 71 loss: 1.6318159103393555, accuracy: 0.3133680522441864\n",
      "Batch 72 loss: 1.6964627504348755, accuracy: 0.31357020139694214\n",
      "Batch 73 loss: 1.7925066947937012, accuracy: 0.3137668967247009\n",
      "Batch 74 loss: 1.8061810731887817, accuracy: 0.3122916519641876\n",
      "Batch 75 loss: 1.6015708446502686, accuracy: 0.3134251534938812\n",
      "Batch 76 loss: 1.7227652072906494, accuracy: 0.3135146200656891\n",
      "Batch 77 loss: 1.6675759553909302, accuracy: 0.3136017620563507\n",
      "Batch 78 loss: 1.7282403707504272, accuracy: 0.31329113245010376\n",
      "Batch 79 loss: 1.6083476543426514, accuracy: 0.3135742247104645\n",
      "Batch 80 loss: 1.6254135370254517, accuracy: 0.3143325746059418\n",
      "Batch 81 loss: 1.7545033693313599, accuracy: 0.31373855471611023\n",
      "Batch 82 loss: 1.638393521308899, accuracy: 0.31410014629364014\n",
      "Batch 83 loss: 1.6087051630020142, accuracy: 0.3147321343421936\n",
      "Batch 84 loss: 1.5566630363464355, accuracy: 0.31663602590560913\n",
      "Batch 85 loss: 1.6525992155075073, accuracy: 0.31704214215278625\n",
      "Batch 86 loss: 1.7369616031646729, accuracy: 0.3170797526836395\n",
      "Batch 87 loss: 1.6408747434616089, accuracy: 0.3173828125\n",
      "Batch 88 loss: 1.633573055267334, accuracy: 0.31838130950927734\n",
      "Batch 89 loss: 1.6001067161560059, accuracy: 0.3198784589767456\n",
      "Batch 90 loss: 1.6951258182525635, accuracy: 0.3199690878391266\n",
      "Batch 91 loss: 1.6082953214645386, accuracy: 0.3209918439388275\n",
      "Batch 92 loss: 1.6417639255523682, accuracy: 0.32106855511665344\n",
      "Batch 93 loss: 1.668043851852417, accuracy: 0.32114362716674805\n",
      "Batch 94 loss: 1.6183942556381226, accuracy: 0.32179275155067444\n",
      "Batch 95 loss: 1.6306322813034058, accuracy: 0.3219400942325592\n",
      "Batch 96 loss: 1.705464482307434, accuracy: 0.3215206265449524\n",
      "Batch 97 loss: 1.7028851509094238, accuracy: 0.3214285671710968\n",
      "Batch 98 loss: 1.689246654510498, accuracy: 0.3216540515422821\n",
      "Batch 99 loss: 1.6200910806655884, accuracy: 0.32249999046325684\n",
      "Batch 100 loss: 1.6228924989700317, accuracy: 0.3227877616882324\n",
      "Batch 101 loss: 1.6297430992126465, accuracy: 0.32314643263816833\n",
      "Batch 102 loss: 1.7241573333740234, accuracy: 0.32289138436317444\n",
      "Batch 103 loss: 1.6141774654388428, accuracy: 0.32354265451431274\n",
      "Batch 104 loss: 1.6216503381729126, accuracy: 0.32313987612724304\n",
      "Batch 105 loss: 1.5247598886489868, accuracy: 0.32421875\n",
      "Batch 106 loss: 1.656591534614563, accuracy: 0.3238171637058258\n",
      "Batch 107 loss: 1.6977953910827637, accuracy: 0.32392939925193787\n",
      "Batch 108 loss: 1.56730055809021, accuracy: 0.3243979215621948\n",
      "Batch 109 loss: 1.6192073822021484, accuracy: 0.32436078786849976\n",
      "Batch 110 loss: 1.779141902923584, accuracy: 0.32439470291137695\n",
      "Batch 111 loss: 1.6224720478057861, accuracy: 0.3239397406578064\n",
      "Batch 112 loss: 1.5557643175125122, accuracy: 0.32480642199516296\n",
      "Batch 113 loss: 1.651529312133789, accuracy: 0.32510966062545776\n",
      "Batch 114 loss: 1.592768907546997, accuracy: 0.32540759444236755\n",
      "Batch 115 loss: 1.73311185836792, accuracy: 0.3253636956214905\n",
      "Batch 116 loss: 1.5236170291900635, accuracy: 0.32612180709838867\n",
      "Batch 117 loss: 1.562256932258606, accuracy: 0.32640358805656433\n",
      "Batch 118 loss: 1.585231065750122, accuracy: 0.326877623796463\n",
      "Batch 119 loss: 1.6143230199813843, accuracy: 0.3274739682674408\n",
      "Batch 120 loss: 1.5538840293884277, accuracy: 0.32851240038871765\n",
      "Batch 121 loss: 1.7028279304504395, accuracy: 0.3279969394207001\n",
      "Batch 122 loss: 1.7275320291519165, accuracy: 0.3275533616542816\n",
      "Batch 123 loss: 1.6318037509918213, accuracy: 0.32787299156188965\n",
      "Batch 124 loss: 1.6697460412979126, accuracy: 0.3278124928474426\n",
      "Batch 125 loss: 1.5894677639007568, accuracy: 0.327938973903656\n",
      "Batch 126 loss: 1.6367470026016235, accuracy: 0.328125\n",
      "Batch 127 loss: 1.5791047811508179, accuracy: 0.32855224609375\n",
      "Batch 128 loss: 1.4933323860168457, accuracy: 0.3293968141078949\n",
      "Batch 129 loss: 1.5789560079574585, accuracy: 0.3296875059604645\n",
      "Batch 130 loss: 1.5074949264526367, accuracy: 0.3303912281990051\n",
      "Batch 131 loss: 1.6661555767059326, accuracy: 0.33066996932029724\n",
      "Batch 132 loss: 1.6656839847564697, accuracy: 0.3307095766067505\n",
      "Batch 133 loss: 1.5318655967712402, accuracy: 0.3309818208217621\n",
      "Batch 134 loss: 1.6092936992645264, accuracy: 0.33119213581085205\n",
      "Batch 135 loss: 1.578883171081543, accuracy: 0.3313993513584137\n",
      "Batch 136 loss: 1.5812098979949951, accuracy: 0.3317176103591919\n",
      "Batch 137 loss: 1.681233286857605, accuracy: 0.3318048119544983\n",
      "Batch 138 loss: 1.564940333366394, accuracy: 0.33228418231010437\n",
      "Batch 139 loss: 1.6430951356887817, accuracy: 0.3327008783817291\n",
      "Batch 140 loss: 1.5680363178253174, accuracy: 0.3331117033958435\n",
      "Batch 141 loss: 1.7012656927108765, accuracy: 0.3330765962600708\n",
      "Batch 142 loss: 1.5736693143844604, accuracy: 0.3336429297924042\n",
      "Batch 143 loss: 1.6809781789779663, accuracy: 0.3338216245174408\n",
      "Batch 144 loss: 1.6211085319519043, accuracy: 0.33389008045196533\n",
      "Batch 145 loss: 1.6698272228240967, accuracy: 0.3336900770664215\n",
      "Batch 146 loss: 1.6187437772750854, accuracy: 0.3339179456233978\n",
      "Batch 147 loss: 1.562592625617981, accuracy: 0.33445945382118225\n",
      "Batch 148 loss: 1.6233348846435547, accuracy: 0.33488884568214417\n",
      "Batch 149 loss: 1.5646193027496338, accuracy: 0.33536458015441895\n",
      "Batch 150 loss: 1.702144742012024, accuracy: 0.33505794405937195\n",
      "Batch 151 loss: 1.43583083152771, accuracy: 0.3361431062221527\n",
      "Batch 152 loss: 1.5480103492736816, accuracy: 0.3366013169288635\n",
      "Batch 153 loss: 1.6015349626541138, accuracy: 0.3361911475658417\n",
      "Batch 154 loss: 1.62720787525177, accuracy: 0.3363407254219055\n",
      "Batch 155 loss: 1.5511246919631958, accuracy: 0.3365885317325592\n",
      "Batch 156 loss: 1.6313607692718506, accuracy: 0.3365843892097473\n",
      "Batch 157 loss: 1.44777250289917, accuracy: 0.3372231125831604\n",
      "Batch 158 loss: 1.5451548099517822, accuracy: 0.33765724301338196\n",
      "Batch 159 loss: 1.633061170578003, accuracy: 0.3377441465854645\n",
      "Batch 160 loss: 1.5504295825958252, accuracy: 0.33807259798049927\n",
      "Batch 161 loss: 1.5728353261947632, accuracy: 0.3386381268501282\n",
      "Batch 162 loss: 1.5677244663238525, accuracy: 0.33881327509880066\n",
      "Batch 163 loss: 1.525889277458191, accuracy: 0.3392244577407837\n",
      "Batch 164 loss: 1.5667248964309692, accuracy: 0.33953598141670227\n",
      "Batch 165 loss: 1.5812159776687622, accuracy: 0.33965548872947693\n",
      "Batch 166 loss: 1.4268338680267334, accuracy: 0.3403817415237427\n",
      "Batch 167 loss: 1.5747599601745605, accuracy: 0.3408668041229248\n",
      "Batch 168 loss: 1.6561040878295898, accuracy: 0.3406989574432373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 169 loss: 1.5539638996124268, accuracy: 0.3409007489681244\n",
      "Batch 170 loss: 1.653363585472107, accuracy: 0.34119153022766113\n",
      "Batch 171 loss: 1.5229859352111816, accuracy: 0.3415243327617645\n",
      "Batch 172 loss: 1.5652165412902832, accuracy: 0.3421242833137512\n",
      "Batch 173 loss: 1.5230414867401123, accuracy: 0.34226831793785095\n",
      "Batch 174 loss: 1.4586350917816162, accuracy: 0.34267857670783997\n",
      "Batch 175 loss: 1.5884920358657837, accuracy: 0.34299537539482117\n",
      "Batch 176 loss: 1.72382652759552, accuracy: 0.34269067645072937\n",
      "Batch 177 loss: 1.4429957866668701, accuracy: 0.34322330355644226\n",
      "Batch 178 loss: 1.5746129751205444, accuracy: 0.3433135449886322\n",
      "Batch 179 loss: 1.5786700248718262, accuracy: 0.34383681416511536\n",
      "Batch 180 loss: 1.583387851715088, accuracy: 0.3440089821815491\n",
      "Batch 181 loss: 1.5684006214141846, accuracy: 0.3443080484867096\n",
      "Batch 182 loss: 1.6048856973648071, accuracy: 0.344433069229126\n",
      "Batch 183 loss: 1.47910737991333, accuracy: 0.34493884444236755\n",
      "Batch 184 loss: 1.5197151899337769, accuracy: 0.3452702760696411\n",
      "Batch 185 loss: 1.4949790239334106, accuracy: 0.3456401228904724\n",
      "Batch 186 loss: 1.6269112825393677, accuracy: 0.34550467133522034\n",
      "Batch 187 loss: 1.606656789779663, accuracy: 0.3456200063228607\n",
      "Batch 188 loss: 1.4998701810836792, accuracy: 0.34614747762680054\n",
      "Batch 189 loss: 1.5240010023117065, accuracy: 0.3464227020740509\n",
      "Batch 190 loss: 1.4438745975494385, accuracy: 0.34689953923225403\n",
      "Batch 191 loss: 1.5055805444717407, accuracy: 0.3471272885799408\n",
      "Batch 192 loss: 1.5741991996765137, accuracy: 0.3474336266517639\n",
      "Batch 193 loss: 1.601293683052063, accuracy: 0.34745490550994873\n",
      "Batch 194 loss: 1.5087685585021973, accuracy: 0.3478766083717346\n",
      "Batch 195 loss: 1.3963521718978882, accuracy: 0.3484135866165161\n",
      "Batch 196 loss: 1.5251983404159546, accuracy: 0.3486675024032593\n",
      "Batch 197 loss: 1.5551179647445679, accuracy: 0.34868213534355164\n",
      "Batch 198 loss: 1.452735185623169, accuracy: 0.3490106761455536\n",
      "Batch 199 loss: 1.5072176456451416, accuracy: 0.3492968678474426\n",
      "Batch 200 loss: 1.6239473819732666, accuracy: 0.3492692708969116\n",
      "Batch 201 loss: 1.4897634983062744, accuracy: 0.34966740012168884\n",
      "Batch 202 loss: 1.5984606742858887, accuracy: 0.34990763664245605\n",
      "Batch 203 loss: 1.532273530960083, accuracy: 0.3501838147640228\n",
      "Batch 204 loss: 1.5869463682174683, accuracy: 0.3502286672592163\n",
      "Batch 205 loss: 1.4948127269744873, accuracy: 0.3506523072719574\n",
      "Batch 206 loss: 1.4806078672409058, accuracy: 0.35099637508392334\n",
      "Batch 207 loss: 1.4512430429458618, accuracy: 0.3514873683452606\n",
      "Batch 208 loss: 1.47465181350708, accuracy: 0.3516746461391449\n",
      "Batch 209 loss: 1.5321201086044312, accuracy: 0.35215774178504944\n",
      "Batch 210 loss: 1.4742673635482788, accuracy: 0.3524881601333618\n",
      "Batch 211 loss: 1.4338388442993164, accuracy: 0.3527785837650299\n",
      "Batch 212 loss: 1.565963864326477, accuracy: 0.3527362048625946\n",
      "Batch 213 loss: 1.560471773147583, accuracy: 0.3530227839946747\n",
      "Batch 214 loss: 1.6058884859085083, accuracy: 0.3530886769294739\n",
      "Batch 215 loss: 1.6204421520233154, accuracy: 0.35293692350387573\n",
      "Batch 216 loss: 1.5119881629943848, accuracy: 0.3533626198768616\n",
      "Batch 217 loss: 1.525482177734375, accuracy: 0.35382023453712463\n",
      "Batch 218 loss: 1.5429964065551758, accuracy: 0.3541666567325592\n",
      "Batch 219 loss: 1.612268090248108, accuracy: 0.3542613685131073\n",
      "Batch 220 loss: 1.4849570989608765, accuracy: 0.3545319437980652\n",
      "Batch 221 loss: 1.559778094291687, accuracy: 0.3549760580062866\n",
      "Batch 222 loss: 1.414521336555481, accuracy: 0.35548627376556396\n",
      "Batch 223 loss: 1.4441560506820679, accuracy: 0.3558175265789032\n",
      "Batch 224 loss: 1.5003042221069336, accuracy: 0.35598593950271606\n",
      "Training epoch: 2, train accuracy: 35.598594665527344, train loss: 1.6235756391949123, valid accuracy: 43.54973602294922, valid loss: 1.4848199836139022 \n",
      "Batch 0 loss: 1.5076950788497925, accuracy: 0.390625\n",
      "Batch 1 loss: 1.4297258853912354, accuracy: 0.41015625\n",
      "Batch 2 loss: 1.36341392993927, accuracy: 0.4192708432674408\n",
      "Batch 3 loss: 1.632957100868225, accuracy: 0.404296875\n",
      "Batch 4 loss: 1.4347959756851196, accuracy: 0.4078125059604645\n",
      "Batch 5 loss: 1.468247413635254, accuracy: 0.4127604067325592\n",
      "Batch 6 loss: 1.4423221349716187, accuracy: 0.4162946343421936\n",
      "Batch 7 loss: 1.2973828315734863, accuracy: 0.4248046875\n",
      "Batch 8 loss: 1.4183672666549683, accuracy: 0.4236111044883728\n",
      "Batch 9 loss: 1.4852608442306519, accuracy: 0.4234375059604645\n",
      "Batch 10 loss: 1.491795301437378, accuracy: 0.4261363744735718\n",
      "Batch 11 loss: 1.562171220779419, accuracy: 0.421875\n",
      "Batch 12 loss: 1.4124873876571655, accuracy: 0.42367789149284363\n",
      "Batch 13 loss: 1.4800925254821777, accuracy: 0.421875\n",
      "Batch 14 loss: 1.434401512145996, accuracy: 0.4234375059604645\n",
      "Batch 15 loss: 1.499214768409729, accuracy: 0.4208984375\n",
      "Batch 16 loss: 1.5654733180999756, accuracy: 0.4163602888584137\n",
      "Batch 17 loss: 1.6274536848068237, accuracy: 0.4153645932674408\n",
      "Batch 18 loss: 1.384375810623169, accuracy: 0.4189967215061188\n",
      "Batch 19 loss: 1.4134695529937744, accuracy: 0.423828125\n",
      "Batch 20 loss: 1.6209367513656616, accuracy: 0.4226190447807312\n",
      "Batch 21 loss: 1.4768576622009277, accuracy: 0.4232954680919647\n",
      "Batch 22 loss: 1.4998971223831177, accuracy: 0.421875\n",
      "Batch 23 loss: 1.4433786869049072, accuracy: 0.4212239682674408\n",
      "Batch 24 loss: 1.422984004020691, accuracy: 0.4231249988079071\n",
      "Batch 25 loss: 1.496537685394287, accuracy: 0.42397835850715637\n",
      "Batch 26 loss: 1.5704033374786377, accuracy: 0.42303240299224854\n",
      "Batch 27 loss: 1.2782049179077148, accuracy: 0.4255022406578064\n",
      "Batch 28 loss: 1.471927523612976, accuracy: 0.42322197556495667\n",
      "Batch 29 loss: 1.5294965505599976, accuracy: 0.4234375059604645\n",
      "Batch 30 loss: 1.401534914970398, accuracy: 0.4233871102333069\n",
      "Batch 31 loss: 1.364464521408081, accuracy: 0.42529296875\n",
      "Batch 32 loss: 1.3943874835968018, accuracy: 0.4270833432674408\n",
      "Batch 33 loss: 1.4370571374893188, accuracy: 0.42784926295280457\n",
      "Batch 34 loss: 1.5111522674560547, accuracy: 0.4270089268684387\n",
      "Batch 35 loss: 1.5498957633972168, accuracy: 0.4249131977558136\n",
      "Batch 36 loss: 1.4260505437850952, accuracy: 0.42609795928001404\n",
      "Batch 37 loss: 1.4901721477508545, accuracy: 0.4245477020740509\n",
      "Batch 38 loss: 1.5286883115768433, accuracy: 0.4240785241127014\n",
      "Batch 39 loss: 1.466090440750122, accuracy: 0.423828125\n",
      "Batch 40 loss: 1.5696499347686768, accuracy: 0.42282775044441223\n",
      "Batch 41 loss: 1.3547419309616089, accuracy: 0.4228050708770752\n",
      "Batch 42 loss: 1.5473735332489014, accuracy: 0.4231468141078949\n",
      "Batch 43 loss: 1.6178886890411377, accuracy: 0.42258521914482117\n",
      "Batch 44 loss: 1.4078303575515747, accuracy: 0.4237847328186035\n",
      "Batch 45 loss: 1.4831690788269043, accuracy: 0.42306384444236755\n",
      "Batch 46 loss: 1.5146775245666504, accuracy: 0.42370346188545227\n",
      "Batch 47 loss: 1.4241971969604492, accuracy: 0.4249674379825592\n",
      "Batch 48 loss: 1.5075081586837769, accuracy: 0.4250637888908386\n",
      "Batch 49 loss: 1.4781646728515625, accuracy: 0.4246875047683716\n",
      "Batch 50 loss: 1.4325164556503296, accuracy: 0.42555147409439087\n",
      "Batch 51 loss: 1.4826419353485107, accuracy: 0.42563101649284363\n",
      "Batch 52 loss: 1.5463762283325195, accuracy: 0.42614975571632385\n",
      "Batch 53 loss: 1.56172513961792, accuracy: 0.42578125\n",
      "Batch 54 loss: 1.5124175548553467, accuracy: 0.42656248807907104\n",
      "Batch 55 loss: 1.553424596786499, accuracy: 0.42578125\n",
      "Batch 56 loss: 1.2790770530700684, accuracy: 0.42790570855140686\n",
      "Batch 57 loss: 1.387184500694275, accuracy: 0.42807111144065857\n",
      "Batch 58 loss: 1.466203212738037, accuracy: 0.4283633530139923\n",
      "Batch 59 loss: 1.5149219036102295, accuracy: 0.42825520038604736\n",
      "Batch 60 loss: 1.5074228048324585, accuracy: 0.4284067749977112\n",
      "Batch 61 loss: 1.5000571012496948, accuracy: 0.4281753897666931\n",
      "Batch 62 loss: 1.5117859840393066, accuracy: 0.4273313581943512\n",
      "Batch 63 loss: 1.442020058631897, accuracy: 0.4283447265625\n",
      "Batch 64 loss: 1.5543359518051147, accuracy: 0.4277644157409668\n",
      "Batch 65 loss: 1.3373628854751587, accuracy: 0.42909565567970276\n",
      "Batch 66 loss: 1.3311820030212402, accuracy: 0.4307369291782379\n",
      "Batch 67 loss: 1.5802439451217651, accuracy: 0.42945772409439087\n",
      "Batch 68 loss: 1.393829345703125, accuracy: 0.42957428097724915\n",
      "Batch 69 loss: 1.4737645387649536, accuracy: 0.4290178716182709\n",
      "Batch 70 loss: 1.391251564025879, accuracy: 0.4290272891521454\n",
      "Batch 71 loss: 1.3999781608581543, accuracy: 0.4295789897441864\n",
      "Batch 72 loss: 1.3735668659210205, accuracy: 0.43022260069847107\n",
      "Batch 73 loss: 1.4652869701385498, accuracy: 0.4298986494541168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 74 loss: 1.3224605321884155, accuracy: 0.4308333396911621\n",
      "Batch 75 loss: 1.436442494392395, accuracy: 0.43081825971603394\n",
      "Batch 76 loss: 1.4616221189498901, accuracy: 0.4309050440788269\n",
      "Batch 77 loss: 1.5448787212371826, accuracy: 0.4307892620563507\n",
      "Batch 78 loss: 1.3651686906814575, accuracy: 0.43087419867515564\n",
      "Batch 79 loss: 1.5015922784805298, accuracy: 0.4306640625\n",
      "Batch 80 loss: 1.4484102725982666, accuracy: 0.4309413433074951\n",
      "Batch 81 loss: 1.4274072647094727, accuracy: 0.4312118887901306\n",
      "Batch 82 loss: 1.4568864107131958, accuracy: 0.4309111535549164\n",
      "Batch 83 loss: 1.3388758897781372, accuracy: 0.431454598903656\n",
      "Batch 84 loss: 1.499086618423462, accuracy: 0.4314338266849518\n",
      "Batch 85 loss: 1.5660438537597656, accuracy: 0.43032339215278625\n",
      "Batch 86 loss: 1.4912997484207153, accuracy: 0.4308548867702484\n",
      "Batch 87 loss: 1.402901530265808, accuracy: 0.4298650622367859\n",
      "Batch 88 loss: 1.4729347229003906, accuracy: 0.4294241666793823\n",
      "Batch 89 loss: 1.4155720472335815, accuracy: 0.4295139014720917\n",
      "Batch 90 loss: 1.3945553302764893, accuracy: 0.42977336049079895\n",
      "Batch 91 loss: 1.4854148626327515, accuracy: 0.42943274974823\n",
      "Batch 92 loss: 1.3294309377670288, accuracy: 0.4301075339317322\n",
      "Batch 93 loss: 1.3648767471313477, accuracy: 0.4301861822605133\n",
      "Batch 94 loss: 1.310398817062378, accuracy: 0.43133223056793213\n",
      "Batch 95 loss: 1.4857078790664673, accuracy: 0.4317220151424408\n",
      "Batch 96 loss: 1.527671217918396, accuracy: 0.4321037232875824\n",
      "Batch 97 loss: 1.6175460815429688, accuracy: 0.43152105808258057\n",
      "Batch 98 loss: 1.4772799015045166, accuracy: 0.4314236044883728\n",
      "Batch 99 loss: 1.3979060649871826, accuracy: 0.43210938572883606\n",
      "Batch 100 loss: 1.3778966665267944, accuracy: 0.4326268434524536\n",
      "Batch 101 loss: 1.3328378200531006, accuracy: 0.43313419818878174\n",
      "Batch 102 loss: 1.4416768550872803, accuracy: 0.43317657709121704\n",
      "Batch 103 loss: 1.3031079769134521, accuracy: 0.4339693486690521\n",
      "Batch 104 loss: 1.4028549194335938, accuracy: 0.43430060148239136\n",
      "Batch 105 loss: 1.4863742589950562, accuracy: 0.43418335914611816\n",
      "Batch 106 loss: 1.4954521656036377, accuracy: 0.4339953362941742\n",
      "Batch 107 loss: 1.385449767112732, accuracy: 0.4338107705116272\n",
      "Batch 108 loss: 1.5618962049484253, accuracy: 0.43327122926712036\n",
      "Batch 109 loss: 1.479164481163025, accuracy: 0.4336647689342499\n",
      "Batch 110 loss: 1.381659746170044, accuracy: 0.43412160873413086\n",
      "Batch 111 loss: 1.4710981845855713, accuracy: 0.4341517984867096\n",
      "Batch 112 loss: 1.4863693714141846, accuracy: 0.43376660346984863\n",
      "Batch 113 loss: 1.4797694683074951, accuracy: 0.43318256735801697\n",
      "Batch 114 loss: 1.4685592651367188, accuracy: 0.433016300201416\n",
      "Batch 115 loss: 1.4514728784561157, accuracy: 0.43318966031074524\n",
      "Batch 116 loss: 1.3361362218856812, accuracy: 0.4334268271923065\n",
      "Batch 117 loss: 1.4129303693771362, accuracy: 0.4332627058029175\n",
      "Batch 118 loss: 1.4757498502731323, accuracy: 0.4328387677669525\n",
      "Batch 119 loss: 1.367128849029541, accuracy: 0.43274739384651184\n",
      "Batch 120 loss: 1.3456553220748901, accuracy: 0.43369060754776\n",
      "Batch 121 loss: 1.2699509859085083, accuracy: 0.4345543086528778\n",
      "Batch 122 loss: 1.2785513401031494, accuracy: 0.43476879596710205\n",
      "Batch 123 loss: 1.3773683309555054, accuracy: 0.4349798262119293\n",
      "Batch 124 loss: 1.407120704650879, accuracy: 0.43549999594688416\n",
      "Batch 125 loss: 1.5119200944900513, accuracy: 0.4352678656578064\n",
      "Batch 126 loss: 1.3258030414581299, accuracy: 0.43577754497528076\n",
      "Batch 127 loss: 1.3614375591278076, accuracy: 0.4359130859375\n",
      "Batch 128 loss: 1.3769575357437134, accuracy: 0.4359859526157379\n",
      "Batch 129 loss: 1.3451489210128784, accuracy: 0.43635818362236023\n",
      "Batch 130 loss: 1.5509097576141357, accuracy: 0.43600907921791077\n",
      "Batch 131 loss: 1.4450979232788086, accuracy: 0.43643465638160706\n",
      "Batch 132 loss: 1.3450244665145874, accuracy: 0.43679511547088623\n",
      "Batch 133 loss: 1.4375413656234741, accuracy: 0.4366837739944458\n",
      "Batch 134 loss: 1.431311011314392, accuracy: 0.43686342239379883\n",
      "Batch 135 loss: 1.3276188373565674, accuracy: 0.4368681013584137\n",
      "Batch 136 loss: 1.281534194946289, accuracy: 0.43755701184272766\n",
      "Batch 137 loss: 1.3826284408569336, accuracy: 0.4378962814807892\n",
      "Batch 138 loss: 1.3353818655014038, accuracy: 0.4385116994380951\n",
      "Batch 139 loss: 1.3562710285186768, accuracy: 0.4387834966182709\n",
      "Batch 140 loss: 1.2323275804519653, accuracy: 0.4394392669200897\n",
      "Batch 141 loss: 1.382570505142212, accuracy: 0.4399207830429077\n",
      "Batch 142 loss: 1.3005797863006592, accuracy: 0.4405048191547394\n",
      "Batch 143 loss: 1.2474182844161987, accuracy: 0.4408094584941864\n",
      "Batch 144 loss: 1.346045970916748, accuracy: 0.44100216031074524\n",
      "Batch 145 loss: 1.4916667938232422, accuracy: 0.4412457048892975\n",
      "Batch 146 loss: 1.4841910600662231, accuracy: 0.441592276096344\n",
      "Batch 147 loss: 1.355738878250122, accuracy: 0.44188132882118225\n",
      "Batch 148 loss: 1.4224305152893066, accuracy: 0.4421665370464325\n",
      "Batch 149 loss: 1.3104610443115234, accuracy: 0.4426041543483734\n",
      "Batch 150 loss: 1.4123154878616333, accuracy: 0.4426220953464508\n",
      "Batch 151 loss: 1.3602088689804077, accuracy: 0.44274258613586426\n",
      "Batch 152 loss: 1.4725922346115112, accuracy: 0.4429636299610138\n",
      "Batch 153 loss: 1.401524543762207, accuracy: 0.4432832896709442\n",
      "Batch 154 loss: 1.4381731748580933, accuracy: 0.4432963728904724\n",
      "Batch 155 loss: 1.3397672176361084, accuracy: 0.4431590437889099\n",
      "Batch 156 loss: 1.469372034072876, accuracy: 0.4433220624923706\n",
      "Batch 157 loss: 1.2290891408920288, accuracy: 0.4439280033111572\n",
      "Batch 158 loss: 1.3159781694412231, accuracy: 0.44398584961891174\n",
      "Batch 159 loss: 1.5133607387542725, accuracy: 0.44379884004592896\n",
      "Batch 160 loss: 1.4388175010681152, accuracy: 0.44356560707092285\n",
      "Batch 161 loss: 1.3653041124343872, accuracy: 0.4440104067325592\n",
      "Batch 162 loss: 1.4892014265060425, accuracy: 0.4440183937549591\n",
      "Batch 163 loss: 1.4380122423171997, accuracy: 0.4439786672592163\n",
      "Batch 164 loss: 1.351351261138916, accuracy: 0.44431817531585693\n",
      "Batch 165 loss: 1.3890876770019531, accuracy: 0.4448418617248535\n",
      "Batch 166 loss: 1.5156371593475342, accuracy: 0.444751113653183\n",
      "Batch 167 loss: 1.499347448348999, accuracy: 0.4448009729385376\n",
      "Batch 168 loss: 1.4509973526000977, accuracy: 0.4449426829814911\n",
      "Batch 169 loss: 1.4514626264572144, accuracy: 0.44503676891326904\n",
      "Batch 170 loss: 1.500566840171814, accuracy: 0.44499269127845764\n",
      "Batch 171 loss: 1.4790595769882202, accuracy: 0.4443586468696594\n",
      "Batch 172 loss: 1.5073833465576172, accuracy: 0.44436416029930115\n",
      "Batch 173 loss: 1.3832862377166748, accuracy: 0.444863498210907\n",
      "Batch 174 loss: 1.4467884302139282, accuracy: 0.4450446367263794\n",
      "Batch 175 loss: 1.2398943901062012, accuracy: 0.44557884335517883\n",
      "Batch 176 loss: 1.3395061492919922, accuracy: 0.4458421468734741\n",
      "Batch 177 loss: 1.3747538328170776, accuracy: 0.44583919644355774\n",
      "Batch 178 loss: 1.406611680984497, accuracy: 0.44583624601364136\n",
      "Batch 179 loss: 1.3578221797943115, accuracy: 0.44605034589767456\n",
      "Batch 180 loss: 1.4108495712280273, accuracy: 0.44583046436309814\n",
      "Batch 181 loss: 1.500215768814087, accuracy: 0.4458705484867096\n",
      "Batch 182 loss: 1.2455073595046997, accuracy: 0.44633710384368896\n",
      "Batch 183 loss: 1.3467764854431152, accuracy: 0.44667118787765503\n",
      "Batch 184 loss: 1.3557426929473877, accuracy: 0.4468327760696411\n",
      "Batch 185 loss: 1.282198429107666, accuracy: 0.4473286271095276\n",
      "Batch 186 loss: 1.4384524822235107, accuracy: 0.44731783866882324\n",
      "Batch 187 loss: 1.6124169826507568, accuracy: 0.447265625\n",
      "Batch 188 loss: 1.3397274017333984, accuracy: 0.44762730598449707\n",
      "Batch 189 loss: 1.3826862573623657, accuracy: 0.44769737124443054\n",
      "Batch 190 loss: 1.485005497932434, accuracy: 0.4476030766963959\n",
      "Batch 191 loss: 1.5049848556518555, accuracy: 0.4473470151424408\n",
      "Batch 192 loss: 1.3646926879882812, accuracy: 0.4474983811378479\n",
      "Batch 193 loss: 1.4095163345336914, accuracy: 0.44760793447494507\n",
      "Batch 194 loss: 1.403804063796997, accuracy: 0.4478365480899811\n",
      "Batch 195 loss: 1.350121259689331, accuracy: 0.4479830861091614\n",
      "Batch 196 loss: 1.2440887689590454, accuracy: 0.448009192943573\n",
      "Batch 197 loss: 1.4662915468215942, accuracy: 0.44807448983192444\n",
      "Batch 198 loss: 1.3460235595703125, accuracy: 0.44813913106918335\n",
      "Batch 199 loss: 1.282219648361206, accuracy: 0.44859373569488525\n",
      "Batch 200 loss: 1.2654348611831665, accuracy: 0.44892725348472595\n",
      "Batch 201 loss: 1.3059865236282349, accuracy: 0.4495668411254883\n",
      "Batch 202 loss: 1.635282278060913, accuracy: 0.4494689106941223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 203 loss: 1.6187824010849, accuracy: 0.4492953419685364\n",
      "Batch 204 loss: 1.4267103672027588, accuracy: 0.4493902325630188\n",
      "Batch 205 loss: 1.2797844409942627, accuracy: 0.44971176981925964\n",
      "Batch 206 loss: 1.322950005531311, accuracy: 0.44980373978614807\n",
      "Batch 207 loss: 1.4326372146606445, accuracy: 0.4496318995952606\n",
      "Batch 208 loss: 1.3539388179779053, accuracy: 0.4496859908103943\n",
      "Batch 209 loss: 1.5585230588912964, accuracy: 0.4495907723903656\n",
      "Batch 210 loss: 1.4281789064407349, accuracy: 0.44957050681114197\n",
      "Batch 211 loss: 1.3504455089569092, accuracy: 0.44984522461891174\n",
      "Batch 212 loss: 1.3603079319000244, accuracy: 0.45022740960121155\n",
      "Batch 213 loss: 1.3482173681259155, accuracy: 0.45038697123527527\n",
      "Batch 214 loss: 1.2600375413894653, accuracy: 0.4504360556602478\n",
      "Batch 215 loss: 1.2812352180480957, accuracy: 0.45091870427131653\n",
      "Batch 216 loss: 1.1722854375839233, accuracy: 0.45154088735580444\n",
      "Batch 217 loss: 1.345201015472412, accuracy: 0.45183485746383667\n",
      "Batch 218 loss: 1.329893708229065, accuracy: 0.452126145362854\n",
      "Batch 219 loss: 1.2758712768554688, accuracy: 0.4524857997894287\n",
      "Batch 220 loss: 1.4740008115768433, accuracy: 0.4522412419319153\n",
      "Batch 221 loss: 1.4103096723556519, accuracy: 0.452210009098053\n",
      "Batch 222 loss: 1.3022469282150269, accuracy: 0.4523542523384094\n",
      "Batch 223 loss: 1.3511086702346802, accuracy: 0.452392578125\n",
      "Batch 224 loss: 1.4100637435913086, accuracy: 0.45240169763565063\n",
      "Training epoch: 3, train accuracy: 45.240169525146484, train loss: 1.4235095426771376, valid accuracy: 49.93034362792969, valid loss: 1.277446882478122 \n",
      "Batch 0 loss: 1.4076329469680786, accuracy: 0.453125\n",
      "Batch 1 loss: 1.2892931699752808, accuracy: 0.4765625\n",
      "Batch 2 loss: 1.3861422538757324, accuracy: 0.4583333432674408\n",
      "Batch 3 loss: 1.3541715145111084, accuracy: 0.46484375\n",
      "Batch 4 loss: 1.4432886838912964, accuracy: 0.45781248807907104\n",
      "Batch 5 loss: 1.3017209768295288, accuracy: 0.4765625\n",
      "Batch 6 loss: 1.2770793437957764, accuracy: 0.4799107015132904\n",
      "Batch 7 loss: 1.4624465703964233, accuracy: 0.470703125\n",
      "Batch 8 loss: 1.2692644596099854, accuracy: 0.4722222089767456\n",
      "Batch 9 loss: 1.3189634084701538, accuracy: 0.4742187559604645\n",
      "Batch 10 loss: 1.3894031047821045, accuracy: 0.47585228085517883\n",
      "Batch 11 loss: 1.2905471324920654, accuracy: 0.4798177182674408\n",
      "Batch 12 loss: 1.3125382661819458, accuracy: 0.48257210850715637\n",
      "Batch 13 loss: 1.4005733728408813, accuracy: 0.484375\n",
      "Batch 14 loss: 1.3719598054885864, accuracy: 0.48124998807907104\n",
      "Batch 15 loss: 1.406478762626648, accuracy: 0.48095703125\n",
      "Batch 16 loss: 1.3596731424331665, accuracy: 0.47931984066963196\n",
      "Batch 17 loss: 1.3525149822235107, accuracy: 0.4817708432674408\n",
      "Batch 18 loss: 1.2858474254608154, accuracy: 0.4831414520740509\n",
      "Batch 19 loss: 1.3087811470031738, accuracy: 0.48359376192092896\n",
      "Batch 20 loss: 1.2565827369689941, accuracy: 0.4851190447807312\n",
      "Batch 21 loss: 1.4347336292266846, accuracy: 0.4857954680919647\n",
      "Batch 22 loss: 1.3430485725402832, accuracy: 0.48573368787765503\n",
      "Batch 23 loss: 1.3433640003204346, accuracy: 0.4869791567325592\n",
      "Batch 24 loss: 1.2900280952453613, accuracy: 0.4868749976158142\n",
      "Batch 25 loss: 1.373589277267456, accuracy: 0.48317307233810425\n",
      "Batch 26 loss: 1.4407163858413696, accuracy: 0.4826388955116272\n",
      "Batch 27 loss: 1.3232589960098267, accuracy: 0.4829799234867096\n",
      "Batch 28 loss: 1.2739129066467285, accuracy: 0.4835668206214905\n",
      "Batch 29 loss: 1.3126965761184692, accuracy: 0.4833333194255829\n",
      "Batch 30 loss: 1.5151015520095825, accuracy: 0.4828628897666931\n",
      "Batch 31 loss: 1.2424184083938599, accuracy: 0.485595703125\n",
      "Batch 32 loss: 1.3853650093078613, accuracy: 0.48508521914482117\n",
      "Batch 33 loss: 1.2736718654632568, accuracy: 0.4857536852359772\n",
      "Batch 34 loss: 1.435549259185791, accuracy: 0.48415178060531616\n",
      "Batch 35 loss: 1.2256970405578613, accuracy: 0.4835069477558136\n",
      "Batch 36 loss: 1.286085844039917, accuracy: 0.4833192527294159\n",
      "Batch 37 loss: 1.5018186569213867, accuracy: 0.48252466320991516\n",
      "Batch 38 loss: 1.4031630754470825, accuracy: 0.4819711446762085\n",
      "Batch 39 loss: 1.453663945198059, accuracy: 0.48046875\n",
      "Batch 40 loss: 1.326934814453125, accuracy: 0.48132622241973877\n",
      "Batch 41 loss: 1.3308069705963135, accuracy: 0.4821428656578064\n",
      "Batch 42 loss: 1.2536407709121704, accuracy: 0.48382994532585144\n",
      "Batch 43 loss: 1.4748040437698364, accuracy: 0.4838423430919647\n",
      "Batch 44 loss: 1.4193159341812134, accuracy: 0.4833333194255829\n",
      "Batch 45 loss: 1.4125245809555054, accuracy: 0.48267662525177\n",
      "Batch 46 loss: 1.3660894632339478, accuracy: 0.48238033056259155\n",
      "Batch 47 loss: 1.3635057210922241, accuracy: 0.4825846254825592\n",
      "Batch 48 loss: 1.3400721549987793, accuracy: 0.4819834232330322\n",
      "Batch 49 loss: 1.4110251665115356, accuracy: 0.4803124964237213\n",
      "Batch 50 loss: 1.3341591358184814, accuracy: 0.48069852590560913\n",
      "Batch 51 loss: 1.3446978330612183, accuracy: 0.4810697138309479\n",
      "Batch 52 loss: 1.3192323446273804, accuracy: 0.4817216992378235\n",
      "Batch 53 loss: 1.3346014022827148, accuracy: 0.48321759700775146\n",
      "Batch 54 loss: 1.32559072971344, accuracy: 0.48394885659217834\n",
      "Batch 55 loss: 1.4485732316970825, accuracy: 0.4835379421710968\n",
      "Batch 56 loss: 1.3352901935577393, accuracy: 0.4834155738353729\n",
      "Batch 57 loss: 1.3685195446014404, accuracy: 0.4837014973163605\n",
      "Batch 58 loss: 1.4446594715118408, accuracy: 0.48331567645072937\n",
      "Batch 59 loss: 1.3040004968643188, accuracy: 0.48294270038604736\n",
      "Batch 60 loss: 1.247961401939392, accuracy: 0.48399078845977783\n",
      "Batch 61 loss: 1.3270512819290161, accuracy: 0.4838709533214569\n",
      "Batch 62 loss: 1.3687177896499634, accuracy: 0.4840029776096344\n",
      "Batch 63 loss: 1.3801934719085693, accuracy: 0.4847412109375\n",
      "Batch 64 loss: 1.5225017070770264, accuracy: 0.4829326868057251\n",
      "Batch 65 loss: 1.2928880453109741, accuracy: 0.483428031206131\n",
      "Batch 66 loss: 1.2779607772827148, accuracy: 0.48344215750694275\n",
      "Batch 67 loss: 1.2158466577529907, accuracy: 0.48460477590560913\n",
      "Batch 68 loss: 1.421683430671692, accuracy: 0.48516756296157837\n",
      "Batch 69 loss: 1.4206409454345703, accuracy: 0.48515623807907104\n",
      "Batch 70 loss: 1.2999266386032104, accuracy: 0.4861355721950531\n",
      "Batch 71 loss: 1.3296889066696167, accuracy: 0.4860025942325592\n",
      "Batch 72 loss: 1.3958648443222046, accuracy: 0.48544520139694214\n",
      "Batch 73 loss: 1.3218222856521606, accuracy: 0.4855363070964813\n",
      "Batch 74 loss: 1.3231666088104248, accuracy: 0.4857291579246521\n",
      "Batch 75 loss: 1.3292330503463745, accuracy: 0.48571133613586426\n",
      "Batch 76 loss: 1.2884876728057861, accuracy: 0.48589691519737244\n",
      "Batch 77 loss: 1.3321071863174438, accuracy: 0.48647835850715637\n",
      "Batch 78 loss: 1.2603259086608887, accuracy: 0.4869461953639984\n",
      "Batch 79 loss: 1.306086540222168, accuracy: 0.48710936307907104\n",
      "Batch 80 loss: 1.3656432628631592, accuracy: 0.4874614179134369\n",
      "Batch 81 loss: 1.2371296882629395, accuracy: 0.4880906939506531\n",
      "Batch 82 loss: 1.3617527484893799, accuracy: 0.4879518151283264\n",
      "Batch 83 loss: 1.2406400442123413, accuracy: 0.4879092276096344\n",
      "Batch 84 loss: 1.3048514127731323, accuracy: 0.4883272051811218\n",
      "Batch 85 loss: 1.2802244424819946, accuracy: 0.48828125\n",
      "Batch 86 loss: 1.3163613080978394, accuracy: 0.4880567491054535\n",
      "Batch 87 loss: 1.3235509395599365, accuracy: 0.4876598119735718\n",
      "Batch 88 loss: 1.4166470766067505, accuracy: 0.48753511905670166\n",
      "Batch 89 loss: 1.2418500185012817, accuracy: 0.48776042461395264\n",
      "Batch 90 loss: 1.392109751701355, accuracy: 0.48780906200408936\n",
      "Batch 91 loss: 1.4291778802871704, accuracy: 0.487177312374115\n",
      "Batch 92 loss: 1.2433286905288696, accuracy: 0.4879032373428345\n",
      "Batch 93 loss: 1.2854247093200684, accuracy: 0.4881981313228607\n",
      "Batch 94 loss: 1.33529531955719, accuracy: 0.48815789818763733\n",
      "Batch 95 loss: 1.3403778076171875, accuracy: 0.4876302182674408\n",
      "Batch 96 loss: 1.1757750511169434, accuracy: 0.48791882395744324\n",
      "Batch 97 loss: 1.422605037689209, accuracy: 0.48804208636283875\n",
      "Batch 98 loss: 1.415165662765503, accuracy: 0.48761048913002014\n",
      "Batch 99 loss: 1.2732936143875122, accuracy: 0.4884375035762787\n",
      "Batch 100 loss: 1.2876813411712646, accuracy: 0.48909345269203186\n",
      "Batch 101 loss: 1.2296953201293945, accuracy: 0.48904716968536377\n",
      "Batch 102 loss: 1.2309902906417847, accuracy: 0.4893810749053955\n",
      "Batch 103 loss: 1.3632700443267822, accuracy: 0.4892578125\n",
      "Batch 104 loss: 1.3045654296875, accuracy: 0.4892113208770752\n",
      "Batch 105 loss: 1.2612123489379883, accuracy: 0.4899764060974121\n",
      "Batch 106 loss: 1.2900049686431885, accuracy: 0.49050816893577576\n",
      "Batch 107 loss: 1.218276023864746, accuracy: 0.4911024272441864\n",
      "Batch 108 loss: 1.4115082025527954, accuracy: 0.49082568287849426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 109 loss: 1.2390432357788086, accuracy: 0.49119317531585693\n",
      "Batch 110 loss: 1.2686872482299805, accuracy: 0.4913429021835327\n",
      "Batch 111 loss: 1.2946168184280396, accuracy: 0.4912807047367096\n",
      "Batch 112 loss: 1.3044613599777222, accuracy: 0.4914270043373108\n",
      "Batch 113 loss: 1.3276313543319702, accuracy: 0.4910910129547119\n",
      "Batch 114 loss: 1.281714916229248, accuracy: 0.491372287273407\n",
      "Batch 115 loss: 1.381849765777588, accuracy: 0.49158135056495667\n",
      "Batch 116 loss: 1.2495583295822144, accuracy: 0.49165332317352295\n",
      "Batch 117 loss: 1.4255971908569336, accuracy: 0.49112817645072937\n",
      "Batch 118 loss: 1.322242259979248, accuracy: 0.4910714328289032\n",
      "Batch 119 loss: 1.143230676651001, accuracy: 0.4918619692325592\n",
      "Batch 120 loss: 1.3631434440612793, accuracy: 0.49121901392936707\n",
      "Batch 121 loss: 1.3048288822174072, accuracy: 0.4912269413471222\n",
      "Batch 122 loss: 1.3674657344818115, accuracy: 0.4913617968559265\n",
      "Batch 123 loss: 1.4263925552368164, accuracy: 0.49086442589759827\n",
      "Batch 124 loss: 1.4022431373596191, accuracy: 0.4908125102519989\n",
      "Batch 125 loss: 1.2229650020599365, accuracy: 0.49138143658638\n",
      "Batch 126 loss: 1.486358404159546, accuracy: 0.49052658677101135\n",
      "Batch 127 loss: 1.2424559593200684, accuracy: 0.490478515625\n",
      "Batch 128 loss: 1.0919866561889648, accuracy: 0.49158188700675964\n",
      "Batch 129 loss: 1.2818440198898315, accuracy: 0.49188703298568726\n",
      "Batch 130 loss: 1.3325023651123047, accuracy: 0.49177002906799316\n",
      "Batch 131 loss: 1.2638707160949707, accuracy: 0.4916548430919647\n",
      "Batch 132 loss: 1.2057812213897705, accuracy: 0.49212875962257385\n",
      "Batch 133 loss: 1.3220261335372925, accuracy: 0.4924207031726837\n",
      "Batch 134 loss: 1.2410296201705933, accuracy: 0.49259260296821594\n",
      "Batch 135 loss: 1.4412460327148438, accuracy: 0.4921875\n",
      "Batch 136 loss: 1.289154052734375, accuracy: 0.4924156069755554\n",
      "Batch 137 loss: 1.4253044128417969, accuracy: 0.4922441244125366\n",
      "Batch 138 loss: 1.3377368450164795, accuracy: 0.4919626712799072\n",
      "Batch 139 loss: 1.3361881971359253, accuracy: 0.4920201003551483\n",
      "Batch 140 loss: 1.2078595161437988, accuracy: 0.49251994490623474\n",
      "Batch 141 loss: 1.2574135065078735, accuracy: 0.49273768067359924\n",
      "Batch 142 loss: 1.3425524234771729, accuracy: 0.4929523468017578\n",
      "Batch 143 loss: 1.2722885608673096, accuracy: 0.4931098222732544\n",
      "Batch 144 loss: 1.334028959274292, accuracy: 0.49283406138420105\n",
      "Batch 145 loss: 1.373162031173706, accuracy: 0.49277612566947937\n",
      "Batch 146 loss: 1.3191001415252686, accuracy: 0.4926658272743225\n",
      "Batch 147 loss: 1.31279456615448, accuracy: 0.4929265081882477\n",
      "Batch 148 loss: 1.2270712852478027, accuracy: 0.4933934509754181\n",
      "Batch 149 loss: 1.2275710105895996, accuracy: 0.4936979115009308\n",
      "Batch 150 loss: 1.107478380203247, accuracy: 0.49399834871292114\n",
      "Batch 151 loss: 1.3398085832595825, accuracy: 0.49383223056793213\n",
      "Batch 152 loss: 1.2339335680007935, accuracy: 0.4941789209842682\n",
      "Batch 153 loss: 1.1326700448989868, accuracy: 0.49457183480262756\n",
      "Batch 154 loss: 1.1889126300811768, accuracy: 0.49440523982048035\n",
      "Batch 155 loss: 1.2430633306503296, accuracy: 0.49429085850715637\n",
      "Batch 156 loss: 1.1722328662872314, accuracy: 0.49442675709724426\n",
      "Batch 157 loss: 1.3489933013916016, accuracy: 0.49461036920547485\n",
      "Batch 158 loss: 1.2882654666900635, accuracy: 0.49454599618911743\n",
      "Batch 159 loss: 1.3179831504821777, accuracy: 0.49462890625\n",
      "Batch 160 loss: 1.3379684686660767, accuracy: 0.49461373686790466\n",
      "Batch 161 loss: 1.4315545558929443, accuracy: 0.4944540858268738\n",
      "Batch 162 loss: 1.2422460317611694, accuracy: 0.49491947889328003\n",
      "Batch 163 loss: 1.335766077041626, accuracy: 0.49499809741973877\n",
      "Batch 164 loss: 1.332723617553711, accuracy: 0.49502840638160706\n",
      "Batch 165 loss: 1.2744946479797363, accuracy: 0.4950583577156067\n",
      "Batch 166 loss: 1.3298367261886597, accuracy: 0.49508795142173767\n",
      "Batch 167 loss: 1.2852203845977783, accuracy: 0.495024174451828\n",
      "Batch 168 loss: 1.379130482673645, accuracy: 0.49477624893188477\n",
      "Batch 169 loss: 1.4748046398162842, accuracy: 0.4945312440395355\n",
      "Batch 170 loss: 1.3467988967895508, accuracy: 0.4942891001701355\n",
      "Batch 171 loss: 1.1827934980392456, accuracy: 0.4944585859775543\n",
      "Batch 172 loss: 1.372684121131897, accuracy: 0.4944905936717987\n",
      "Batch 173 loss: 1.3252346515655518, accuracy: 0.4945671558380127\n",
      "Batch 174 loss: 1.3615962266921997, accuracy: 0.49464285373687744\n",
      "Batch 175 loss: 1.228049397468567, accuracy: 0.49502840638160706\n",
      "Batch 176 loss: 1.2383614778518677, accuracy: 0.495144784450531\n",
      "Batch 177 loss: 1.3601303100585938, accuracy: 0.4950842559337616\n",
      "Batch 178 loss: 1.2435921430587769, accuracy: 0.4954608976840973\n",
      "Batch 179 loss: 1.2953211069107056, accuracy: 0.49535590410232544\n",
      "Batch 180 loss: 1.2532421350479126, accuracy: 0.49533841013908386\n",
      "Batch 181 loss: 1.2236486673355103, accuracy: 0.4953210949897766\n",
      "Batch 182 loss: 1.4132118225097656, accuracy: 0.4953039586544037\n",
      "Batch 183 loss: 1.4004101753234863, accuracy: 0.4948199689388275\n",
      "Batch 184 loss: 1.373497724533081, accuracy: 0.4946368336677551\n",
      "Batch 185 loss: 1.2556495666503906, accuracy: 0.4948756694793701\n",
      "Batch 186 loss: 1.406581997871399, accuracy: 0.49486130475997925\n",
      "Batch 187 loss: 1.3226001262664795, accuracy: 0.4948470890522003\n",
      "Batch 188 loss: 1.265738844871521, accuracy: 0.49508100748062134\n",
      "Batch 189 loss: 1.4563379287719727, accuracy: 0.4946545958518982\n",
      "Batch 190 loss: 1.3025572299957275, accuracy: 0.49468258023262024\n",
      "Batch 191 loss: 1.2019524574279785, accuracy: 0.4948323667049408\n",
      "Batch 192 loss: 1.180784821510315, accuracy: 0.495344877243042\n",
      "Batch 193 loss: 1.1439170837402344, accuracy: 0.49589240550994873\n",
      "Batch 194 loss: 1.2375184297561646, accuracy: 0.49627402424812317\n",
      "Batch 195 loss: 1.308829426765442, accuracy: 0.4962930381298065\n",
      "Batch 196 loss: 1.4106193780899048, accuracy: 0.4964308440685272\n",
      "Batch 197 loss: 1.2029834985733032, accuracy: 0.4969223439693451\n",
      "Batch 198 loss: 1.2468745708465576, accuracy: 0.49717336893081665\n",
      "Batch 199 loss: 1.323167085647583, accuracy: 0.4973828196525574\n",
      "Batch 200 loss: 1.296315312385559, accuracy: 0.4973180890083313\n",
      "Batch 201 loss: 1.228967308998108, accuracy: 0.4974087178707123\n",
      "Batch 202 loss: 1.2186710834503174, accuracy: 0.49753695726394653\n",
      "Batch 203 loss: 1.3326152563095093, accuracy: 0.4976639151573181\n",
      "Batch 204 loss: 1.239859938621521, accuracy: 0.49763718247413635\n",
      "Batch 205 loss: 1.22150456905365, accuracy: 0.4977245032787323\n",
      "Batch 206 loss: 1.2152079343795776, accuracy: 0.49796196818351746\n",
      "Batch 207 loss: 1.221359372138977, accuracy: 0.49819710850715637\n",
      "Batch 208 loss: 1.3588345050811768, accuracy: 0.49805623292922974\n",
      "Batch 209 loss: 1.3206911087036133, accuracy: 0.4977678656578064\n",
      "Batch 210 loss: 1.2649329900741577, accuracy: 0.49781545996665955\n",
      "Batch 211 loss: 1.3370994329452515, accuracy: 0.4978626072406769\n",
      "Batch 212 loss: 1.2252190113067627, accuracy: 0.49787265062332153\n",
      "Batch 213 loss: 1.328858494758606, accuracy: 0.4977000653743744\n",
      "Batch 214 loss: 1.2193666696548462, accuracy: 0.49781978130340576\n",
      "Batch 215 loss: 1.2617623805999756, accuracy: 0.4978298544883728\n",
      "Batch 216 loss: 1.2124688625335693, accuracy: 0.4978758692741394\n",
      "Batch 217 loss: 1.1496999263763428, accuracy: 0.4982081353664398\n",
      "Batch 218 loss: 1.233752727508545, accuracy: 0.498501718044281\n",
      "Batch 219 loss: 1.346192479133606, accuracy: 0.4984019994735718\n",
      "Batch 220 loss: 1.2022981643676758, accuracy: 0.4986213147640228\n",
      "Batch 221 loss: 1.147075891494751, accuracy: 0.49887385964393616\n",
      "Batch 222 loss: 1.3227195739746094, accuracy: 0.49866873025894165\n",
      "Batch 223 loss: 1.3335648775100708, accuracy: 0.4985700249671936\n",
      "Batch 224 loss: 1.2453136444091797, accuracy: 0.49858930706977844\n",
      "Training epoch: 4, train accuracy: 49.85892868041992, train loss: 1.3141534031762017, valid accuracy: 53.190303802490234, valid loss: 1.2073012138235157 \n",
      "Batch 0 loss: 1.300434947013855, accuracy: 0.484375\n",
      "Batch 1 loss: 1.385461688041687, accuracy: 0.47265625\n",
      "Batch 2 loss: 1.3168977499008179, accuracy: 0.4817708432674408\n",
      "Batch 3 loss: 1.3134245872497559, accuracy: 0.484375\n",
      "Batch 4 loss: 1.2284209728240967, accuracy: 0.4921875\n",
      "Batch 5 loss: 1.2994332313537598, accuracy: 0.48828125\n",
      "Batch 6 loss: 1.3198845386505127, accuracy: 0.4933035671710968\n",
      "Batch 7 loss: 1.370811939239502, accuracy: 0.494140625\n",
      "Batch 8 loss: 1.313628911972046, accuracy: 0.4982638955116272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 loss: 1.2378500699996948, accuracy: 0.504687488079071\n",
      "Batch 10 loss: 1.2549506425857544, accuracy: 0.5120738744735718\n",
      "Batch 11 loss: 1.3793193101882935, accuracy: 0.5104166865348816\n",
      "Batch 12 loss: 1.2996617555618286, accuracy: 0.510817289352417\n",
      "Batch 13 loss: 1.1617199182510376, accuracy: 0.5150669813156128\n",
      "Batch 14 loss: 1.229566216468811, accuracy: 0.5151041746139526\n",
      "Batch 15 loss: 1.3216955661773682, accuracy: 0.51416015625\n",
      "Batch 16 loss: 1.377762794494629, accuracy: 0.5110294222831726\n",
      "Batch 17 loss: 1.1543387174606323, accuracy: 0.5134548544883728\n",
      "Batch 18 loss: 1.32472562789917, accuracy: 0.5102795958518982\n",
      "Batch 19 loss: 1.3214359283447266, accuracy: 0.5082031488418579\n",
      "Batch 20 loss: 1.1655020713806152, accuracy: 0.511904776096344\n",
      "Batch 21 loss: 1.1856619119644165, accuracy: 0.5131391882896423\n",
      "Batch 22 loss: 1.1006746292114258, accuracy: 0.5163043737411499\n",
      "Batch 23 loss: 1.1892024278640747, accuracy: 0.517578125\n",
      "Batch 24 loss: 1.228329062461853, accuracy: 0.5190625190734863\n",
      "Batch 25 loss: 1.3117413520812988, accuracy: 0.5171273946762085\n",
      "Batch 26 loss: 1.2038136720657349, accuracy: 0.5185185074806213\n",
      "Batch 27 loss: 1.1999740600585938, accuracy: 0.5186942219734192\n",
      "Batch 28 loss: 1.2628318071365356, accuracy: 0.5183189511299133\n",
      "Batch 29 loss: 1.3814500570297241, accuracy: 0.5161458253860474\n",
      "Batch 30 loss: 1.15278959274292, accuracy: 0.5176411271095276\n",
      "Batch 31 loss: 1.202644944190979, accuracy: 0.5166015625\n",
      "Batch 32 loss: 1.3670990467071533, accuracy: 0.5158617496490479\n",
      "Batch 33 loss: 1.2863284349441528, accuracy: 0.5163143277168274\n",
      "Batch 34 loss: 1.4115873575210571, accuracy: 0.5162946581840515\n",
      "Batch 35 loss: 1.1658308506011963, accuracy: 0.5173611044883728\n",
      "Batch 36 loss: 1.3079490661621094, accuracy: 0.5166807174682617\n",
      "Batch 37 loss: 1.2628824710845947, accuracy: 0.5166529417037964\n",
      "Batch 38 loss: 1.523792028427124, accuracy: 0.5160256624221802\n",
      "Batch 39 loss: 1.3612741231918335, accuracy: 0.515625\n",
      "Batch 40 loss: 1.255799412727356, accuracy: 0.5152438879013062\n",
      "Batch 41 loss: 1.2089295387268066, accuracy: 0.515625\n",
      "Batch 42 loss: 1.2483491897583008, accuracy: 0.5159883499145508\n",
      "Batch 43 loss: 1.284352421760559, accuracy: 0.515625\n",
      "Batch 44 loss: 1.143920660018921, accuracy: 0.5171874761581421\n",
      "Batch 45 loss: 1.2315365076065063, accuracy: 0.5174931883811951\n",
      "Batch 46 loss: 1.2926726341247559, accuracy: 0.5179521441459656\n",
      "Batch 47 loss: 1.2036683559417725, accuracy: 0.5185546875\n",
      "Batch 48 loss: 1.2076013088226318, accuracy: 0.5186543464660645\n",
      "Batch 49 loss: 1.2551627159118652, accuracy: 0.5184375047683716\n",
      "Batch 50 loss: 1.278049349784851, accuracy: 0.5177696347236633\n",
      "Batch 51 loss: 1.1479697227478027, accuracy: 0.5190805196762085\n",
      "Batch 52 loss: 1.2813053131103516, accuracy: 0.5182783007621765\n",
      "Batch 53 loss: 1.2472593784332275, accuracy: 0.5182291865348816\n",
      "Batch 54 loss: 1.4360806941986084, accuracy: 0.5167613625526428\n",
      "Batch 55 loss: 1.2549889087677002, accuracy: 0.5167410969734192\n",
      "Batch 56 loss: 1.206889033317566, accuracy: 0.5164473652839661\n",
      "Batch 57 loss: 1.4113953113555908, accuracy: 0.515625\n",
      "Batch 58 loss: 1.2918874025344849, accuracy: 0.5153601765632629\n",
      "Batch 59 loss: 1.233128547668457, accuracy: 0.5160156488418579\n",
      "Batch 60 loss: 1.2953650951385498, accuracy: 0.5153688788414001\n",
      "Batch 61 loss: 1.2851468324661255, accuracy: 0.5151209831237793\n",
      "Batch 62 loss: 1.2763348817825317, accuracy: 0.5151289701461792\n",
      "Batch 63 loss: 1.336525321006775, accuracy: 0.5147705078125\n",
      "Batch 64 loss: 1.1850001811981201, accuracy: 0.5152644515037537\n",
      "Batch 65 loss: 1.3417154550552368, accuracy: 0.5151515007019043\n",
      "Batch 66 loss: 1.2730731964111328, accuracy: 0.5151585936546326\n",
      "Batch 67 loss: 1.3006136417388916, accuracy: 0.5151654481887817\n",
      "Batch 68 loss: 1.216910481452942, accuracy: 0.5153985619544983\n",
      "Batch 69 loss: 1.2475892305374146, accuracy: 0.515625\n",
      "Batch 70 loss: 1.1651690006256104, accuracy: 0.5163952708244324\n",
      "Batch 71 loss: 1.0944887399673462, accuracy: 0.517578125\n",
      "Batch 72 loss: 1.1752111911773682, accuracy: 0.5177654027938843\n",
      "Batch 73 loss: 1.1922012567520142, accuracy: 0.5177364945411682\n",
      "Batch 74 loss: 1.2798950672149658, accuracy: 0.5179166793823242\n",
      "Batch 75 loss: 1.129473328590393, accuracy: 0.5188117027282715\n",
      "Batch 76 loss: 1.1401653289794922, accuracy: 0.5193790793418884\n",
      "Batch 77 loss: 1.29652738571167, accuracy: 0.5189303159713745\n",
      "Batch 78 loss: 1.3097795248031616, accuracy: 0.5179983973503113\n",
      "Batch 79 loss: 1.1267077922821045, accuracy: 0.5185546875\n",
      "Batch 80 loss: 1.3049664497375488, accuracy: 0.5184220671653748\n",
      "Batch 81 loss: 1.1858806610107422, accuracy: 0.5185785293579102\n",
      "Batch 82 loss: 1.1978094577789307, accuracy: 0.5188252925872803\n",
      "Batch 83 loss: 1.232282042503357, accuracy: 0.5189732313156128\n",
      "Batch 84 loss: 1.304223656654358, accuracy: 0.5188419222831726\n",
      "Batch 85 loss: 1.140953540802002, accuracy: 0.51953125\n",
      "Batch 86 loss: 1.1420444250106812, accuracy: 0.5203843116760254\n",
      "Batch 87 loss: 1.2431803941726685, accuracy: 0.5206853747367859\n",
      "Batch 88 loss: 1.324651837348938, accuracy: 0.5202773809432983\n",
      "Batch 89 loss: 1.2578954696655273, accuracy: 0.5200520753860474\n",
      "Batch 90 loss: 1.2605183124542236, accuracy: 0.5200892686843872\n",
      "Batch 91 loss: 1.1937891244888306, accuracy: 0.5199558138847351\n",
      "Batch 92 loss: 1.3120242357254028, accuracy: 0.5193212628364563\n",
      "Batch 93 loss: 1.198880672454834, accuracy: 0.5200299024581909\n",
      "Batch 94 loss: 1.1971831321716309, accuracy: 0.5205591917037964\n",
      "Batch 95 loss: 1.196526050567627, accuracy: 0.52099609375\n",
      "Batch 96 loss: 1.1559085845947266, accuracy: 0.5210212469100952\n",
      "Batch 97 loss: 1.2673622369766235, accuracy: 0.5208864808082581\n",
      "Batch 98 loss: 1.2081676721572876, accuracy: 0.5214646458625793\n",
      "Batch 99 loss: 1.2759045362472534, accuracy: 0.5210937261581421\n",
      "Batch 100 loss: 1.267892599105835, accuracy: 0.5208075642585754\n",
      "Batch 101 loss: 1.2421122789382935, accuracy: 0.521369457244873\n",
      "Batch 102 loss: 1.3218711614608765, accuracy: 0.5209344625473022\n",
      "Batch 103 loss: 1.3011655807495117, accuracy: 0.5205078125\n",
      "Batch 104 loss: 1.4417681694030762, accuracy: 0.5195684432983398\n",
      "Batch 105 loss: 1.3216067552566528, accuracy: 0.5193838477134705\n",
      "Batch 106 loss: 1.165684700012207, accuracy: 0.5193487405776978\n",
      "Batch 107 loss: 1.230379581451416, accuracy: 0.5191695690155029\n",
      "Batch 108 loss: 1.288649559020996, accuracy: 0.51949542760849\n",
      "Batch 109 loss: 1.2760924100875854, accuracy: 0.5194602012634277\n",
      "Batch 110 loss: 1.1301697492599487, accuracy: 0.5196368098258972\n",
      "Batch 111 loss: 1.3491400480270386, accuracy: 0.5192522406578064\n",
      "Batch 112 loss: 1.2898545265197754, accuracy: 0.5194275379180908\n",
      "Batch 113 loss: 1.2214770317077637, accuracy: 0.519257128238678\n",
      "Batch 114 loss: 1.4276739358901978, accuracy: 0.5188858509063721\n",
      "Batch 115 loss: 1.3184505701065063, accuracy: 0.5188577771186829\n",
      "Batch 116 loss: 1.1956108808517456, accuracy: 0.5188968777656555\n",
      "Batch 117 loss: 1.0946216583251953, accuracy: 0.51953125\n",
      "Batch 118 loss: 1.2760688066482544, accuracy: 0.519629716873169\n",
      "Batch 119 loss: 1.2875128984451294, accuracy: 0.5192708373069763\n",
      "Batch 120 loss: 1.1893506050109863, accuracy: 0.5193052887916565\n",
      "Batch 121 loss: 1.2604613304138184, accuracy: 0.5193391442298889\n",
      "Batch 122 loss: 1.2961606979370117, accuracy: 0.5189913511276245\n",
      "Batch 123 loss: 1.2986596822738647, accuracy: 0.5190272331237793\n",
      "Batch 124 loss: 1.2208324670791626, accuracy: 0.5189375281333923\n",
      "Batch 125 loss: 1.1190111637115479, accuracy: 0.5194692611694336\n",
      "Batch 126 loss: 1.2444040775299072, accuracy: 0.5194389820098877\n",
      "Batch 127 loss: 1.2890405654907227, accuracy: 0.51934814453125\n",
      "Batch 128 loss: 1.2959312200546265, accuracy: 0.5192587375640869\n",
      "Batch 129 loss: 1.3265646696090698, accuracy: 0.5189303159713745\n",
      "Batch 130 loss: 1.2867356538772583, accuracy: 0.5185472369194031\n",
      "Batch 131 loss: 1.1970103979110718, accuracy: 0.5191169381141663\n",
      "Batch 132 loss: 1.2450751066207886, accuracy: 0.5192081928253174\n",
      "Batch 133 loss: 1.2985148429870605, accuracy: 0.5190648436546326\n",
      "Batch 134 loss: 1.2109298706054688, accuracy: 0.5192129611968994\n",
      "Batch 135 loss: 1.2436119318008423, accuracy: 0.5192440152168274\n",
      "Batch 136 loss: 1.222101092338562, accuracy: 0.5195597410202026\n",
      "Batch 137 loss: 1.208153247833252, accuracy: 0.5195878744125366\n",
      "Batch 138 loss: 1.1191765069961548, accuracy: 0.5198965668678284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 139 loss: 1.4420197010040283, accuracy: 0.5191964507102966\n",
      "Batch 140 loss: 1.1859302520751953, accuracy: 0.5192264914512634\n",
      "Batch 141 loss: 1.2711379528045654, accuracy: 0.5189260840415955\n",
      "Batch 142 loss: 1.1696486473083496, accuracy: 0.5190122127532959\n",
      "Batch 143 loss: 1.1443312168121338, accuracy: 0.5191514492034912\n",
      "Batch 144 loss: 1.1895394325256348, accuracy: 0.5191271305084229\n",
      "Batch 145 loss: 0.9608858823776245, accuracy: 0.5200128555297852\n",
      "Batch 146 loss: 1.124340295791626, accuracy: 0.5201956033706665\n",
      "Batch 147 loss: 1.1965433359146118, accuracy: 0.5204814076423645\n",
      "Batch 148 loss: 1.1227564811706543, accuracy: 0.5207633972167969\n",
      "Batch 149 loss: 1.2946414947509766, accuracy: 0.5208854079246521\n",
      "Batch 150 loss: 1.190484642982483, accuracy: 0.5211092829704285\n",
      "Batch 151 loss: 1.1955225467681885, accuracy: 0.5210217833518982\n",
      "Batch 152 loss: 1.269411563873291, accuracy: 0.5210886597633362\n",
      "Batch 153 loss: 1.410760521888733, accuracy: 0.5207995176315308\n",
      "Batch 154 loss: 1.2736726999282837, accuracy: 0.5208669304847717\n",
      "Batch 155 loss: 1.1681182384490967, accuracy: 0.5213842391967773\n",
      "Batch 156 loss: 1.16733717918396, accuracy: 0.521795392036438\n",
      "Batch 157 loss: 1.2494075298309326, accuracy: 0.5220035314559937\n",
      "Batch 158 loss: 1.2989976406097412, accuracy: 0.5223565101623535\n",
      "Batch 159 loss: 1.3529536724090576, accuracy: 0.522265613079071\n",
      "Batch 160 loss: 1.3350439071655273, accuracy: 0.5224184989929199\n",
      "Batch 161 loss: 1.247477412223816, accuracy: 0.5223283171653748\n",
      "Batch 162 loss: 1.1280781030654907, accuracy: 0.5226227045059204\n",
      "Batch 163 loss: 1.336348295211792, accuracy: 0.5224847793579102\n",
      "Batch 164 loss: 1.0995992422103882, accuracy: 0.5230587124824524\n",
      "Batch 165 loss: 1.194981575012207, accuracy: 0.5232021808624268\n",
      "Batch 166 loss: 1.3078656196594238, accuracy: 0.522922933101654\n",
      "Batch 167 loss: 1.3551242351531982, accuracy: 0.5226934552192688\n",
      "Batch 168 loss: 1.26931631565094, accuracy: 0.5227440595626831\n",
      "Batch 169 loss: 1.3375540971755981, accuracy: 0.5224264860153198\n",
      "Batch 170 loss: 1.2450809478759766, accuracy: 0.5223866701126099\n",
      "Batch 171 loss: 1.2356581687927246, accuracy: 0.5224382281303406\n",
      "Batch 172 loss: 1.162549614906311, accuracy: 0.5227149724960327\n",
      "Batch 173 loss: 1.162039041519165, accuracy: 0.5231680870056152\n",
      "Batch 174 loss: 1.0764398574829102, accuracy: 0.5233928561210632\n",
      "Batch 175 loss: 1.1876987218856812, accuracy: 0.5236594676971436\n",
      "Batch 176 loss: 1.3574262857437134, accuracy: 0.5234816670417786\n",
      "Batch 177 loss: 1.3448607921600342, accuracy: 0.5231741666793823\n",
      "Batch 178 loss: 1.206610083580017, accuracy: 0.5233502388000488\n",
      "Batch 179 loss: 1.1919857263565063, accuracy: 0.5231770873069763\n",
      "Batch 180 loss: 1.0169305801391602, accuracy: 0.5238259434700012\n",
      "Batch 181 loss: 1.4192264080047607, accuracy: 0.5231799483299255\n",
      "Batch 182 loss: 1.3057186603546143, accuracy: 0.522754430770874\n",
      "Batch 183 loss: 1.4089723825454712, accuracy: 0.5224184989929199\n",
      "Batch 184 loss: 1.3618919849395752, accuracy: 0.5219594836235046\n",
      "Batch 185 loss: 1.3993099927902222, accuracy: 0.5214633941650391\n",
      "Batch 186 loss: 1.1819047927856445, accuracy: 0.5216827988624573\n",
      "Batch 187 loss: 1.3125734329223633, accuracy: 0.5214012861251831\n",
      "Batch 188 loss: 1.2077981233596802, accuracy: 0.5215773582458496\n",
      "Batch 189 loss: 1.2150750160217285, accuracy: 0.5215460658073425\n",
      "Batch 190 loss: 1.2417343854904175, accuracy: 0.5218423008918762\n",
      "Batch 191 loss: 1.1659733057022095, accuracy: 0.5220947265625\n",
      "Batch 192 loss: 1.2404435873031616, accuracy: 0.5221421718597412\n",
      "Batch 193 loss: 1.097246766090393, accuracy: 0.5223501920700073\n",
      "Batch 194 loss: 1.22355055809021, accuracy: 0.5225961804389954\n",
      "Batch 195 loss: 1.2526284456253052, accuracy: 0.5224011540412903\n",
      "Batch 196 loss: 1.070539116859436, accuracy: 0.5228822827339172\n",
      "Batch 197 loss: 1.1187981367111206, accuracy: 0.5231612920761108\n",
      "Batch 198 loss: 1.1649218797683716, accuracy: 0.5234375\n",
      "Batch 199 loss: 1.3243589401245117, accuracy: 0.5234375\n",
      "Batch 200 loss: 1.2576788663864136, accuracy: 0.5234763622283936\n",
      "Batch 201 loss: 1.4625200033187866, accuracy: 0.5233214497566223\n",
      "Batch 202 loss: 1.306323766708374, accuracy: 0.523360550403595\n",
      "Batch 203 loss: 1.3391329050064087, accuracy: 0.5234375\n",
      "Batch 204 loss: 1.2605791091918945, accuracy: 0.5231707096099854\n",
      "Batch 205 loss: 1.08712637424469, accuracy: 0.5234754085540771\n",
      "Batch 206 loss: 1.153753638267517, accuracy: 0.523928165435791\n",
      "Batch 207 loss: 1.270054578781128, accuracy: 0.5237004160881042\n",
      "Batch 208 loss: 1.225767731666565, accuracy: 0.5237365365028381\n",
      "Batch 209 loss: 1.2253037691116333, accuracy: 0.5239955186843872\n",
      "Batch 210 loss: 1.3380404710769653, accuracy: 0.5238077640533447\n",
      "Batch 211 loss: 1.3396904468536377, accuracy: 0.5236217379570007\n",
      "Batch 212 loss: 1.208930492401123, accuracy: 0.5239143371582031\n",
      "Batch 213 loss: 1.252053141593933, accuracy: 0.5238025784492493\n",
      "Batch 214 loss: 1.2032933235168457, accuracy: 0.5240552425384521\n",
      "Batch 215 loss: 1.2127820253372192, accuracy: 0.5240523815155029\n",
      "Batch 216 loss: 1.3386260271072388, accuracy: 0.5239055156707764\n",
      "Batch 217 loss: 1.2227174043655396, accuracy: 0.5240467190742493\n",
      "Batch 218 loss: 1.128483533859253, accuracy: 0.5242223143577576\n",
      "Batch 219 loss: 1.3049782514572144, accuracy: 0.5238636136054993\n",
      "Batch 220 loss: 1.2198045253753662, accuracy: 0.5238263607025146\n",
      "Batch 221 loss: 1.1784380674362183, accuracy: 0.5239653587341309\n",
      "Batch 222 loss: 1.2825956344604492, accuracy: 0.5239980220794678\n",
      "Batch 223 loss: 1.2391029596328735, accuracy: 0.5238909125328064\n",
      "Batch 224 loss: 1.0076215267181396, accuracy: 0.5242258310317993\n",
      "Training epoch: 5, train accuracy: 52.422584533691406, train loss: 1.2482065688239203, valid accuracy: 55.67010498046875, valid loss: 1.1388971435612645 \n",
      "Batch 0 loss: 1.2036644220352173, accuracy: 0.53125\n",
      "Batch 1 loss: 1.1315832138061523, accuracy: 0.5625\n",
      "Batch 2 loss: 1.1831352710723877, accuracy: 0.5651041865348816\n",
      "Batch 3 loss: 1.2156842947006226, accuracy: 0.552734375\n",
      "Batch 4 loss: 1.3632349967956543, accuracy: 0.5328124761581421\n",
      "Batch 5 loss: 1.2951878309249878, accuracy: 0.5299479365348816\n",
      "Batch 6 loss: 1.1278362274169922, accuracy: 0.5379464030265808\n",
      "Batch 7 loss: 1.3491510152816772, accuracy: 0.529296875\n",
      "Batch 8 loss: 1.389424443244934, accuracy: 0.515625\n",
      "Batch 9 loss: 1.111106038093567, accuracy: 0.524218738079071\n",
      "Batch 10 loss: 1.2399535179138184, accuracy: 0.5248579382896423\n",
      "Batch 11 loss: 1.3686268329620361, accuracy: 0.517578125\n",
      "Batch 12 loss: 1.1413724422454834, accuracy: 0.518629789352417\n",
      "Batch 13 loss: 1.3964866399765015, accuracy: 0.5172991156578064\n",
      "Batch 14 loss: 1.2529926300048828, accuracy: 0.520312488079071\n",
      "Batch 15 loss: 1.153836727142334, accuracy: 0.5205078125\n",
      "Batch 16 loss: 1.2452281713485718, accuracy: 0.5229779481887817\n",
      "Batch 17 loss: 1.1832305192947388, accuracy: 0.5238715410232544\n",
      "Batch 18 loss: 1.2330232858657837, accuracy: 0.5234375\n",
      "Batch 19 loss: 1.1927309036254883, accuracy: 0.524609386920929\n",
      "Batch 20 loss: 1.1542702913284302, accuracy: 0.5282738208770752\n",
      "Batch 21 loss: 1.1282000541687012, accuracy: 0.5294744372367859\n",
      "Batch 22 loss: 1.0482372045516968, accuracy: 0.531589686870575\n",
      "Batch 23 loss: 1.1254262924194336, accuracy: 0.5335286259651184\n",
      "Batch 24 loss: 1.147977590560913, accuracy: 0.5356249809265137\n",
      "Batch 25 loss: 1.0939089059829712, accuracy: 0.5369591116905212\n",
      "Batch 26 loss: 1.2997727394104004, accuracy: 0.5355902910232544\n",
      "Batch 27 loss: 1.241451382637024, accuracy: 0.5348772406578064\n",
      "Batch 28 loss: 1.2079309225082397, accuracy: 0.5344827771186829\n",
      "Batch 29 loss: 1.2552967071533203, accuracy: 0.5354166626930237\n",
      "Batch 30 loss: 1.1115376949310303, accuracy: 0.5360382795333862\n",
      "Batch 31 loss: 1.289383053779602, accuracy: 0.533447265625\n",
      "Batch 32 loss: 1.2645812034606934, accuracy: 0.5314867496490479\n",
      "Batch 33 loss: 1.0790340900421143, accuracy: 0.5321691036224365\n",
      "Batch 34 loss: 1.3397891521453857, accuracy: 0.5305803418159485\n",
      "Batch 35 loss: 1.2349798679351807, accuracy: 0.5301649570465088\n",
      "Batch 36 loss: 1.4308537244796753, accuracy: 0.5272381901741028\n",
      "Batch 37 loss: 1.1960705518722534, accuracy: 0.5283716917037964\n",
      "Batch 38 loss: 1.145824670791626, accuracy: 0.5300480723381042\n",
      "Batch 39 loss: 1.28736412525177, accuracy: 0.5283203125\n",
      "Batch 40 loss: 1.153859257698059, accuracy: 0.5291539430618286\n",
      "Batch 41 loss: 1.0681143999099731, accuracy: 0.530877947807312\n",
      "Batch 42 loss: 1.206558346748352, accuracy: 0.530704915523529\n",
      "Batch 43 loss: 1.1983675956726074, accuracy: 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44 loss: 1.1633137464523315, accuracy: 0.5315971970558167\n",
      "Batch 45 loss: 1.1298489570617676, accuracy: 0.53175950050354\n",
      "Batch 46 loss: 1.202701449394226, accuracy: 0.5310837626457214\n",
      "Batch 47 loss: 1.1555395126342773, accuracy: 0.53271484375\n",
      "Batch 48 loss: 1.2278021574020386, accuracy: 0.5325255393981934\n",
      "Batch 49 loss: 1.1550004482269287, accuracy: 0.5329687595367432\n",
      "Batch 50 loss: 1.2156118154525757, accuracy: 0.5327818393707275\n",
      "Batch 51 loss: 1.1668481826782227, accuracy: 0.5329026579856873\n",
      "Batch 52 loss: 1.334403395652771, accuracy: 0.5328714847564697\n",
      "Batch 53 loss: 1.1325194835662842, accuracy: 0.5332754850387573\n",
      "Batch 54 loss: 1.1929681301116943, accuracy: 0.5330966114997864\n",
      "Batch 55 loss: 1.2555665969848633, accuracy: 0.5330635905265808\n",
      "Batch 56 loss: 1.1514647006988525, accuracy: 0.5328947305679321\n",
      "Batch 57 loss: 1.2483701705932617, accuracy: 0.5327316522598267\n",
      "Batch 58 loss: 1.26856529712677, accuracy: 0.5325741767883301\n",
      "Batch 59 loss: 1.0927664041519165, accuracy: 0.5338541865348816\n",
      "Batch 60 loss: 1.1065841913223267, accuracy: 0.5343237519264221\n",
      "Batch 61 loss: 1.1265634298324585, accuracy: 0.5352822542190552\n",
      "Batch 62 loss: 1.2123351097106934, accuracy: 0.5357142686843872\n",
      "Batch 63 loss: 1.167934775352478, accuracy: 0.5367431640625\n",
      "Batch 64 loss: 1.1063573360443115, accuracy: 0.5364182591438293\n",
      "Batch 65 loss: 1.155489444732666, accuracy: 0.5376420617103577\n",
      "Batch 66 loss: 1.2751585245132446, accuracy: 0.53614741563797\n",
      "Batch 67 loss: 1.2506179809570312, accuracy: 0.5356158018112183\n",
      "Batch 68 loss: 1.2089946269989014, accuracy: 0.5357789993286133\n",
      "Batch 69 loss: 1.1487765312194824, accuracy: 0.535937488079071\n",
      "Batch 70 loss: 1.2624422311782837, accuracy: 0.5360915660858154\n",
      "Batch 71 loss: 1.2671971321105957, accuracy: 0.5360243320465088\n",
      "Batch 72 loss: 1.089190125465393, accuracy: 0.5367080569267273\n",
      "Batch 73 loss: 1.2173821926116943, accuracy: 0.5368454456329346\n",
      "Batch 74 loss: 1.1814438104629517, accuracy: 0.5370833277702332\n",
      "Batch 75 loss: 1.168487310409546, accuracy: 0.537109375\n",
      "Batch 76 loss: 1.1739680767059326, accuracy: 0.5374391078948975\n",
      "Batch 77 loss: 1.2619327306747437, accuracy: 0.5373597741127014\n",
      "Batch 78 loss: 1.1619784832000732, accuracy: 0.5386669039726257\n",
      "Batch 79 loss: 1.152446985244751, accuracy: 0.5389648675918579\n",
      "Batch 80 loss: 1.0988287925720215, accuracy: 0.5398340821266174\n",
      "Batch 81 loss: 1.2408455610275269, accuracy: 0.5391577482223511\n",
      "Batch 82 loss: 1.1636409759521484, accuracy: 0.5390625\n",
      "Batch 83 loss: 1.150989055633545, accuracy: 0.539248526096344\n",
      "Batch 84 loss: 1.1515436172485352, accuracy: 0.5396139621734619\n",
      "Batch 85 loss: 1.3354130983352661, accuracy: 0.5391533374786377\n",
      "Batch 86 loss: 1.1187918186187744, accuracy: 0.5398706793785095\n",
      "Batch 87 loss: 1.2007453441619873, accuracy: 0.5396839380264282\n",
      "Batch 88 loss: 1.2030082941055298, accuracy: 0.5396769642829895\n",
      "Batch 89 loss: 1.0766135454177856, accuracy: 0.5402777791023254\n",
      "Batch 90 loss: 1.1888831853866577, accuracy: 0.5400927066802979\n",
      "Batch 91 loss: 1.4797359704971313, accuracy: 0.5389775633811951\n",
      "Batch 92 loss: 1.0637400150299072, accuracy: 0.5399025678634644\n",
      "Batch 93 loss: 1.1598451137542725, accuracy: 0.5405585169792175\n",
      "Batch 94 loss: 1.0808144807815552, accuracy: 0.5416118502616882\n",
      "Batch 95 loss: 1.2716788053512573, accuracy: 0.541015625\n",
      "Batch 96 loss: 1.3668814897537231, accuracy: 0.5405927896499634\n",
      "Batch 97 loss: 1.2403281927108765, accuracy: 0.5404177308082581\n",
      "Batch 98 loss: 1.0828090906143188, accuracy: 0.5410353541374207\n",
      "Batch 99 loss: 1.3509916067123413, accuracy: 0.5401562452316284\n",
      "Batch 100 loss: 1.1653618812561035, accuracy: 0.5405321717262268\n",
      "Batch 101 loss: 1.1831941604614258, accuracy: 0.5401347875595093\n",
      "Batch 102 loss: 1.388787031173706, accuracy: 0.5402002334594727\n",
      "Batch 103 loss: 1.216827392578125, accuracy: 0.5402644276618958\n",
      "Batch 104 loss: 1.2353780269622803, accuracy: 0.5403273701667786\n",
      "Batch 105 loss: 1.13625967502594, accuracy: 0.5403154492378235\n",
      "Batch 106 loss: 1.0579856634140015, accuracy: 0.5408878326416016\n",
      "Batch 107 loss: 1.2629398107528687, accuracy: 0.5406539440155029\n",
      "Batch 108 loss: 1.1801866292953491, accuracy: 0.540639340877533\n",
      "Batch 109 loss: 1.1195515394210815, accuracy: 0.5409091114997864\n",
      "Batch 110 loss: 1.2527563571929932, accuracy: 0.5406109094619751\n",
      "Batch 111 loss: 1.2485359907150269, accuracy: 0.5413643717765808\n",
      "Batch 112 loss: 1.2274210453033447, accuracy: 0.5416205525398254\n",
      "Batch 113 loss: 1.1427992582321167, accuracy: 0.5418037176132202\n",
      "Batch 114 loss: 1.205820083618164, accuracy: 0.5419837236404419\n",
      "Batch 115 loss: 1.256385326385498, accuracy: 0.5414870977401733\n",
      "Batch 116 loss: 1.3070157766342163, accuracy: 0.5413327813148499\n",
      "Batch 117 loss: 1.1780157089233398, accuracy: 0.5409825444221497\n",
      "Batch 118 loss: 1.2399938106536865, accuracy: 0.5411633253097534\n",
      "Batch 119 loss: 1.1982712745666504, accuracy: 0.5413411259651184\n",
      "Batch 120 loss: 1.0932860374450684, accuracy: 0.5419034361839294\n",
      "Batch 121 loss: 1.1619189977645874, accuracy: 0.5415599346160889\n",
      "Batch 122 loss: 1.1743147373199463, accuracy: 0.541793704032898\n",
      "Batch 123 loss: 1.2706611156463623, accuracy: 0.5415826439857483\n",
      "Batch 124 loss: 1.2058347463607788, accuracy: 0.5414999723434448\n",
      "Batch 125 loss: 1.2363759279251099, accuracy: 0.5414186716079712\n",
      "Batch 126 loss: 1.0484097003936768, accuracy: 0.5415846705436707\n",
      "Batch 127 loss: 1.122538447380066, accuracy: 0.54168701171875\n",
      "Batch 128 loss: 1.2799373865127563, accuracy: 0.5416061282157898\n",
      "Batch 129 loss: 1.2213935852050781, accuracy: 0.5418269038200378\n",
      "Batch 130 loss: 1.275526762008667, accuracy: 0.5420443415641785\n",
      "Batch 131 loss: 1.1671369075775146, accuracy: 0.5418442487716675\n",
      "Batch 132 loss: 1.192352056503296, accuracy: 0.541705846786499\n",
      "Batch 133 loss: 1.2433820962905884, accuracy: 0.5413945913314819\n",
      "Batch 134 loss: 1.153953194618225, accuracy: 0.5420138835906982\n",
      "Batch 135 loss: 1.2048851251602173, accuracy: 0.5421645045280457\n",
      "Batch 136 loss: 1.145931363105774, accuracy: 0.5423699617385864\n",
      "Batch 137 loss: 1.1882652044296265, accuracy: 0.5425158739089966\n",
      "Batch 138 loss: 1.2266420125961304, accuracy: 0.5422661900520325\n",
      "Batch 139 loss: 1.2022753953933716, accuracy: 0.5424665212631226\n",
      "Batch 140 loss: 1.2721928358078003, accuracy: 0.5421099066734314\n",
      "Batch 141 loss: 1.1822434663772583, accuracy: 0.5420884490013123\n",
      "Batch 142 loss: 1.0868109464645386, accuracy: 0.5422312021255493\n",
      "Batch 143 loss: 1.1218078136444092, accuracy: 0.5423176884651184\n",
      "Batch 144 loss: 1.237349033355713, accuracy: 0.5425646305084229\n",
      "Batch 145 loss: 1.1656028032302856, accuracy: 0.5425406694412231\n",
      "Batch 146 loss: 1.1791709661483765, accuracy: 0.5426232814788818\n",
      "Batch 147 loss: 1.2887228727340698, accuracy: 0.5425992608070374\n",
      "Batch 148 loss: 1.436468243598938, accuracy: 0.5424706339836121\n",
      "Batch 149 loss: 1.25271475315094, accuracy: 0.5424479246139526\n",
      "Batch 150 loss: 1.1152746677398682, accuracy: 0.5425289869308472\n",
      "Batch 151 loss: 1.132481336593628, accuracy: 0.5425575375556946\n",
      "Batch 152 loss: 0.9628980755805969, accuracy: 0.5435048937797546\n",
      "Batch 153 loss: 1.1954518556594849, accuracy: 0.5437804460525513\n",
      "Batch 154 loss: 1.1407017707824707, accuracy: 0.5441532135009766\n",
      "Batch 155 loss: 1.1669620275497437, accuracy: 0.5441706776618958\n",
      "Batch 156 loss: 1.149062156677246, accuracy: 0.5442874431610107\n",
      "Batch 157 loss: 0.9690622687339783, accuracy: 0.5447982549667358\n",
      "Batch 158 loss: 1.2153865098953247, accuracy: 0.5450569987297058\n",
      "Batch 159 loss: 1.2450228929519653, accuracy: 0.544873058795929\n",
      "Batch 160 loss: 1.1747771501541138, accuracy: 0.5451281070709229\n",
      "Batch 161 loss: 1.1451377868652344, accuracy: 0.5453799962997437\n",
      "Batch 162 loss: 1.2415574789047241, accuracy: 0.5453412532806396\n",
      "Batch 163 loss: 1.0458317995071411, accuracy: 0.5457793474197388\n",
      "Batch 164 loss: 1.266831874847412, accuracy: 0.5454545617103577\n",
      "Batch 165 loss: 1.1305676698684692, accuracy: 0.545651376247406\n",
      "Batch 166 loss: 1.1294536590576172, accuracy: 0.5456119179725647\n",
      "Batch 167 loss: 1.1298404932022095, accuracy: 0.545712411403656\n",
      "Batch 168 loss: 1.134264588356018, accuracy: 0.5454419255256653\n",
      "Batch 169 loss: 1.1820697784423828, accuracy: 0.545542299747467\n",
      "Batch 170 loss: 1.165277123451233, accuracy: 0.5450931787490845\n",
      "Batch 171 loss: 1.0226184129714966, accuracy: 0.5456486344337463\n",
      "Batch 172 loss: 1.1001312732696533, accuracy: 0.5457008481025696\n",
      "Batch 173 loss: 1.1587316989898682, accuracy: 0.5456178188323975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 174 loss: 1.3243205547332764, accuracy: 0.5451785922050476\n",
      "Batch 175 loss: 1.0577263832092285, accuracy: 0.5456321239471436\n",
      "Batch 176 loss: 1.1202794313430786, accuracy: 0.5454184412956238\n",
      "Batch 177 loss: 1.0204004049301147, accuracy: 0.5457777380943298\n",
      "Batch 178 loss: 1.166124701499939, accuracy: 0.5457838773727417\n",
      "Batch 179 loss: 1.3452671766281128, accuracy: 0.5457899570465088\n",
      "Batch 180 loss: 1.235692024230957, accuracy: 0.5456233024597168\n",
      "Batch 181 loss: 1.0280215740203857, accuracy: 0.5458447933197021\n",
      "Batch 182 loss: 1.223097801208496, accuracy: 0.5460211634635925\n",
      "Batch 183 loss: 1.1247012615203857, accuracy: 0.54615318775177\n",
      "Batch 184 loss: 1.1584237813949585, accuracy: 0.5460726618766785\n",
      "Batch 185 loss: 1.185705542564392, accuracy: 0.5460769534111023\n",
      "Batch 186 loss: 1.3040761947631836, accuracy: 0.5460394620895386\n",
      "Batch 187 loss: 1.1440609693527222, accuracy: 0.5458776354789734\n",
      "Batch 188 loss: 1.1750885248184204, accuracy: 0.5457175970077515\n",
      "Batch 189 loss: 1.2163746356964111, accuracy: 0.5457236766815186\n",
      "Batch 190 loss: 1.254355549812317, accuracy: 0.5456070303916931\n",
      "Batch 191 loss: 1.1973928213119507, accuracy: 0.5457763671875\n",
      "Batch 192 loss: 1.2004461288452148, accuracy: 0.5457415580749512\n",
      "Batch 193 loss: 1.1912221908569336, accuracy: 0.5456668734550476\n",
      "Batch 194 loss: 1.1532789468765259, accuracy: 0.5457131266593933\n",
      "Batch 195 loss: 1.4060797691345215, accuracy: 0.5450414419174194\n",
      "Batch 196 loss: 1.2429029941558838, accuracy: 0.5448921322822571\n",
      "Batch 197 loss: 1.2945035696029663, accuracy: 0.5446259379386902\n",
      "Batch 198 loss: 1.1712552309036255, accuracy: 0.5445587038993835\n",
      "Batch 199 loss: 1.0707740783691406, accuracy: 0.5446484088897705\n",
      "Batch 200 loss: 1.2965983152389526, accuracy: 0.5445429086685181\n",
      "Batch 201 loss: 1.106650710105896, accuracy: 0.5445931553840637\n",
      "Batch 202 loss: 1.2927440404891968, accuracy: 0.5445659160614014\n",
      "Batch 203 loss: 1.2144039869308472, accuracy: 0.5445771813392639\n",
      "Batch 204 loss: 1.162791132926941, accuracy: 0.5445883870124817\n",
      "Batch 205 loss: 1.1364692449569702, accuracy: 0.5447891354560852\n",
      "Batch 206 loss: 1.1047199964523315, accuracy: 0.5449124574661255\n",
      "Batch 207 loss: 1.2189996242523193, accuracy: 0.5449594259262085\n",
      "Batch 208 loss: 1.1977615356445312, accuracy: 0.5448564887046814\n",
      "Batch 209 loss: 1.1612693071365356, accuracy: 0.5447172522544861\n",
      "Batch 210 loss: 1.1812442541122437, accuracy: 0.544764518737793\n",
      "Batch 211 loss: 1.2346962690353394, accuracy: 0.5445165038108826\n",
      "Batch 212 loss: 1.2162777185440063, accuracy: 0.5445275902748108\n",
      "Batch 213 loss: 1.1847375631332397, accuracy: 0.5443925261497498\n",
      "Batch 214 loss: 1.0261764526367188, accuracy: 0.544767439365387\n",
      "Batch 215 loss: 1.117535948753357, accuracy: 0.5449580550193787\n",
      "Batch 216 loss: 1.3747855424880981, accuracy: 0.5445708632469177\n",
      "Batch 217 loss: 1.0298981666564941, accuracy: 0.5449398159980774\n",
      "Batch 218 loss: 1.0859218835830688, accuracy: 0.5449486374855042\n",
      "Batch 219 loss: 1.2308712005615234, accuracy: 0.5448153614997864\n",
      "Batch 220 loss: 1.0769758224487305, accuracy: 0.5451074838638306\n",
      "Batch 221 loss: 1.3180432319641113, accuracy: 0.544693112373352\n",
      "Batch 222 loss: 1.0611830949783325, accuracy: 0.544808030128479\n",
      "Batch 223 loss: 1.014059066772461, accuracy: 0.5450614094734192\n",
      "Batch 224 loss: 1.2632296085357666, accuracy: 0.54509037733078\n",
      "Training epoch: 6, train accuracy: 54.50904083251953, train loss: 1.191918133629693, valid accuracy: 57.48119354248047, valid loss: 1.143000943907376 \n",
      "Batch 0 loss: 1.1241698265075684, accuracy: 0.5546875\n",
      "Batch 1 loss: 1.1880042552947998, accuracy: 0.58984375\n",
      "Batch 2 loss: 1.1870148181915283, accuracy: 0.578125\n",
      "Batch 3 loss: 1.2971224784851074, accuracy: 0.5546875\n",
      "Batch 4 loss: 1.2584705352783203, accuracy: 0.543749988079071\n",
      "Batch 5 loss: 1.1562844514846802, accuracy: 0.55078125\n",
      "Batch 6 loss: 1.136125922203064, accuracy: 0.5580357313156128\n",
      "Batch 7 loss: 1.1816613674163818, accuracy: 0.5556640625\n",
      "Batch 8 loss: 1.1336485147476196, accuracy: 0.5598958134651184\n",
      "Batch 9 loss: 1.168584942817688, accuracy: 0.557812511920929\n",
      "Batch 10 loss: 1.1544795036315918, accuracy: 0.5575284361839294\n",
      "Batch 11 loss: 1.1933718919754028, accuracy: 0.5572916865348816\n",
      "Batch 12 loss: 1.2217276096343994, accuracy: 0.5564903616905212\n",
      "Batch 13 loss: 1.1044535636901855, accuracy: 0.5608258843421936\n",
      "Batch 14 loss: 1.0542938709259033, accuracy: 0.5661458373069763\n",
      "Batch 15 loss: 1.2252345085144043, accuracy: 0.5634765625\n",
      "Batch 16 loss: 1.204906940460205, accuracy: 0.5597426295280457\n",
      "Batch 17 loss: 1.18252432346344, accuracy: 0.55859375\n",
      "Batch 18 loss: 1.1511836051940918, accuracy: 0.5612664222717285\n",
      "Batch 19 loss: 1.190247654914856, accuracy: 0.5589843988418579\n",
      "Batch 20 loss: 1.142114281654358, accuracy: 0.5598958134651184\n",
      "Batch 21 loss: 1.2434465885162354, accuracy: 0.5571733117103577\n",
      "Batch 22 loss: 1.2995631694793701, accuracy: 0.5553668737411499\n",
      "Batch 23 loss: 1.1477524042129517, accuracy: 0.556640625\n",
      "Batch 24 loss: 1.1320322751998901, accuracy: 0.5571874976158142\n",
      "Batch 25 loss: 1.2153635025024414, accuracy: 0.5564903616905212\n",
      "Batch 26 loss: 1.276654839515686, accuracy: 0.5555555820465088\n",
      "Batch 27 loss: 1.0964540243148804, accuracy: 0.5574776530265808\n",
      "Batch 28 loss: 1.1621794700622559, accuracy: 0.5573814511299133\n",
      "Batch 29 loss: 1.059286117553711, accuracy: 0.559374988079071\n",
      "Batch 30 loss: 1.2708615064620972, accuracy: 0.5567036271095276\n",
      "Batch 31 loss: 1.1424076557159424, accuracy: 0.55615234375\n",
      "Batch 32 loss: 1.1084017753601074, accuracy: 0.5556344985961914\n",
      "Batch 33 loss: 1.2498284578323364, accuracy: 0.5542279481887817\n",
      "Batch 34 loss: 1.1421983242034912, accuracy: 0.5537946224212646\n",
      "Batch 35 loss: 1.1842390298843384, accuracy: 0.5516493320465088\n",
      "Batch 36 loss: 1.0534652471542358, accuracy: 0.5525760054588318\n",
      "Batch 37 loss: 1.1946672201156616, accuracy: 0.5520148277282715\n",
      "Batch 38 loss: 1.0052177906036377, accuracy: 0.5548878312110901\n",
      "Batch 39 loss: 1.163130283355713, accuracy: 0.5541015863418579\n",
      "Batch 40 loss: 1.2257308959960938, accuracy: 0.5531631112098694\n",
      "Batch 41 loss: 1.0766541957855225, accuracy: 0.5535714030265808\n",
      "Batch 42 loss: 1.4198644161224365, accuracy: 0.5535973906517029\n",
      "Batch 43 loss: 1.2656357288360596, accuracy: 0.5523792505264282\n",
      "Batch 44 loss: 1.2989652156829834, accuracy: 0.5501736402511597\n",
      "Batch 45 loss: 1.3081376552581787, accuracy: 0.549422562122345\n",
      "Batch 46 loss: 1.1437642574310303, accuracy: 0.5505319237709045\n",
      "Batch 47 loss: 1.2356438636779785, accuracy: 0.5491536259651184\n",
      "Batch 48 loss: 1.0669457912445068, accuracy: 0.5507015585899353\n",
      "Batch 49 loss: 1.2278995513916016, accuracy: 0.5509374737739563\n",
      "Batch 50 loss: 1.1023558378219604, accuracy: 0.5510110259056091\n",
      "Batch 51 loss: 1.1042864322662354, accuracy: 0.5513821840286255\n",
      "Batch 52 loss: 1.3296091556549072, accuracy: 0.5517393946647644\n",
      "Batch 53 loss: 1.197923183441162, accuracy: 0.55078125\n",
      "Batch 54 loss: 1.0924322605133057, accuracy: 0.551562488079071\n",
      "Batch 55 loss: 1.2028621435165405, accuracy: 0.5513392686843872\n",
      "Batch 56 loss: 1.18158757686615, accuracy: 0.5505756735801697\n",
      "Batch 57 loss: 1.1369551420211792, accuracy: 0.5509159564971924\n",
      "Batch 58 loss: 1.1109925508499146, accuracy: 0.5517743825912476\n",
      "Batch 59 loss: 1.2522969245910645, accuracy: 0.5513020753860474\n",
      "Batch 60 loss: 1.0528448820114136, accuracy: 0.5516137480735779\n",
      "Batch 61 loss: 1.1466337442398071, accuracy: 0.551537275314331\n",
      "Batch 62 loss: 1.1734086275100708, accuracy: 0.5512152910232544\n",
      "Batch 63 loss: 1.0661649703979492, accuracy: 0.55224609375\n",
      "Batch 64 loss: 1.1120047569274902, accuracy: 0.5527644157409668\n",
      "Batch 65 loss: 1.1693572998046875, accuracy: 0.5531486868858337\n",
      "Batch 66 loss: 1.2295676469802856, accuracy: 0.5530550479888916\n",
      "Batch 67 loss: 1.260322093963623, accuracy: 0.5522748231887817\n",
      "Batch 68 loss: 1.3507791757583618, accuracy: 0.5515171885490417\n",
      "Batch 69 loss: 1.0805394649505615, accuracy: 0.5520089268684387\n",
      "Batch 70 loss: 1.0062103271484375, accuracy: 0.5528169274330139\n",
      "Batch 71 loss: 1.1029703617095947, accuracy: 0.5529513955116272\n",
      "Batch 72 loss: 1.3232781887054443, accuracy: 0.5515838861465454\n",
      "Batch 73 loss: 1.2975332736968994, accuracy: 0.5500422120094299\n",
      "Batch 74 loss: 1.1888529062271118, accuracy: 0.5504166483879089\n",
      "Batch 75 loss: 1.1117149591445923, accuracy: 0.5503700375556946\n",
      "Batch 76 loss: 1.0797710418701172, accuracy: 0.5502232313156128\n",
      "Batch 77 loss: 1.2108445167541504, accuracy: 0.5500801205635071\n",
      "Batch 78 loss: 1.179184079170227, accuracy: 0.5498417615890503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 79 loss: 1.0737017393112183, accuracy: 0.5494140386581421\n",
      "Batch 80 loss: 1.2319738864898682, accuracy: 0.5489004850387573\n",
      "Batch 81 loss: 1.0971077680587769, accuracy: 0.5492568612098694\n",
      "Batch 82 loss: 1.2064845561981201, accuracy: 0.5488516688346863\n",
      "Batch 83 loss: 1.2416638135910034, accuracy: 0.5482701063156128\n",
      "Batch 84 loss: 1.0743578672409058, accuracy: 0.5485293865203857\n",
      "Batch 85 loss: 1.2414231300354004, accuracy: 0.5483284592628479\n",
      "Batch 86 loss: 1.1547242403030396, accuracy: 0.5484015941619873\n",
      "Batch 87 loss: 1.262503743171692, accuracy: 0.5483842492103577\n",
      "Batch 88 loss: 1.1480084657669067, accuracy: 0.5488939881324768\n",
      "Batch 89 loss: 1.1884791851043701, accuracy: 0.5485243201255798\n",
      "Batch 90 loss: 1.16099214553833, accuracy: 0.5486778616905212\n",
      "Batch 91 loss: 1.1456551551818848, accuracy: 0.54908287525177\n",
      "Batch 92 loss: 1.1388185024261475, accuracy: 0.5492271780967712\n",
      "Batch 93 loss: 1.096697211265564, accuracy: 0.5495345592498779\n",
      "Batch 94 loss: 1.271484375, accuracy: 0.5487664341926575\n",
      "Batch 95 loss: 0.9855061173439026, accuracy: 0.5502116084098816\n",
      "Batch 96 loss: 1.186942219734192, accuracy: 0.5503382682800293\n",
      "Batch 97 loss: 1.1219031810760498, accuracy: 0.550382673740387\n",
      "Batch 98 loss: 0.9682198762893677, accuracy: 0.5509785413742065\n",
      "Batch 99 loss: 1.1017534732818604, accuracy: 0.5510156154632568\n",
      "Batch 100 loss: 1.1303164958953857, accuracy: 0.5515934228897095\n",
      "Batch 101 loss: 1.1196584701538086, accuracy: 0.5521599054336548\n",
      "Batch 102 loss: 1.3094829320907593, accuracy: 0.5515776872634888\n",
      "Batch 103 loss: 1.1867834329605103, accuracy: 0.5513821840286255\n",
      "Batch 104 loss: 0.9918107986450195, accuracy: 0.5520833134651184\n",
      "Batch 105 loss: 1.0778197050094604, accuracy: 0.5526975393295288\n",
      "Batch 106 loss: 1.308070421218872, accuracy: 0.5522780418395996\n",
      "Batch 107 loss: 1.091840386390686, accuracy: 0.5529513955116272\n",
      "Batch 108 loss: 1.0771936178207397, accuracy: 0.553039014339447\n",
      "Batch 109 loss: 1.108276128768921, accuracy: 0.5529829263687134\n",
      "Batch 110 loss: 1.1771788597106934, accuracy: 0.5532798171043396\n",
      "Batch 111 loss: 1.2110986709594727, accuracy: 0.5530133843421936\n",
      "Batch 112 loss: 1.1339799165725708, accuracy: 0.5528208017349243\n",
      "Batch 113 loss: 1.1441545486450195, accuracy: 0.5538651347160339\n",
      "Batch 114 loss: 1.2116985321044922, accuracy: 0.553872287273407\n",
      "Batch 115 loss: 1.1037545204162598, accuracy: 0.5536099076271057\n",
      "Batch 116 loss: 1.0887154340744019, accuracy: 0.5534188151359558\n",
      "Batch 117 loss: 1.1674684286117554, accuracy: 0.5531647205352783\n",
      "Batch 118 loss: 1.0335620641708374, accuracy: 0.5534401535987854\n",
      "Batch 119 loss: 1.163487195968628, accuracy: 0.5532552003860474\n",
      "Batch 120 loss: 1.3109008073806763, accuracy: 0.5526213645935059\n",
      "Batch 121 loss: 1.088329792022705, accuracy: 0.5527023673057556\n",
      "Batch 122 loss: 1.0983493328094482, accuracy: 0.553036093711853\n",
      "Batch 123 loss: 1.0077327489852905, accuracy: 0.553931474685669\n",
      "Batch 124 loss: 1.1239943504333496, accuracy: 0.5540000200271606\n",
      "Batch 125 loss: 1.1182928085327148, accuracy: 0.5539434552192688\n",
      "Batch 126 loss: 1.1993181705474854, accuracy: 0.5536417365074158\n",
      "Batch 127 loss: 1.322145700454712, accuracy: 0.55303955078125\n",
      "Batch 128 loss: 1.0529110431671143, accuracy: 0.5536579489707947\n",
      "Batch 129 loss: 1.1732813119888306, accuracy: 0.5539663434028625\n",
      "Batch 130 loss: 1.1582502126693726, accuracy: 0.5540911555290222\n",
      "Batch 131 loss: 1.083909034729004, accuracy: 0.5541548132896423\n",
      "Batch 132 loss: 1.1826492547988892, accuracy: 0.5545112490653992\n",
      "Batch 133 loss: 1.186974287033081, accuracy: 0.5543959736824036\n",
      "Batch 134 loss: 1.094382882118225, accuracy: 0.5546296238899231\n",
      "Batch 135 loss: 1.0430330038070679, accuracy: 0.5552045106887817\n",
      "Batch 136 loss: 1.131860613822937, accuracy: 0.5554858446121216\n",
      "Batch 137 loss: 1.152623176574707, accuracy: 0.5553668737411499\n",
      "Batch 138 loss: 0.9538583755493164, accuracy: 0.5560926198959351\n",
      "Batch 139 loss: 1.11859929561615, accuracy: 0.5561941862106323\n",
      "Batch 140 loss: 1.133700966835022, accuracy: 0.5558510422706604\n",
      "Batch 141 loss: 1.089569330215454, accuracy: 0.5557878613471985\n",
      "Batch 142 loss: 1.1231251955032349, accuracy: 0.5557801723480225\n",
      "Batch 143 loss: 1.406907320022583, accuracy: 0.5550130009651184\n",
      "Batch 144 loss: 1.0001507997512817, accuracy: 0.5552262663841248\n",
      "Batch 145 loss: 1.0610804557800293, accuracy: 0.5555436611175537\n",
      "Batch 146 loss: 1.2522482872009277, accuracy: 0.5551126599311829\n",
      "Batch 147 loss: 1.3310410976409912, accuracy: 0.5546875\n",
      "Batch 148 loss: 1.2240766286849976, accuracy: 0.5546350479125977\n",
      "Batch 149 loss: 1.1211577653884888, accuracy: 0.5550000071525574\n",
      "Batch 150 loss: 1.1164792776107788, accuracy: 0.5548944473266602\n",
      "Batch 151 loss: 1.099195957183838, accuracy: 0.5552014708518982\n",
      "Batch 152 loss: 1.0751471519470215, accuracy: 0.555300235748291\n",
      "Batch 153 loss: 1.185630440711975, accuracy: 0.5552962422370911\n",
      "Batch 154 loss: 1.014844298362732, accuracy: 0.5556955933570862\n",
      "Batch 155 loss: 1.203857660293579, accuracy: 0.555588960647583\n",
      "Batch 156 loss: 1.2074276208877563, accuracy: 0.5557324886322021\n",
      "Batch 157 loss: 1.0067936182022095, accuracy: 0.5562203526496887\n",
      "Batch 158 loss: 1.013363003730774, accuracy: 0.5565055012702942\n",
      "Batch 159 loss: 1.0072962045669556, accuracy: 0.556884765625\n",
      "Batch 160 loss: 1.1921701431274414, accuracy: 0.5567255616188049\n",
      "Batch 161 loss: 1.260611653327942, accuracy: 0.5568094253540039\n",
      "Batch 162 loss: 1.2566578388214111, accuracy: 0.5566046833992004\n",
      "Batch 163 loss: 1.1014900207519531, accuracy: 0.5565930008888245\n",
      "Batch 164 loss: 1.215410828590393, accuracy: 0.5563446879386902\n",
      "Batch 165 loss: 1.2199891805648804, accuracy: 0.5561935305595398\n",
      "Batch 166 loss: 1.075292944908142, accuracy: 0.5565587282180786\n",
      "Batch 167 loss: 1.0986357927322388, accuracy: 0.5568731427192688\n",
      "Batch 168 loss: 1.1253845691680908, accuracy: 0.5572762489318848\n",
      "Batch 169 loss: 1.150972604751587, accuracy: 0.5577665567398071\n",
      "Batch 170 loss: 1.2184301614761353, accuracy: 0.5579313039779663\n",
      "Batch 171 loss: 1.1333098411560059, accuracy: 0.5579124093055725\n",
      "Batch 172 loss: 1.159254789352417, accuracy: 0.5578034520149231\n",
      "Batch 173 loss: 0.9865527153015137, accuracy: 0.5581896305084229\n",
      "Batch 174 loss: 1.3163244724273682, accuracy: 0.5579464435577393\n",
      "Batch 175 loss: 1.0791311264038086, accuracy: 0.5579723119735718\n",
      "Batch 176 loss: 1.1858320236206055, accuracy: 0.557644784450531\n",
      "Batch 177 loss: 1.117192029953003, accuracy: 0.5579354166984558\n",
      "Batch 178 loss: 1.1436595916748047, accuracy: 0.5578299760818481\n",
      "Batch 179 loss: 1.1121102571487427, accuracy: 0.5579426884651184\n",
      "Batch 180 loss: 1.162066102027893, accuracy: 0.557924747467041\n",
      "Batch 181 loss: 1.3628913164138794, accuracy: 0.557348906993866\n",
      "Batch 182 loss: 1.0823194980621338, accuracy: 0.5573770403862\n",
      "Batch 183 loss: 1.1678905487060547, accuracy: 0.5576596260070801\n",
      "Batch 184 loss: 1.086564064025879, accuracy: 0.5576435923576355\n",
      "Batch 185 loss: 1.2037898302078247, accuracy: 0.5573336482048035\n",
      "Batch 186 loss: 1.1410874128341675, accuracy: 0.5572359561920166\n",
      "Batch 187 loss: 1.0135259628295898, accuracy: 0.5574302077293396\n",
      "Batch 188 loss: 1.1575425863265991, accuracy: 0.5573329925537109\n",
      "Batch 189 loss: 1.244030237197876, accuracy: 0.5571957230567932\n",
      "Batch 190 loss: 1.1144747734069824, accuracy: 0.5572643876075745\n",
      "Batch 191 loss: 1.1125404834747314, accuracy: 0.5574544072151184\n",
      "Batch 192 loss: 1.2545404434204102, accuracy: 0.5573186278343201\n",
      "Batch 193 loss: 1.0865142345428467, accuracy: 0.5575467348098755\n",
      "Batch 194 loss: 1.1088570356369019, accuracy: 0.557732343673706\n",
      "Batch 195 loss: 1.1453005075454712, accuracy: 0.5577168464660645\n",
      "Batch 196 loss: 1.1964224576950073, accuracy: 0.5575428009033203\n",
      "Batch 197 loss: 1.1685833930969238, accuracy: 0.5577256679534912\n",
      "Batch 198 loss: 1.1442457437515259, accuracy: 0.5578281879425049\n",
      "Batch 199 loss: 1.0179036855697632, accuracy: 0.5582031011581421\n",
      "Batch 200 loss: 1.0479962825775146, accuracy: 0.5584577322006226\n",
      "Batch 201 loss: 1.0279158353805542, accuracy: 0.5587871074676514\n",
      "Batch 202 loss: 1.1464534997940063, accuracy: 0.5589208602905273\n",
      "Batch 203 loss: 1.4377570152282715, accuracy: 0.55859375\n",
      "Batch 204 loss: 1.0415655374526978, accuracy: 0.5587271451950073\n",
      "Batch 205 loss: 1.22927725315094, accuracy: 0.5588592290878296\n",
      "Batch 206 loss: 1.1700438261032104, accuracy: 0.5588390827178955\n",
      "Batch 207 loss: 1.027000904083252, accuracy: 0.5591946840286255\n",
      "Batch 208 loss: 1.1817747354507446, accuracy: 0.5591357946395874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 209 loss: 1.01291024684906, accuracy: 0.5595982074737549\n",
      "Batch 210 loss: 1.1379071474075317, accuracy: 0.5596489906311035\n",
      "Batch 211 loss: 1.1904128789901733, accuracy: 0.5595518946647644\n",
      "Batch 212 loss: 1.0437211990356445, accuracy: 0.5596757531166077\n",
      "Batch 213 loss: 1.0421291589736938, accuracy: 0.5599079728126526\n",
      "Batch 214 loss: 1.051590085029602, accuracy: 0.5599927306175232\n",
      "Batch 215 loss: 1.173803687095642, accuracy: 0.5601490139961243\n",
      "Batch 216 loss: 1.0131945610046387, accuracy: 0.5603758692741394\n",
      "Batch 217 loss: 1.099494457244873, accuracy: 0.5605289340019226\n",
      "Batch 218 loss: 1.0152111053466797, accuracy: 0.5605379343032837\n",
      "Batch 219 loss: 1.0165352821350098, accuracy: 0.5606889128684998\n",
      "Batch 220 loss: 1.0834661722183228, accuracy: 0.5607324838638306\n",
      "Batch 221 loss: 1.1873739957809448, accuracy: 0.5607052445411682\n",
      "Batch 222 loss: 1.1427537202835083, accuracy: 0.5607483386993408\n",
      "Batch 223 loss: 1.1257268190383911, accuracy: 0.5608607530593872\n",
      "Batch 224 loss: 1.4057725667953491, accuracy: 0.5607649087905884\n",
      "Training epoch: 7, train accuracy: 56.07649230957031, train loss: 1.1524703600671555, valid accuracy: 57.704097747802734, valid loss: 1.1179302704745326 \n",
      "Batch 0 loss: 1.0618914365768433, accuracy: 0.5859375\n",
      "Batch 1 loss: 1.0262755155563354, accuracy: 0.5859375\n",
      "Batch 2 loss: 1.1023719310760498, accuracy: 0.578125\n",
      "Batch 3 loss: 1.1652235984802246, accuracy: 0.5703125\n",
      "Batch 4 loss: 1.1232463121414185, accuracy: 0.5640624761581421\n",
      "Batch 5 loss: 1.150262475013733, accuracy: 0.56640625\n",
      "Batch 6 loss: 1.3429851531982422, accuracy: 0.5535714030265808\n",
      "Batch 7 loss: 1.2516810894012451, accuracy: 0.5478515625\n",
      "Batch 8 loss: 1.1857023239135742, accuracy: 0.5477430820465088\n",
      "Batch 9 loss: 1.022882103919983, accuracy: 0.557812511920929\n",
      "Batch 10 loss: 1.2089067697525024, accuracy: 0.5589488744735718\n",
      "Batch 11 loss: 1.0430299043655396, accuracy: 0.5625\n",
      "Batch 12 loss: 1.1487410068511963, accuracy: 0.5643028616905212\n",
      "Batch 13 loss: 1.154394507408142, accuracy: 0.5647321343421936\n",
      "Batch 14 loss: 1.023349404335022, accuracy: 0.5687500238418579\n",
      "Batch 15 loss: 1.1272567510604858, accuracy: 0.57080078125\n",
      "Batch 16 loss: 1.0932657718658447, accuracy: 0.5749080777168274\n",
      "Batch 17 loss: 1.0477904081344604, accuracy: 0.5776909589767456\n",
      "Batch 18 loss: 1.0899124145507812, accuracy: 0.5785362124443054\n",
      "Batch 19 loss: 1.1114072799682617, accuracy: 0.578906238079071\n",
      "Batch 20 loss: 1.2241989374160767, accuracy: 0.574404776096344\n",
      "Batch 21 loss: 1.042087435722351, accuracy: 0.5756391882896423\n",
      "Batch 22 loss: 1.0121229887008667, accuracy: 0.5771059989929199\n",
      "Batch 23 loss: 1.1106737852096558, accuracy: 0.5764973759651184\n",
      "Batch 24 loss: 1.0744258165359497, accuracy: 0.5759375095367432\n",
      "Batch 25 loss: 1.0362348556518555, accuracy: 0.5784254670143127\n",
      "Batch 26 loss: 0.9953686594963074, accuracy: 0.5787037014961243\n",
      "Batch 27 loss: 1.0233516693115234, accuracy: 0.5792410969734192\n",
      "Batch 28 loss: 1.246473789215088, accuracy: 0.5773168206214905\n",
      "Batch 29 loss: 1.1051276922225952, accuracy: 0.5770833492279053\n",
      "Batch 30 loss: 1.0752956867218018, accuracy: 0.5768648982048035\n",
      "Batch 31 loss: 1.1424214839935303, accuracy: 0.5771484375\n",
      "Batch 32 loss: 1.1368814706802368, accuracy: 0.5767045617103577\n",
      "Batch 33 loss: 1.1346843242645264, accuracy: 0.5767463445663452\n",
      "Batch 34 loss: 1.2039158344268799, accuracy: 0.574999988079071\n",
      "Batch 35 loss: 1.1530423164367676, accuracy: 0.5746527910232544\n",
      "Batch 36 loss: 1.138420820236206, accuracy: 0.5753800868988037\n",
      "Batch 37 loss: 1.1292270421981812, accuracy: 0.5746299624443054\n",
      "Batch 38 loss: 1.1340872049331665, accuracy: 0.575120210647583\n",
      "Batch 39 loss: 1.2137444019317627, accuracy: 0.574023425579071\n",
      "Batch 40 loss: 1.2783615589141846, accuracy: 0.5714557766914368\n",
      "Batch 41 loss: 1.1760811805725098, accuracy: 0.5714285969734192\n",
      "Batch 42 loss: 1.0873931646347046, accuracy: 0.5710392594337463\n",
      "Batch 43 loss: 1.2240071296691895, accuracy: 0.5688920617103577\n",
      "Batch 44 loss: 1.0207651853561401, accuracy: 0.5699653029441833\n",
      "Batch 45 loss: 1.2536661624908447, accuracy: 0.5674252510070801\n",
      "Batch 46 loss: 1.1285595893859863, accuracy: 0.5679853558540344\n",
      "Batch 47 loss: 0.9784291982650757, accuracy: 0.5690104365348816\n",
      "Batch 48 loss: 1.0642781257629395, accuracy: 0.5703125\n",
      "Batch 49 loss: 0.9896723628044128, accuracy: 0.5723437666893005\n",
      "Batch 50 loss: 1.2438921928405762, accuracy: 0.5718443393707275\n",
      "Batch 51 loss: 1.1306936740875244, accuracy: 0.5719651579856873\n",
      "Batch 52 loss: 1.0974119901657104, accuracy: 0.5723761916160583\n",
      "Batch 53 loss: 1.1915338039398193, accuracy: 0.5720486044883728\n",
      "Batch 54 loss: 1.082045078277588, accuracy: 0.5725852251052856\n",
      "Batch 55 loss: 1.0567671060562134, accuracy: 0.5735211968421936\n",
      "Batch 56 loss: 1.1132954359054565, accuracy: 0.5745614171028137\n",
      "Batch 57 loss: 1.2922394275665283, accuracy: 0.5739493370056152\n",
      "Batch 58 loss: 1.187219500541687, accuracy: 0.5732256174087524\n",
      "Batch 59 loss: 1.090795874595642, accuracy: 0.5729166865348816\n",
      "Batch 60 loss: 1.1495721340179443, accuracy: 0.5726178288459778\n",
      "Batch 61 loss: 1.135315179824829, accuracy: 0.5722026228904724\n",
      "Batch 62 loss: 1.0095375776290894, accuracy: 0.5729166865348816\n",
      "Batch 63 loss: 1.1796903610229492, accuracy: 0.5726318359375\n",
      "Batch 64 loss: 1.2754549980163574, accuracy: 0.5721153616905212\n",
      "Batch 65 loss: 1.0380531549453735, accuracy: 0.5727983117103577\n",
      "Batch 66 loss: 1.0336132049560547, accuracy: 0.5738106369972229\n",
      "Batch 67 loss: 1.1094465255737305, accuracy: 0.574333667755127\n",
      "Batch 68 loss: 1.228866457939148, accuracy: 0.57370924949646\n",
      "Batch 69 loss: 1.109967589378357, accuracy: 0.5737723112106323\n",
      "Batch 70 loss: 1.1796356439590454, accuracy: 0.5735034942626953\n",
      "Batch 71 loss: 1.0581179857254028, accuracy: 0.5734592080116272\n",
      "Batch 72 loss: 1.283247947692871, accuracy: 0.5720248222351074\n",
      "Batch 73 loss: 1.0884088277816772, accuracy: 0.572212815284729\n",
      "Batch 74 loss: 1.0045663118362427, accuracy: 0.5733333230018616\n",
      "Batch 75 loss: 1.2750720977783203, accuracy: 0.5727795958518982\n",
      "Batch 76 loss: 1.1470786333084106, accuracy: 0.5736607313156128\n",
      "Batch 77 loss: 1.1098185777664185, accuracy: 0.5736178159713745\n",
      "Batch 78 loss: 1.0511354207992554, accuracy: 0.5733781456947327\n",
      "Batch 79 loss: 1.1215413808822632, accuracy: 0.5728515386581421\n",
      "Batch 80 loss: 1.2848155498504639, accuracy: 0.5716627836227417\n",
      "Batch 81 loss: 1.0069687366485596, accuracy: 0.5715510845184326\n",
      "Batch 82 loss: 1.0913158655166626, accuracy: 0.5716302990913391\n",
      "Batch 83 loss: 1.0382654666900635, accuracy: 0.5715215802192688\n",
      "Batch 84 loss: 1.2536534070968628, accuracy: 0.5711396932601929\n",
      "Batch 85 loss: 1.0302830934524536, accuracy: 0.5718568563461304\n",
      "Batch 86 loss: 0.9549576044082642, accuracy: 0.5724676847457886\n",
      "Batch 87 loss: 1.3587541580200195, accuracy: 0.5707563757896423\n",
      "Batch 88 loss: 1.2134690284729004, accuracy: 0.5700491666793823\n",
      "Batch 89 loss: 1.0454221963882446, accuracy: 0.570399284362793\n",
      "Batch 90 loss: 1.226460576057434, accuracy: 0.569969117641449\n",
      "Batch 91 loss: 1.151597023010254, accuracy: 0.5695482492446899\n",
      "Batch 92 loss: 1.0133849382400513, accuracy: 0.5699764490127563\n",
      "Batch 93 loss: 1.1155176162719727, accuracy: 0.5696476101875305\n",
      "Batch 94 loss: 1.147314429283142, accuracy: 0.5699835419654846\n",
      "Batch 95 loss: 1.0264599323272705, accuracy: 0.5704752802848816\n",
      "Batch 96 loss: 1.165256381034851, accuracy: 0.5711179375648499\n",
      "Batch 97 loss: 1.1350584030151367, accuracy: 0.5714285969734192\n",
      "Batch 98 loss: 1.101097583770752, accuracy: 0.5711805820465088\n",
      "Batch 99 loss: 0.9666444659233093, accuracy: 0.5711718797683716\n",
      "Batch 100 loss: 1.013522744178772, accuracy: 0.5721689462661743\n",
      "Batch 101 loss: 1.1078777313232422, accuracy: 0.5724571347236633\n",
      "Batch 102 loss: 1.151439905166626, accuracy: 0.5722087621688843\n",
      "Batch 103 loss: 1.159956693649292, accuracy: 0.5718899965286255\n",
      "Batch 104 loss: 1.2961102724075317, accuracy: 0.5715029835700989\n",
      "Batch 105 loss: 1.2105064392089844, accuracy: 0.5713443160057068\n",
      "Batch 106 loss: 1.2411178350448608, accuracy: 0.570823609828949\n",
      "Batch 107 loss: 1.0915546417236328, accuracy: 0.5712528824806213\n",
      "Batch 108 loss: 1.2593623399734497, accuracy: 0.5707425475120544\n",
      "Batch 109 loss: 1.0289289951324463, accuracy: 0.5715199112892151\n",
      "Batch 110 loss: 1.1416292190551758, accuracy: 0.5715090036392212\n",
      "Batch 111 loss: 0.9844338893890381, accuracy: 0.5720563530921936\n",
      "Batch 112 loss: 1.1194881200790405, accuracy: 0.5722483396530151\n",
      "Batch 113 loss: 1.1967982053756714, accuracy: 0.5720257759094238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 114 loss: 1.1477611064910889, accuracy: 0.571739137172699\n",
      "Batch 115 loss: 1.080739974975586, accuracy: 0.571928858757019\n",
      "Batch 116 loss: 1.0792663097381592, accuracy: 0.572449266910553\n",
      "Batch 117 loss: 0.9065108895301819, accuracy: 0.5732918381690979\n",
      "Batch 118 loss: 1.1655746698379517, accuracy: 0.5733324289321899\n",
      "Batch 119 loss: 0.9664541482925415, accuracy: 0.5736328363418579\n",
      "Batch 120 loss: 1.0841459035873413, accuracy: 0.5737345218658447\n",
      "Batch 121 loss: 1.0752859115600586, accuracy: 0.5741547346115112\n",
      "Batch 122 loss: 1.2769759893417358, accuracy: 0.5731707215309143\n",
      "Batch 123 loss: 1.1677701473236084, accuracy: 0.5728956460952759\n",
      "Batch 124 loss: 1.1876399517059326, accuracy: 0.5731250047683716\n",
      "Batch 125 loss: 1.110507845878601, accuracy: 0.5733506679534912\n",
      "Batch 126 loss: 1.1461548805236816, accuracy: 0.5735728144645691\n",
      "Batch 127 loss: 1.406826138496399, accuracy: 0.5728759765625\n",
      "Batch 128 loss: 1.226481556892395, accuracy: 0.5728561282157898\n",
      "Batch 129 loss: 1.1050533056259155, accuracy: 0.5727764368057251\n",
      "Batch 130 loss: 1.033495545387268, accuracy: 0.5729365348815918\n",
      "Batch 131 loss: 1.0949426889419556, accuracy: 0.5735085010528564\n",
      "Batch 132 loss: 1.2671549320220947, accuracy: 0.5730145573616028\n",
      "Batch 133 loss: 1.0035799741744995, accuracy: 0.5729944109916687\n",
      "Batch 134 loss: 0.9944769144058228, accuracy: 0.5733217597007751\n",
      "Batch 135 loss: 1.1696768999099731, accuracy: 0.5735294222831726\n",
      "Batch 136 loss: 1.163952112197876, accuracy: 0.573391854763031\n",
      "Batch 137 loss: 1.0907214879989624, accuracy: 0.5735960006713867\n",
      "Batch 138 loss: 1.2358478307724, accuracy: 0.5734599828720093\n",
      "Batch 139 loss: 1.2010774612426758, accuracy: 0.5732700824737549\n",
      "Batch 140 loss: 0.9514206647872925, accuracy: 0.573803186416626\n",
      "Batch 141 loss: 1.2089709043502808, accuracy: 0.5738336443901062\n",
      "Batch 142 loss: 1.0631895065307617, accuracy: 0.5743007063865662\n",
      "Batch 143 loss: 1.1462563276290894, accuracy: 0.5739474892616272\n",
      "Batch 144 loss: 1.0494508743286133, accuracy: 0.5740301609039307\n",
      "Batch 145 loss: 1.188568115234375, accuracy: 0.5737371444702148\n",
      "Batch 146 loss: 1.1375775337219238, accuracy: 0.5736607313156128\n",
      "Batch 147 loss: 1.1387577056884766, accuracy: 0.5733213424682617\n",
      "Batch 148 loss: 1.3627668619155884, accuracy: 0.5726195573806763\n",
      "Batch 149 loss: 1.0582784414291382, accuracy: 0.5724478960037231\n",
      "Batch 150 loss: 1.0630520582199097, accuracy: 0.5727959275245667\n",
      "Batch 151 loss: 1.1088353395462036, accuracy: 0.5731393694877625\n",
      "Batch 152 loss: 1.0963371992111206, accuracy: 0.5732230544090271\n",
      "Batch 153 loss: 1.2258598804473877, accuracy: 0.5728490352630615\n",
      "Batch 154 loss: 1.148053765296936, accuracy: 0.5727318525314331\n",
      "Batch 155 loss: 1.094968318939209, accuracy: 0.5729166865348816\n",
      "Batch 156 loss: 1.2163547277450562, accuracy: 0.5723029375076294\n",
      "Batch 157 loss: 0.9679442048072815, accuracy: 0.5726364850997925\n",
      "Batch 158 loss: 1.0706452131271362, accuracy: 0.5730640888214111\n",
      "Batch 159 loss: 1.1015915870666504, accuracy: 0.572949230670929\n",
      "Batch 160 loss: 1.1534725427627563, accuracy: 0.5725931525230408\n",
      "Batch 161 loss: 1.0991877317428589, accuracy: 0.5729166865348816\n",
      "Batch 162 loss: 1.1090892553329468, accuracy: 0.5726610422134399\n",
      "Batch 163 loss: 1.1752197742462158, accuracy: 0.5727419853210449\n",
      "Batch 164 loss: 0.9618929624557495, accuracy: 0.5732480883598328\n",
      "Batch 165 loss: 1.1958478689193726, accuracy: 0.5732304453849792\n",
      "Batch 166 loss: 1.020281434059143, accuracy: 0.5736807584762573\n",
      "Batch 167 loss: 1.1627446413040161, accuracy: 0.573614239692688\n",
      "Batch 168 loss: 1.116851568222046, accuracy: 0.5736408829689026\n",
      "Batch 169 loss: 1.1648530960083008, accuracy: 0.573437511920929\n",
      "Batch 170 loss: 1.0365798473358154, accuracy: 0.5735563039779663\n",
      "Batch 171 loss: 1.1946707963943481, accuracy: 0.5734465718269348\n",
      "Batch 172 loss: 1.1550164222717285, accuracy: 0.5736542344093323\n",
      "Batch 173 loss: 1.086625337600708, accuracy: 0.5739044547080994\n",
      "Batch 174 loss: 1.173028588294983, accuracy: 0.5738393068313599\n",
      "Batch 175 loss: 1.1964869499206543, accuracy: 0.5737748742103577\n",
      "Batch 176 loss: 1.137615442276001, accuracy: 0.5738435983657837\n",
      "Batch 177 loss: 1.1381340026855469, accuracy: 0.5738237500190735\n",
      "Batch 178 loss: 1.143018364906311, accuracy: 0.5735858678817749\n",
      "Batch 179 loss: 1.0555849075317383, accuracy: 0.5736979246139526\n",
      "Batch 180 loss: 1.1787952184677124, accuracy: 0.5736792087554932\n",
      "Batch 181 loss: 1.1497191190719604, accuracy: 0.5736178159713745\n",
      "Batch 182 loss: 1.099997639656067, accuracy: 0.5737704634666443\n",
      "Batch 183 loss: 1.0661735534667969, accuracy: 0.5740489363670349\n",
      "Batch 184 loss: 1.1625183820724487, accuracy: 0.5739020109176636\n",
      "Batch 185 loss: 1.0288830995559692, accuracy: 0.5740507245063782\n",
      "Batch 186 loss: 1.1439555883407593, accuracy: 0.574072539806366\n",
      "Batch 187 loss: 1.0571753978729248, accuracy: 0.5740525126457214\n",
      "Batch 188 loss: 1.060377597808838, accuracy: 0.5742807388305664\n",
      "Batch 189 loss: 1.148292064666748, accuracy: 0.5743421316146851\n",
      "Batch 190 loss: 0.9484078884124756, accuracy: 0.5747300386428833\n",
      "Batch 191 loss: 1.0453392267227173, accuracy: 0.5745849609375\n",
      "Batch 192 loss: 1.130768895149231, accuracy: 0.5747652053833008\n",
      "Batch 193 loss: 1.0954176187515259, accuracy: 0.5749033689498901\n",
      "Batch 194 loss: 1.030785322189331, accuracy: 0.5752003192901611\n",
      "Batch 195 loss: 1.0437475442886353, accuracy: 0.5750956535339355\n",
      "Batch 196 loss: 1.1792985200881958, accuracy: 0.5750316977500916\n",
      "Batch 197 loss: 1.2224291563034058, accuracy: 0.5750473737716675\n",
      "Batch 198 loss: 1.1716612577438354, accuracy: 0.5749843120574951\n",
      "Batch 199 loss: 1.1196708679199219, accuracy: 0.5748828053474426\n",
      "Batch 200 loss: 0.9462530612945557, accuracy: 0.5751321315765381\n",
      "Batch 201 loss: 1.1492643356323242, accuracy: 0.5749536156654358\n",
      "Batch 202 loss: 1.165364146232605, accuracy: 0.5750076770782471\n",
      "Batch 203 loss: 1.1919928789138794, accuracy: 0.5746783018112183\n",
      "Batch 204 loss: 1.0990058183670044, accuracy: 0.5746188759803772\n",
      "Batch 205 loss: 1.222412347793579, accuracy: 0.5742566585540771\n",
      "Batch 206 loss: 1.0792977809906006, accuracy: 0.5743131041526794\n",
      "Batch 207 loss: 1.2014696598052979, accuracy: 0.5740684866905212\n",
      "Batch 208 loss: 1.0805001258850098, accuracy: 0.574349582195282\n",
      "Batch 209 loss: 1.0810502767562866, accuracy: 0.574590802192688\n",
      "Batch 210 loss: 0.9758303761482239, accuracy: 0.5747556090354919\n",
      "Batch 211 loss: 1.2701411247253418, accuracy: 0.5744029879570007\n",
      "Batch 212 loss: 1.019403100013733, accuracy: 0.5747506022453308\n",
      "Batch 213 loss: 1.2095531225204468, accuracy: 0.5744742751121521\n",
      "Batch 214 loss: 1.1022223234176636, accuracy: 0.5741642713546753\n",
      "Batch 215 loss: 1.263102650642395, accuracy: 0.5740017294883728\n",
      "Batch 216 loss: 1.1401530504226685, accuracy: 0.5737687349319458\n",
      "Batch 217 loss: 0.9622277021408081, accuracy: 0.5739320516586304\n",
      "Batch 218 loss: 1.0674573183059692, accuracy: 0.5741652250289917\n",
      "Batch 219 loss: 1.1515063047409058, accuracy: 0.574431836605072\n",
      "Batch 220 loss: 0.9056863784790039, accuracy: 0.5750141143798828\n",
      "Batch 221 loss: 1.1018157005310059, accuracy: 0.5748873949050903\n",
      "Batch 222 loss: 1.057309627532959, accuracy: 0.5750770568847656\n",
      "Batch 223 loss: 1.168761968612671, accuracy: 0.5751255750656128\n",
      "Batch 224 loss: 0.9912492036819458, accuracy: 0.5751854777336121\n",
      "Training epoch: 8, train accuracy: 57.51854705810547, train loss: 1.1216294813156127, valid accuracy: 58.76288604736328, valid loss: 1.0609550876864071 \n",
      "Batch 0 loss: 1.0579197406768799, accuracy: 0.5859375\n",
      "Batch 1 loss: 1.0784581899642944, accuracy: 0.5859375\n",
      "Batch 2 loss: 1.0578794479370117, accuracy: 0.5963541865348816\n",
      "Batch 3 loss: 1.0634875297546387, accuracy: 0.599609375\n",
      "Batch 4 loss: 1.0685561895370483, accuracy: 0.5953124761581421\n",
      "Batch 5 loss: 1.2331817150115967, accuracy: 0.578125\n",
      "Batch 6 loss: 0.9062472581863403, accuracy: 0.5892857313156128\n",
      "Batch 7 loss: 1.1459180116653442, accuracy: 0.5830078125\n",
      "Batch 8 loss: 1.0703175067901611, accuracy: 0.5798611044883728\n",
      "Batch 9 loss: 1.1822742223739624, accuracy: 0.57421875\n",
      "Batch 10 loss: 1.083740472793579, accuracy: 0.5752840638160706\n",
      "Batch 11 loss: 1.1375426054000854, accuracy: 0.5735676884651184\n",
      "Batch 12 loss: 1.1206854581832886, accuracy: 0.5763221383094788\n",
      "Batch 13 loss: 1.0062689781188965, accuracy: 0.5797991156578064\n",
      "Batch 14 loss: 1.1069560050964355, accuracy: 0.5817708373069763\n",
      "Batch 15 loss: 1.0218021869659424, accuracy: 0.5849609375\n",
      "Batch 16 loss: 1.0718168020248413, accuracy: 0.5850183963775635\n",
      "Batch 17 loss: 0.96785569190979, accuracy: 0.5876736044883728\n",
      "Batch 18 loss: 1.0073013305664062, accuracy: 0.5904605388641357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 loss: 1.1378487348556519, accuracy: 0.58984375\n",
      "Batch 20 loss: 0.9339660406112671, accuracy: 0.5930059552192688\n",
      "Batch 21 loss: 1.124302864074707, accuracy: 0.5923295617103577\n",
      "Batch 22 loss: 1.1339329481124878, accuracy: 0.5900135636329651\n",
      "Batch 23 loss: 0.9626120924949646, accuracy: 0.5895182490348816\n",
      "Batch 24 loss: 1.0555026531219482, accuracy: 0.5896875262260437\n",
      "Batch 25 loss: 1.092969536781311, accuracy: 0.5874398946762085\n",
      "Batch 26 loss: 1.1342275142669678, accuracy: 0.5876736044883728\n",
      "Batch 27 loss: 0.992784321308136, accuracy: 0.587890625\n",
      "Batch 28 loss: 1.0107932090759277, accuracy: 0.5880926847457886\n",
      "Batch 29 loss: 1.203086018562317, accuracy: 0.5872395634651184\n",
      "Batch 30 loss: 1.0038191080093384, accuracy: 0.5884576439857483\n",
      "Batch 31 loss: 1.1185332536697388, accuracy: 0.58642578125\n",
      "Batch 32 loss: 1.1881797313690186, accuracy: 0.5849905014038086\n",
      "Batch 33 loss: 1.0148214101791382, accuracy: 0.5852481722831726\n",
      "Batch 34 loss: 1.0937440395355225, accuracy: 0.5854910612106323\n",
      "Batch 35 loss: 1.1759063005447388, accuracy: 0.5842013955116272\n",
      "Batch 36 loss: 0.9832326173782349, accuracy: 0.5859375\n",
      "Batch 37 loss: 1.0262354612350464, accuracy: 0.5865542888641357\n",
      "Batch 38 loss: 1.055182695388794, accuracy: 0.5873397588729858\n",
      "Batch 39 loss: 0.920733630657196, accuracy: 0.588085949420929\n",
      "Batch 40 loss: 1.0799875259399414, accuracy: 0.5887957215309143\n",
      "Batch 41 loss: 1.015350341796875, accuracy: 0.5890997052192688\n",
      "Batch 42 loss: 1.0128717422485352, accuracy: 0.5901162624359131\n",
      "Batch 43 loss: 1.0386102199554443, accuracy: 0.5900213122367859\n",
      "Batch 44 loss: 1.2056294679641724, accuracy: 0.590624988079071\n",
      "Batch 45 loss: 1.1936671733856201, accuracy: 0.58933424949646\n",
      "Batch 46 loss: 1.0239052772521973, accuracy: 0.5899268388748169\n",
      "Batch 47 loss: 1.1033620834350586, accuracy: 0.58984375\n",
      "Batch 48 loss: 0.9483211636543274, accuracy: 0.5910395383834839\n",
      "Batch 49 loss: 0.9937350749969482, accuracy: 0.5914062261581421\n",
      "Batch 50 loss: 1.0885695219039917, accuracy: 0.5916053652763367\n",
      "Batch 51 loss: 1.0958362817764282, accuracy: 0.5913461446762085\n",
      "Batch 52 loss: 1.0461560487747192, accuracy: 0.591244101524353\n",
      "Batch 53 loss: 1.0664860010147095, accuracy: 0.5902777910232544\n",
      "Batch 54 loss: 1.0742014646530151, accuracy: 0.589630663394928\n",
      "Batch 55 loss: 1.301987886428833, accuracy: 0.5881696343421936\n",
      "Batch 56 loss: 1.0772706270217896, accuracy: 0.5879934430122375\n",
      "Batch 57 loss: 1.1004425287246704, accuracy: 0.5886314511299133\n",
      "Batch 58 loss: 1.0940394401550293, accuracy: 0.5895127058029175\n",
      "Batch 59 loss: 1.1544142961502075, accuracy: 0.5885416865348816\n",
      "Batch 60 loss: 1.055408000946045, accuracy: 0.5887551307678223\n",
      "Batch 61 loss: 1.1538236141204834, accuracy: 0.5888356566429138\n",
      "Batch 62 loss: 1.1544373035430908, accuracy: 0.5881696343421936\n",
      "Batch 63 loss: 1.289776086807251, accuracy: 0.587158203125\n",
      "Batch 64 loss: 1.2333160638809204, accuracy: 0.5866586565971375\n",
      "Batch 65 loss: 1.1627049446105957, accuracy: 0.5864109992980957\n",
      "Batch 66 loss: 1.0459747314453125, accuracy: 0.5868703126907349\n",
      "Batch 67 loss: 1.0627703666687012, accuracy: 0.5868566036224365\n",
      "Batch 68 loss: 1.285758376121521, accuracy: 0.586277186870575\n",
      "Batch 69 loss: 1.0017943382263184, accuracy: 0.5866071581840515\n",
      "Batch 70 loss: 1.1738667488098145, accuracy: 0.5867077708244324\n",
      "Batch 71 loss: 1.069757103919983, accuracy: 0.5864800214767456\n",
      "Batch 72 loss: 1.0752007961273193, accuracy: 0.5864726305007935\n",
      "Batch 73 loss: 1.072871446609497, accuracy: 0.5864653587341309\n",
      "Batch 74 loss: 1.0996465682983398, accuracy: 0.5869791507720947\n",
      "Batch 75 loss: 1.029115080833435, accuracy: 0.5868626832962036\n",
      "Batch 76 loss: 1.0321531295776367, accuracy: 0.5870535969734192\n",
      "Batch 77 loss: 1.0499857664108276, accuracy: 0.5870392918586731\n",
      "Batch 78 loss: 1.1103928089141846, accuracy: 0.5872231125831604\n",
      "Batch 79 loss: 1.0980808734893799, accuracy: 0.587695300579071\n",
      "Batch 80 loss: 1.193362832069397, accuracy: 0.5872877836227417\n",
      "Batch 81 loss: 0.9639286994934082, accuracy: 0.5874618887901306\n",
      "Batch 82 loss: 1.0816829204559326, accuracy: 0.5874435305595398\n",
      "Batch 83 loss: 1.0543767213821411, accuracy: 0.5876116156578064\n",
      "Batch 84 loss: 1.1184406280517578, accuracy: 0.5874999761581421\n",
      "Batch 85 loss: 1.0538843870162964, accuracy: 0.5870276093482971\n",
      "Batch 86 loss: 1.008537769317627, accuracy: 0.5876436829566956\n",
      "Batch 87 loss: 1.0711089372634888, accuracy: 0.5874467492103577\n",
      "Batch 88 loss: 1.2005257606506348, accuracy: 0.5866397619247437\n",
      "Batch 89 loss: 1.1537295579910278, accuracy: 0.5859375\n",
      "Batch 90 loss: 1.1790932416915894, accuracy: 0.5856799483299255\n",
      "Batch 91 loss: 1.1621142625808716, accuracy: 0.5858525633811951\n",
      "Batch 92 loss: 1.1970587968826294, accuracy: 0.5859375\n",
      "Batch 93 loss: 1.3447321653366089, accuracy: 0.5850232839584351\n",
      "Batch 94 loss: 0.9927470684051514, accuracy: 0.5853618383407593\n",
      "Batch 95 loss: 1.2370673418045044, accuracy: 0.5840657353401184\n",
      "Batch 96 loss: 1.1691803932189941, accuracy: 0.5840045213699341\n",
      "Batch 97 loss: 1.1460767984390259, accuracy: 0.5840242505073547\n",
      "Batch 98 loss: 1.1881214380264282, accuracy: 0.5839646458625793\n",
      "Batch 99 loss: 1.1374645233154297, accuracy: 0.5832812786102295\n",
      "Batch 100 loss: 1.0484079122543335, accuracy: 0.5833075642585754\n",
      "Batch 101 loss: 1.251358985900879, accuracy: 0.5827205777168274\n",
      "Batch 102 loss: 1.0367262363433838, accuracy: 0.5826759934425354\n",
      "Batch 103 loss: 1.1550894975662231, accuracy: 0.5818809866905212\n",
      "Batch 104 loss: 1.0877599716186523, accuracy: 0.5816220045089722\n",
      "Batch 105 loss: 1.1492221355438232, accuracy: 0.5815153121948242\n",
      "Batch 106 loss: 1.122761607170105, accuracy: 0.5819947719573975\n",
      "Batch 107 loss: 1.1293392181396484, accuracy: 0.5816695690155029\n",
      "Batch 108 loss: 1.080054759979248, accuracy: 0.5810636281967163\n",
      "Batch 109 loss: 1.0123019218444824, accuracy: 0.5816051363945007\n",
      "Batch 110 loss: 1.1188263893127441, accuracy: 0.5819960832595825\n",
      "Batch 111 loss: 0.9973061680793762, accuracy: 0.58203125\n",
      "Batch 112 loss: 1.1820582151412964, accuracy: 0.5814436078071594\n",
      "Batch 113 loss: 1.074928641319275, accuracy: 0.5811403393745422\n",
      "Batch 114 loss: 1.107380986213684, accuracy: 0.58152174949646\n",
      "Batch 115 loss: 1.169042706489563, accuracy: 0.5810883641242981\n",
      "Batch 116 loss: 1.2508758306503296, accuracy: 0.5811966061592102\n",
      "Batch 117 loss: 1.2036776542663574, accuracy: 0.5809719562530518\n",
      "Batch 118 loss: 1.036561369895935, accuracy: 0.5813419222831726\n",
      "Batch 119 loss: 1.1436206102371216, accuracy: 0.5810546875\n",
      "Batch 120 loss: 0.9993441104888916, accuracy: 0.5813533067703247\n",
      "Batch 121 loss: 1.0474761724472046, accuracy: 0.5815189480781555\n",
      "Batch 122 loss: 1.0765774250030518, accuracy: 0.581745445728302\n",
      "Batch 123 loss: 1.0504947900772095, accuracy: 0.5817792415618896\n",
      "Batch 124 loss: 1.137967824935913, accuracy: 0.5816249847412109\n",
      "Batch 125 loss: 1.1140491962432861, accuracy: 0.5814732313156128\n",
      "Batch 126 loss: 1.0654059648513794, accuracy: 0.5818774700164795\n",
      "Batch 127 loss: 1.0683597326278687, accuracy: 0.58203125\n",
      "Batch 128 loss: 1.227007269859314, accuracy: 0.5819404125213623\n",
      "Batch 129 loss: 1.054398775100708, accuracy: 0.5824519395828247\n",
      "Batch 130 loss: 1.149908423423767, accuracy: 0.582359254360199\n",
      "Batch 131 loss: 1.150490641593933, accuracy: 0.58203125\n",
      "Batch 132 loss: 1.0286078453063965, accuracy: 0.5824130773544312\n",
      "Batch 133 loss: 0.9773268103599548, accuracy: 0.583197295665741\n",
      "Batch 134 loss: 1.318697214126587, accuracy: 0.5829282402992249\n",
      "Batch 135 loss: 1.224757194519043, accuracy: 0.5824333429336548\n",
      "Batch 136 loss: 1.091057300567627, accuracy: 0.5823448896408081\n",
      "Batch 137 loss: 1.043813705444336, accuracy: 0.5827671885490417\n",
      "Batch 138 loss: 1.043451189994812, accuracy: 0.5826776027679443\n",
      "Batch 139 loss: 1.204733967781067, accuracy: 0.5822544693946838\n",
      "Batch 140 loss: 1.0807468891143799, accuracy: 0.5820589661598206\n",
      "Batch 141 loss: 0.962304949760437, accuracy: 0.5824163556098938\n",
      "Batch 142 loss: 1.2383785247802734, accuracy: 0.5820039510726929\n",
      "Batch 143 loss: 1.1054728031158447, accuracy: 0.5820854902267456\n",
      "Batch 144 loss: 1.092603325843811, accuracy: 0.5823276042938232\n",
      "Batch 145 loss: 1.144497275352478, accuracy: 0.5821382999420166\n",
      "Batch 146 loss: 1.0838254690170288, accuracy: 0.5819515585899353\n",
      "Batch 147 loss: 1.0367323160171509, accuracy: 0.5822951793670654\n",
      "Batch 148 loss: 1.0915679931640625, accuracy: 0.5820574760437012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 149 loss: 1.224256157875061, accuracy: 0.5821354389190674\n",
      "Batch 150 loss: 1.006765604019165, accuracy: 0.5822640657424927\n",
      "Batch 151 loss: 1.0992802381515503, accuracy: 0.5821854472160339\n",
      "Batch 152 loss: 1.2599824666976929, accuracy: 0.5816482901573181\n",
      "Batch 153 loss: 1.0265965461730957, accuracy: 0.58203125\n",
      "Batch 154 loss: 1.1307717561721802, accuracy: 0.5818044543266296\n",
      "Batch 155 loss: 1.2117964029312134, accuracy: 0.5817808508872986\n",
      "Batch 156 loss: 1.1445343494415283, accuracy: 0.5816580653190613\n",
      "Batch 157 loss: 1.0283161401748657, accuracy: 0.5816356539726257\n",
      "Batch 158 loss: 1.0521184206008911, accuracy: 0.5817610025405884\n",
      "Batch 159 loss: 1.2393221855163574, accuracy: 0.5812011957168579\n",
      "Batch 160 loss: 0.9619624614715576, accuracy: 0.5813761353492737\n",
      "Batch 161 loss: 0.9814643263816833, accuracy: 0.5816454291343689\n",
      "Batch 162 loss: 0.9328629970550537, accuracy: 0.5819114446640015\n",
      "Batch 163 loss: 1.0762228965759277, accuracy: 0.5819360017776489\n",
      "Batch 164 loss: 1.191355586051941, accuracy: 0.5814867615699768\n",
      "Batch 165 loss: 1.1115385293960571, accuracy: 0.5816076993942261\n",
      "Batch 166 loss: 1.0988694429397583, accuracy: 0.5815868377685547\n",
      "Batch 167 loss: 1.0213314294815063, accuracy: 0.5819382667541504\n",
      "Batch 168 loss: 1.1263628005981445, accuracy: 0.5814996361732483\n",
      "Batch 169 loss: 1.079849123954773, accuracy: 0.5816176533699036\n",
      "Batch 170 loss: 1.203326940536499, accuracy: 0.5814144611358643\n",
      "Batch 171 loss: 1.0411721467971802, accuracy: 0.581486165523529\n",
      "Batch 172 loss: 1.0529694557189941, accuracy: 0.5815570950508118\n",
      "Batch 173 loss: 0.9878106117248535, accuracy: 0.5821659564971924\n",
      "Batch 174 loss: 1.1960170269012451, accuracy: 0.5816964507102966\n",
      "Batch 175 loss: 1.0524221658706665, accuracy: 0.58154296875\n",
      "Batch 176 loss: 1.186258316040039, accuracy: 0.5813471078872681\n",
      "Batch 177 loss: 0.9940807819366455, accuracy: 0.581811785697937\n",
      "Batch 178 loss: 1.005306363105774, accuracy: 0.5818784832954407\n",
      "Batch 179 loss: 1.0160658359527588, accuracy: 0.5822482705116272\n",
      "Batch 180 loss: 1.0444608926773071, accuracy: 0.5823118090629578\n",
      "Batch 181 loss: 1.1211566925048828, accuracy: 0.5822458863258362\n",
      "Batch 182 loss: 1.0085937976837158, accuracy: 0.5822233557701111\n",
      "Batch 183 loss: 1.1241439580917358, accuracy: 0.5820736885070801\n",
      "Batch 184 loss: 1.1813405752182007, accuracy: 0.5817567706108093\n",
      "Batch 185 loss: 0.991298496723175, accuracy: 0.5819472670555115\n",
      "Batch 186 loss: 1.0875955820083618, accuracy: 0.5820103883743286\n",
      "Batch 187 loss: 1.0295051336288452, accuracy: 0.58203125\n",
      "Batch 188 loss: 1.1222416162490845, accuracy: 0.5819692611694336\n",
      "Batch 189 loss: 1.1083885431289673, accuracy: 0.582072377204895\n",
      "Batch 190 loss: 1.0222012996673584, accuracy: 0.5820925831794739\n",
      "Batch 191 loss: 1.0733463764190674, accuracy: 0.5819905400276184\n",
      "Batch 192 loss: 1.1622406244277954, accuracy: 0.5817276835441589\n",
      "Batch 193 loss: 1.1257492303848267, accuracy: 0.5815077424049377\n",
      "Batch 194 loss: 1.0405689477920532, accuracy: 0.5816907286643982\n",
      "Batch 195 loss: 1.1526578664779663, accuracy: 0.5815927982330322\n",
      "Batch 196 loss: 1.051295518875122, accuracy: 0.5817338228225708\n",
      "Batch 197 loss: 1.2111029624938965, accuracy: 0.5818339586257935\n",
      "Batch 198 loss: 1.1197720766067505, accuracy: 0.5817368030548096\n",
      "Batch 199 loss: 1.1219197511672974, accuracy: 0.5818750262260437\n",
      "Batch 200 loss: 1.1740909814834595, accuracy: 0.5815842747688293\n",
      "Batch 201 loss: 1.1467492580413818, accuracy: 0.5815671682357788\n",
      "Batch 202 loss: 1.1763540506362915, accuracy: 0.5815501809120178\n",
      "Batch 203 loss: 0.9957582354545593, accuracy: 0.5820695757865906\n",
      "Batch 204 loss: 1.1185927391052246, accuracy: 0.5820503234863281\n",
      "Batch 205 loss: 1.0648576021194458, accuracy: 0.58203125\n",
      "Batch 206 loss: 1.0902057886123657, accuracy: 0.5820878744125366\n",
      "Batch 207 loss: 0.9658645987510681, accuracy: 0.58203125\n",
      "Batch 208 loss: 1.0739772319793701, accuracy: 0.5822741985321045\n",
      "Batch 209 loss: 1.1346931457519531, accuracy: 0.5821428298950195\n",
      "Batch 210 loss: 1.1137763261795044, accuracy: 0.582271933555603\n",
      "Batch 211 loss: 1.037529468536377, accuracy: 0.5822154879570007\n",
      "Batch 212 loss: 0.9891089200973511, accuracy: 0.5823796987533569\n",
      "Batch 213 loss: 1.251874327659607, accuracy: 0.58203125\n",
      "Batch 214 loss: 1.067980408668518, accuracy: 0.5821584463119507\n",
      "Batch 215 loss: 1.0671217441558838, accuracy: 0.5821759104728699\n",
      "Batch 216 loss: 1.0818185806274414, accuracy: 0.5820852518081665\n",
      "Batch 217 loss: 1.1065900325775146, accuracy: 0.5821745991706848\n",
      "Batch 218 loss: 1.0363675355911255, accuracy: 0.5823344588279724\n",
      "Batch 219 loss: 1.0193537473678589, accuracy: 0.5825994610786438\n",
      "Batch 220 loss: 1.0318834781646729, accuracy: 0.5827205777168274\n",
      "Batch 221 loss: 1.0101675987243652, accuracy: 0.5827702879905701\n",
      "Batch 222 loss: 0.8887763023376465, accuracy: 0.5832399129867554\n",
      "Batch 223 loss: 1.1814026832580566, accuracy: 0.5830775499343872\n",
      "Batch 224 loss: 0.8955822587013245, accuracy: 0.5832317471504211\n",
      "Training epoch: 9, train accuracy: 58.32317352294922, train loss: 1.0933508377605015, valid accuracy: 59.1529655456543, valid loss: 1.0709309660155197 \n",
      "Batch 0 loss: 1.214312195777893, accuracy: 0.5390625\n",
      "Batch 1 loss: 1.1847978830337524, accuracy: 0.54296875\n",
      "Batch 2 loss: 0.9899570345878601, accuracy: 0.5598958134651184\n",
      "Batch 3 loss: 1.0784188508987427, accuracy: 0.5703125\n",
      "Batch 4 loss: 1.098050594329834, accuracy: 0.573437511920929\n",
      "Batch 5 loss: 1.1011077165603638, accuracy: 0.5794270634651184\n",
      "Batch 6 loss: 1.0565185546875, accuracy: 0.5848214030265808\n",
      "Batch 7 loss: 1.1188095808029175, accuracy: 0.5859375\n",
      "Batch 8 loss: 1.1084283590316772, accuracy: 0.5902777910232544\n",
      "Batch 9 loss: 0.9732528924942017, accuracy: 0.5914062261581421\n",
      "Batch 10 loss: 1.1002223491668701, accuracy: 0.5916193127632141\n",
      "Batch 11 loss: 1.0294257402420044, accuracy: 0.58984375\n",
      "Batch 12 loss: 1.0402055978775024, accuracy: 0.5901442170143127\n",
      "Batch 13 loss: 1.1228936910629272, accuracy: 0.5848214030265808\n",
      "Batch 14 loss: 1.1531685590744019, accuracy: 0.5843750238418579\n",
      "Batch 15 loss: 1.052156686782837, accuracy: 0.583984375\n",
      "Batch 16 loss: 0.9320706129074097, accuracy: 0.5868566036224365\n",
      "Batch 17 loss: 0.9112827777862549, accuracy: 0.5928819179534912\n",
      "Batch 18 loss: 1.1853142976760864, accuracy: 0.5908716917037964\n",
      "Batch 19 loss: 1.132822871208191, accuracy: 0.5894531011581421\n",
      "Batch 20 loss: 0.9464149475097656, accuracy: 0.5930059552192688\n",
      "Batch 21 loss: 1.0736134052276611, accuracy: 0.5919744372367859\n",
      "Batch 22 loss: 1.037550449371338, accuracy: 0.592391312122345\n",
      "Batch 23 loss: 1.0116838216781616, accuracy: 0.5927734375\n",
      "Batch 24 loss: 1.1305254697799683, accuracy: 0.5934374928474426\n",
      "Batch 25 loss: 1.1007542610168457, accuracy: 0.59375\n",
      "Batch 26 loss: 0.994056224822998, accuracy: 0.5940393805503845\n",
      "Batch 27 loss: 1.04231595993042, accuracy: 0.5945870280265808\n",
      "Batch 28 loss: 1.184203863143921, accuracy: 0.5926724076271057\n",
      "Batch 29 loss: 1.2062909603118896, accuracy: 0.5934895873069763\n",
      "Batch 30 loss: 1.0661883354187012, accuracy: 0.592993974685669\n",
      "Batch 31 loss: 1.1374964714050293, accuracy: 0.592529296875\n",
      "Batch 32 loss: 1.0954939126968384, accuracy: 0.5916193127632141\n",
      "Batch 33 loss: 1.0922642946243286, accuracy: 0.5909926295280457\n",
      "Batch 34 loss: 1.0398280620574951, accuracy: 0.5915178656578064\n",
      "Batch 35 loss: 1.1709134578704834, accuracy: 0.5900607705116272\n",
      "Batch 36 loss: 1.0877306461334229, accuracy: 0.5897381901741028\n",
      "Batch 37 loss: 1.1198292970657349, accuracy: 0.5892269611358643\n",
      "Batch 38 loss: 1.1477075815200806, accuracy: 0.5883413553237915\n",
      "Batch 39 loss: 1.2008662223815918, accuracy: 0.5884765386581421\n",
      "Batch 40 loss: 1.1095596551895142, accuracy: 0.5882241129875183\n",
      "Batch 41 loss: 1.0320557355880737, accuracy: 0.5879836082458496\n",
      "Batch 42 loss: 1.0123636722564697, accuracy: 0.589026153087616\n",
      "Batch 43 loss: 0.9619780778884888, accuracy: 0.58984375\n",
      "Batch 44 loss: 1.0900131464004517, accuracy: 0.5895833373069763\n",
      "Batch 45 loss: 1.0799851417541504, accuracy: 0.590183436870575\n",
      "Batch 46 loss: 1.147793173789978, accuracy: 0.5897606611251831\n",
      "Batch 47 loss: 1.066070556640625, accuracy: 0.59033203125\n",
      "Batch 48 loss: 1.0388612747192383, accuracy: 0.590242326259613\n",
      "Batch 49 loss: 1.1923130750656128, accuracy: 0.5895312428474426\n",
      "Batch 50 loss: 1.155534267425537, accuracy: 0.5896139740943909\n",
      "Batch 51 loss: 1.065283179283142, accuracy: 0.5901442170143127\n",
      "Batch 52 loss: 0.9829044342041016, accuracy: 0.5915389060974121\n",
      "Batch 53 loss: 1.002882480621338, accuracy: 0.5924479365348816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 54 loss: 1.1032638549804688, accuracy: 0.5928977131843567\n",
      "Batch 55 loss: 1.0603225231170654, accuracy: 0.5927734375\n",
      "Batch 56 loss: 1.175582766532898, accuracy: 0.5922423005104065\n",
      "Batch 57 loss: 0.9394968748092651, accuracy: 0.5938847064971924\n",
      "Batch 58 loss: 1.143267035484314, accuracy: 0.593087911605835\n",
      "Batch 59 loss: 0.9288084506988525, accuracy: 0.5942708253860474\n",
      "Batch 60 loss: 1.0115877389907837, accuracy: 0.5941342115402222\n",
      "Batch 61 loss: 1.0166856050491333, accuracy: 0.5947580933570862\n",
      "Batch 62 loss: 0.8672928214073181, accuracy: 0.5958581566810608\n",
      "Batch 63 loss: 1.2218735218048096, accuracy: 0.5948486328125\n",
      "Batch 64 loss: 1.1659117937088013, accuracy: 0.5947115421295166\n",
      "Batch 65 loss: 1.119673490524292, accuracy: 0.5938683748245239\n",
      "Batch 66 loss: 1.069968342781067, accuracy: 0.5944496393203735\n",
      "Batch 67 loss: 1.0703431367874146, accuracy: 0.5944393277168274\n",
      "Batch 68 loss: 1.0656741857528687, accuracy: 0.5938632488250732\n",
      "Batch 69 loss: 1.1921169757843018, accuracy: 0.5934152007102966\n",
      "Batch 70 loss: 1.0878355503082275, accuracy: 0.593089759349823\n",
      "Batch 71 loss: 0.8566937446594238, accuracy: 0.5939670205116272\n",
      "Batch 72 loss: 1.106191635131836, accuracy: 0.5946061611175537\n",
      "Batch 73 loss: 1.1571497917175293, accuracy: 0.5944890379905701\n",
      "Batch 74 loss: 1.0617191791534424, accuracy: 0.5946875214576721\n",
      "Batch 75 loss: 0.9159740805625916, accuracy: 0.5956003069877625\n",
      "Batch 76 loss: 1.1139894723892212, accuracy: 0.5954748392105103\n",
      "Batch 77 loss: 1.0158015489578247, accuracy: 0.5959535241127014\n",
      "Batch 78 loss: 0.9854457974433899, accuracy: 0.5958267450332642\n",
      "Batch 79 loss: 0.9829892516136169, accuracy: 0.5958007574081421\n",
      "Batch 80 loss: 1.082063913345337, accuracy: 0.5959683656692505\n",
      "Batch 81 loss: 1.2384617328643799, accuracy: 0.5957507491111755\n",
      "Batch 82 loss: 1.1331781148910522, accuracy: 0.5955383777618408\n",
      "Batch 83 loss: 1.0577664375305176, accuracy: 0.5951451063156128\n",
      "Batch 84 loss: 1.0823571681976318, accuracy: 0.5947610139846802\n",
      "Batch 85 loss: 1.229094386100769, accuracy: 0.594295084476471\n",
      "Batch 86 loss: 1.1086212396621704, accuracy: 0.5944684147834778\n",
      "Batch 87 loss: 1.1122525930404663, accuracy: 0.5941938757896423\n",
      "Batch 88 loss: 1.0965393781661987, accuracy: 0.5941011309623718\n",
      "Batch 89 loss: 1.0958200693130493, accuracy: 0.5947916507720947\n",
      "Batch 90 loss: 1.0552842617034912, accuracy: 0.5944368243217468\n",
      "Batch 91 loss: 1.2400490045547485, accuracy: 0.59375\n",
      "Batch 92 loss: 1.0803827047348022, accuracy: 0.5941700339317322\n",
      "Batch 93 loss: 1.0823134183883667, accuracy: 0.5942486524581909\n",
      "Batch 94 loss: 1.0392673015594482, accuracy: 0.5949013233184814\n",
      "Batch 95 loss: 1.065271258354187, accuracy: 0.59521484375\n",
      "Batch 96 loss: 0.9129694700241089, accuracy: 0.5954413414001465\n",
      "Batch 97 loss: 1.095261573791504, accuracy: 0.5955038070678711\n",
      "Batch 98 loss: 1.2438644170761108, accuracy: 0.5944602489471436\n",
      "Batch 99 loss: 1.1578129529953003, accuracy: 0.5935937762260437\n",
      "Batch 100 loss: 0.9872183203697205, accuracy: 0.5939046740531921\n",
      "Batch 101 loss: 1.2125838994979858, accuracy: 0.5932904481887817\n",
      "Batch 102 loss: 1.1222940683364868, accuracy: 0.5929915308952332\n",
      "Batch 103 loss: 1.099068284034729, accuracy: 0.5923978090286255\n",
      "Batch 104 loss: 1.0412548780441284, accuracy: 0.5924107432365417\n",
      "Batch 105 loss: 1.0678627490997314, accuracy: 0.5924970507621765\n",
      "Batch 106 loss: 1.0466022491455078, accuracy: 0.5928007960319519\n",
      "Batch 107 loss: 1.0963501930236816, accuracy: 0.5925925970077515\n",
      "Batch 108 loss: 1.104718804359436, accuracy: 0.5922448635101318\n",
      "Batch 109 loss: 1.1011724472045898, accuracy: 0.5921875238418579\n",
      "Batch 110 loss: 1.2470182180404663, accuracy: 0.591920018196106\n",
      "Batch 111 loss: 1.211176872253418, accuracy: 0.5915876030921936\n",
      "Batch 112 loss: 1.087947130203247, accuracy: 0.591537594795227\n",
      "Batch 113 loss: 1.1015541553497314, accuracy: 0.5914884805679321\n",
      "Batch 114 loss: 1.2862833738327026, accuracy: 0.590896725654602\n",
      "Batch 115 loss: 1.0812550783157349, accuracy: 0.5907866358757019\n",
      "Batch 116 loss: 1.1575708389282227, accuracy: 0.5908119678497314\n",
      "Batch 117 loss: 1.1285679340362549, accuracy: 0.5904396176338196\n",
      "Batch 118 loss: 1.0523842573165894, accuracy: 0.5901392102241516\n",
      "Batch 119 loss: 1.146040678024292, accuracy: 0.5899088382720947\n",
      "Batch 120 loss: 1.0565450191497803, accuracy: 0.5894886255264282\n",
      "Batch 121 loss: 1.125205397605896, accuracy: 0.5895876288414001\n",
      "Batch 122 loss: 1.1537623405456543, accuracy: 0.5892403721809387\n",
      "Batch 123 loss: 1.1311800479888916, accuracy: 0.5888356566429138\n",
      "Batch 124 loss: 1.117760181427002, accuracy: 0.5885624885559082\n",
      "Batch 125 loss: 0.98540198802948, accuracy: 0.5889756679534912\n",
      "Batch 126 loss: 1.0170811414718628, accuracy: 0.5891978144645691\n",
      "Batch 127 loss: 1.2114038467407227, accuracy: 0.58880615234375\n",
      "Batch 128 loss: 1.0955220460891724, accuracy: 0.5886628031730652\n",
      "Batch 129 loss: 1.1787142753601074, accuracy: 0.588161051273346\n",
      "Batch 130 loss: 1.0404044389724731, accuracy: 0.588144063949585\n",
      "Batch 131 loss: 1.0188570022583008, accuracy: 0.5883049368858337\n",
      "Batch 132 loss: 1.2197699546813965, accuracy: 0.5876410007476807\n",
      "Batch 133 loss: 1.0970780849456787, accuracy: 0.5872201323509216\n",
      "Batch 134 loss: 1.2421910762786865, accuracy: 0.5866319537162781\n",
      "Batch 135 loss: 1.2357609272003174, accuracy: 0.5865119695663452\n",
      "Batch 136 loss: 1.2041256427764893, accuracy: 0.5861656069755554\n",
      "Batch 137 loss: 1.0743050575256348, accuracy: 0.58644700050354\n",
      "Batch 138 loss: 1.0084813833236694, accuracy: 0.5871177911758423\n",
      "Batch 139 loss: 1.0940932035446167, accuracy: 0.587109386920929\n",
      "Batch 140 loss: 1.053545594215393, accuracy: 0.5871564745903015\n",
      "Batch 141 loss: 1.2484910488128662, accuracy: 0.5869278311729431\n",
      "Batch 142 loss: 1.137668251991272, accuracy: 0.5867023468017578\n",
      "Batch 143 loss: 0.9636944532394409, accuracy: 0.5869140625\n",
      "Batch 144 loss: 0.9892880916595459, accuracy: 0.5871767401695251\n",
      "Batch 145 loss: 1.171900987625122, accuracy: 0.5871147513389587\n",
      "Batch 146 loss: 1.0781631469726562, accuracy: 0.5870004296302795\n",
      "Batch 147 loss: 1.0376003980636597, accuracy: 0.5872572064399719\n",
      "Batch 148 loss: 0.97915118932724, accuracy: 0.5873007774353027\n",
      "Batch 149 loss: 1.054258108139038, accuracy: 0.5875520706176758\n",
      "Batch 150 loss: 1.0380138158798218, accuracy: 0.5876966118812561\n",
      "Batch 151 loss: 1.067367434501648, accuracy: 0.5873766541481018\n",
      "Batch 152 loss: 1.0678555965423584, accuracy: 0.5872140526771545\n",
      "Batch 153 loss: 1.0848770141601562, accuracy: 0.5872057676315308\n",
      "Batch 154 loss: 1.005545735359192, accuracy: 0.5872479677200317\n",
      "Batch 155 loss: 1.090601921081543, accuracy: 0.5871894955635071\n",
      "Batch 156 loss: 1.0135868787765503, accuracy: 0.5873308181762695\n",
      "Batch 157 loss: 0.9968775510787964, accuracy: 0.587717592716217\n",
      "Batch 158 loss: 0.9105480313301086, accuracy: 0.5878046154975891\n",
      "Batch 159 loss: 1.1169188022613525, accuracy: 0.5877929925918579\n",
      "Batch 160 loss: 1.168809175491333, accuracy: 0.5875873565673828\n",
      "Batch 161 loss: 1.1421053409576416, accuracy: 0.5873842835426331\n",
      "Batch 162 loss: 0.8800706267356873, accuracy: 0.5879505276679993\n",
      "Batch 163 loss: 1.241917371749878, accuracy: 0.5877000689506531\n",
      "Batch 164 loss: 1.0029658079147339, accuracy: 0.5878787636756897\n",
      "Batch 165 loss: 1.1132817268371582, accuracy: 0.5877729654312134\n",
      "Batch 166 loss: 1.0045249462127686, accuracy: 0.5879958868026733\n",
      "Batch 167 loss: 1.033427357673645, accuracy: 0.5883556604385376\n",
      "Batch 168 loss: 1.1106033325195312, accuracy: 0.5882026553153992\n",
      "Batch 169 loss: 0.9319920539855957, accuracy: 0.5885570049285889\n",
      "Batch 170 loss: 1.1370402574539185, accuracy: 0.5884045958518982\n",
      "Batch 171 loss: 0.9179242253303528, accuracy: 0.5887990593910217\n",
      "Batch 172 loss: 1.025133728981018, accuracy: 0.5890534520149231\n",
      "Batch 173 loss: 1.0564920902252197, accuracy: 0.5890804529190063\n",
      "Batch 174 loss: 1.0836516618728638, accuracy: 0.5892857313156128\n",
      "Batch 175 loss: 1.0078190565109253, accuracy: 0.5891335010528564\n",
      "Batch 176 loss: 1.1553388833999634, accuracy: 0.5888506174087524\n",
      "Batch 177 loss: 1.0879393815994263, accuracy: 0.5889659523963928\n",
      "Batch 178 loss: 1.0357377529144287, accuracy: 0.5890362858772278\n",
      "Batch 179 loss: 0.9492705464363098, accuracy: 0.5895399451255798\n",
      "Batch 180 loss: 0.9556050896644592, accuracy: 0.5896926522254944\n",
      "Batch 181 loss: 1.0572130680084229, accuracy: 0.5895432829856873\n",
      "Batch 182 loss: 1.1195673942565918, accuracy: 0.5894808769226074\n",
      "Batch 183 loss: 1.0555667877197266, accuracy: 0.58984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 184 loss: 1.0607857704162598, accuracy: 0.5899071097373962\n",
      "Batch 185 loss: 0.9149491786956787, accuracy: 0.5902637839317322\n",
      "Batch 186 loss: 1.0524134635925293, accuracy: 0.5903659462928772\n",
      "Batch 187 loss: 1.0642069578170776, accuracy: 0.5905917286872864\n",
      "Batch 188 loss: 1.1174315214157104, accuracy: 0.5906911492347717\n",
      "Batch 189 loss: 1.0779684782028198, accuracy: 0.5906661152839661\n",
      "Batch 190 loss: 1.0340991020202637, accuracy: 0.5906413793563843\n",
      "Batch 191 loss: 1.0857807397842407, accuracy: 0.5906982421875\n",
      "Batch 192 loss: 0.928222119808197, accuracy: 0.5911593437194824\n",
      "Batch 193 loss: 0.9546902775764465, accuracy: 0.5913740396499634\n",
      "Batch 194 loss: 1.0691964626312256, accuracy: 0.5912660360336304\n",
      "Batch 195 loss: 1.10750150680542, accuracy: 0.5910794138908386\n",
      "Batch 196 loss: 1.1044774055480957, accuracy: 0.5911722779273987\n",
      "Batch 197 loss: 1.0996437072753906, accuracy: 0.5910668969154358\n",
      "Batch 198 loss: 1.164064884185791, accuracy: 0.5908055901527405\n",
      "Batch 199 loss: 1.1490591764450073, accuracy: 0.590624988079071\n",
      "Batch 200 loss: 1.144087791442871, accuracy: 0.5904462337493896\n",
      "Batch 201 loss: 1.0015456676483154, accuracy: 0.5905399322509766\n",
      "Batch 202 loss: 1.0176790952682495, accuracy: 0.5906326770782471\n",
      "Batch 203 loss: 1.2086536884307861, accuracy: 0.5902650356292725\n",
      "Batch 204 loss: 1.0099940299987793, accuracy: 0.5902057886123657\n",
      "Batch 205 loss: 1.062585711479187, accuracy: 0.590260922908783\n",
      "Batch 206 loss: 1.1136112213134766, accuracy: 0.5902400612831116\n",
      "Batch 207 loss: 0.976585328578949, accuracy: 0.5904822945594788\n",
      "Batch 208 loss: 1.2956818342208862, accuracy: 0.590236246585846\n",
      "Batch 209 loss: 1.1332671642303467, accuracy: 0.5901413559913635\n",
      "Batch 210 loss: 0.9203693866729736, accuracy: 0.5904917120933533\n",
      "Batch 211 loss: 1.0722882747650146, accuracy: 0.5908387303352356\n",
      "Batch 212 loss: 1.0252697467803955, accuracy: 0.5909991264343262\n",
      "Batch 213 loss: 1.0593996047973633, accuracy: 0.5910120010375977\n",
      "Batch 214 loss: 1.0805848836898804, accuracy: 0.5908430218696594\n",
      "Batch 215 loss: 0.9480507969856262, accuracy: 0.5911819934844971\n",
      "Batch 216 loss: 1.0904195308685303, accuracy: 0.5911938548088074\n",
      "Batch 217 loss: 1.1311334371566772, accuracy: 0.5910621881484985\n",
      "Batch 218 loss: 1.2008644342422485, accuracy: 0.590753436088562\n",
      "Batch 219 loss: 1.0380713939666748, accuracy: 0.5908380746841431\n",
      "Batch 220 loss: 1.0493232011795044, accuracy: 0.5911694169044495\n",
      "Batch 221 loss: 0.9330663681030273, accuracy: 0.5913921594619751\n",
      "Batch 222 loss: 0.9243061542510986, accuracy: 0.5917530655860901\n",
      "Batch 223 loss: 1.0269806385040283, accuracy: 0.591796875\n",
      "Batch 224 loss: 0.9428261518478394, accuracy: 0.5919398069381714\n",
      "Training epoch: 10, train accuracy: 59.1939811706543, train loss: 1.0778155040740967, valid accuracy: 59.34800720214844, valid loss: 1.0809925527408206 \n",
      "Batch 0 loss: 1.0820934772491455, accuracy: 0.5859375\n",
      "Batch 1 loss: 0.9539518356323242, accuracy: 0.61328125\n",
      "Batch 2 loss: 0.8342231512069702, accuracy: 0.6588541865348816\n",
      "Batch 3 loss: 1.1582978963851929, accuracy: 0.630859375\n",
      "Batch 4 loss: 1.050381064414978, accuracy: 0.628125011920929\n",
      "Batch 5 loss: 0.9904840588569641, accuracy: 0.6341145634651184\n",
      "Batch 6 loss: 0.9661447405815125, accuracy: 0.6428571343421936\n",
      "Batch 7 loss: 1.0208441019058228, accuracy: 0.640625\n",
      "Batch 8 loss: 1.071409821510315, accuracy: 0.6362847089767456\n",
      "Batch 9 loss: 1.1030125617980957, accuracy: 0.6328125\n",
      "Batch 10 loss: 0.9840717315673828, accuracy: 0.6349431872367859\n",
      "Batch 11 loss: 1.0441110134124756, accuracy: 0.6315104365348816\n",
      "Batch 12 loss: 1.0768235921859741, accuracy: 0.6292067170143127\n",
      "Batch 13 loss: 1.0370185375213623, accuracy: 0.6261160969734192\n",
      "Batch 14 loss: 1.0993337631225586, accuracy: 0.6244791746139526\n",
      "Batch 15 loss: 1.078619122505188, accuracy: 0.623046875\n",
      "Batch 16 loss: 0.9915504455566406, accuracy: 0.623161792755127\n",
      "Batch 17 loss: 1.0181396007537842, accuracy: 0.6219618320465088\n",
      "Batch 18 loss: 1.0057053565979004, accuracy: 0.6217105388641357\n",
      "Batch 19 loss: 1.061934471130371, accuracy: 0.62109375\n",
      "Batch 20 loss: 1.169930338859558, accuracy: 0.6183035969734192\n",
      "Batch 21 loss: 1.0069738626480103, accuracy: 0.6182528138160706\n",
      "Batch 22 loss: 1.1267249584197998, accuracy: 0.61548912525177\n",
      "Batch 23 loss: 1.163734793663025, accuracy: 0.6139323115348816\n",
      "Batch 24 loss: 1.0839309692382812, accuracy: 0.6128125190734863\n",
      "Batch 25 loss: 1.00948166847229, accuracy: 0.6138821840286255\n",
      "Batch 26 loss: 1.0930628776550293, accuracy: 0.6131365895271301\n",
      "Batch 27 loss: 1.121170997619629, accuracy: 0.6116071343421936\n",
      "Batch 28 loss: 1.003652811050415, accuracy: 0.6126077771186829\n",
      "Batch 29 loss: 1.011641263961792, accuracy: 0.6122395992279053\n",
      "Batch 30 loss: 0.987379252910614, accuracy: 0.6129032373428345\n",
      "Batch 31 loss: 1.1163207292556763, accuracy: 0.611328125\n",
      "Batch 32 loss: 0.9986380934715271, accuracy: 0.6126893758773804\n",
      "Batch 33 loss: 1.1263759136199951, accuracy: 0.6102941036224365\n",
      "Batch 34 loss: 1.0940966606140137, accuracy: 0.6087053418159485\n",
      "Batch 35 loss: 1.0738410949707031, accuracy: 0.6076388955116272\n",
      "Batch 36 loss: 1.079393982887268, accuracy: 0.6068412065505981\n",
      "Batch 37 loss: 1.1875559091567993, accuracy: 0.6050575375556946\n",
      "Batch 38 loss: 1.0058550834655762, accuracy: 0.6051682829856873\n",
      "Batch 39 loss: 1.0400980710983276, accuracy: 0.6050781011581421\n",
      "Batch 40 loss: 1.1562469005584717, accuracy: 0.6038491129875183\n",
      "Batch 41 loss: 1.0673450231552124, accuracy: 0.6041666865348816\n",
      "Batch 42 loss: 1.1904287338256836, accuracy: 0.6019258499145508\n",
      "Batch 43 loss: 1.1958242654800415, accuracy: 0.6015625\n",
      "Batch 44 loss: 1.1445975303649902, accuracy: 0.6000000238418579\n",
      "Batch 45 loss: 1.09814453125, accuracy: 0.59986412525177\n",
      "Batch 46 loss: 1.0486781597137451, accuracy: 0.5990691781044006\n",
      "Batch 47 loss: 1.161109447479248, accuracy: 0.5989583134651184\n",
      "Batch 48 loss: 1.093449592590332, accuracy: 0.5983737111091614\n",
      "Batch 49 loss: 1.1073296070098877, accuracy: 0.5975000262260437\n",
      "Batch 50 loss: 1.1295207738876343, accuracy: 0.5969669222831726\n",
      "Batch 51 loss: 0.9839187264442444, accuracy: 0.5988581776618958\n",
      "Batch 52 loss: 1.1165640354156494, accuracy: 0.5986143946647644\n",
      "Batch 53 loss: 0.9983254075050354, accuracy: 0.5995370149612427\n",
      "Batch 54 loss: 0.961950421333313, accuracy: 0.6000000238418579\n",
      "Batch 55 loss: 1.047382116317749, accuracy: 0.599609375\n",
      "Batch 56 loss: 1.0994861125946045, accuracy: 0.5992324352264404\n",
      "Batch 57 loss: 0.9850727915763855, accuracy: 0.5985991358757019\n",
      "Batch 58 loss: 1.2492330074310303, accuracy: 0.5970603823661804\n",
      "Batch 59 loss: 0.9621889591217041, accuracy: 0.5975260138511658\n",
      "Batch 60 loss: 1.0035710334777832, accuracy: 0.5977202653884888\n",
      "Batch 61 loss: 1.0492613315582275, accuracy: 0.5971522331237793\n",
      "Batch 62 loss: 1.1464807987213135, accuracy: 0.5958581566810608\n",
      "Batch 63 loss: 1.1041513681411743, accuracy: 0.5955810546875\n",
      "Batch 64 loss: 1.0662235021591187, accuracy: 0.5945913195610046\n",
      "Batch 65 loss: 0.9595011472702026, accuracy: 0.595762312412262\n",
      "Batch 66 loss: 1.093988299369812, accuracy: 0.5947994589805603\n",
      "Batch 67 loss: 1.0615038871765137, accuracy: 0.5944393277168274\n",
      "Batch 68 loss: 0.95875084400177, accuracy: 0.59544837474823\n",
      "Batch 69 loss: 1.0427489280700684, accuracy: 0.5956473350524902\n",
      "Batch 70 loss: 0.9286574125289917, accuracy: 0.5961707830429077\n",
      "Batch 71 loss: 1.0226598978042603, accuracy: 0.5967881679534912\n",
      "Batch 72 loss: 1.023984670639038, accuracy: 0.5973886847496033\n",
      "Batch 73 loss: 0.9945854544639587, accuracy: 0.5978673696517944\n",
      "Batch 74 loss: 0.9729562401771545, accuracy: 0.5983333587646484\n",
      "Batch 75 loss: 1.0646986961364746, accuracy: 0.5983757972717285\n",
      "Batch 76 loss: 1.1919677257537842, accuracy: 0.5982142686843872\n",
      "Batch 77 loss: 1.0725427865982056, accuracy: 0.5980569124221802\n",
      "Batch 78 loss: 1.152084469795227, accuracy: 0.5982001423835754\n",
      "Batch 79 loss: 1.0274460315704346, accuracy: 0.59814453125\n",
      "Batch 80 loss: 0.986009418964386, accuracy: 0.5983796119689941\n",
      "Batch 81 loss: 0.9992475509643555, accuracy: 0.5986089706420898\n",
      "Batch 82 loss: 1.0732988119125366, accuracy: 0.5987387299537659\n",
      "Batch 83 loss: 1.0805450677871704, accuracy: 0.5989583134651184\n",
      "Batch 84 loss: 1.072727918624878, accuracy: 0.5985293984413147\n",
      "Batch 85 loss: 1.1192586421966553, accuracy: 0.598473846912384\n",
      "Batch 86 loss: 1.0085701942443848, accuracy: 0.5983297228813171\n",
      "Batch 87 loss: 1.107262372970581, accuracy: 0.5981001257896423\n",
      "Batch 88 loss: 0.9525824189186096, accuracy: 0.5984901785850525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 89 loss: 1.1072736978530884, accuracy: 0.5982638597488403\n",
      "Batch 90 loss: 1.0283401012420654, accuracy: 0.5983001589775085\n",
      "Batch 91 loss: 1.0434355735778809, accuracy: 0.5980808138847351\n",
      "Batch 92 loss: 0.9834718108177185, accuracy: 0.5982862710952759\n",
      "Batch 93 loss: 1.083548903465271, accuracy: 0.5987367033958435\n",
      "Batch 94 loss: 1.0807448625564575, accuracy: 0.5986841917037964\n",
      "Batch 95 loss: 0.9787221550941467, accuracy: 0.5989583134651184\n",
      "Batch 96 loss: 1.0631846189498901, accuracy: 0.5989046096801758\n",
      "Batch 97 loss: 1.0309207439422607, accuracy: 0.5988520383834839\n",
      "Batch 98 loss: 1.035197138786316, accuracy: 0.5988793969154358\n",
      "Batch 99 loss: 0.965758204460144, accuracy: 0.5992187261581421\n",
      "Batch 100 loss: 1.1320934295654297, accuracy: 0.598855197429657\n",
      "Batch 101 loss: 0.9136892557144165, accuracy: 0.599724292755127\n",
      "Batch 102 loss: 1.0535575151443481, accuracy: 0.5997421145439148\n",
      "Batch 103 loss: 1.053396463394165, accuracy: 0.5993840098381042\n",
      "Batch 104 loss: 0.9420548677444458, accuracy: 0.5996279716491699\n",
      "Batch 105 loss: 1.0119291543960571, accuracy: 0.5999410152435303\n",
      "Batch 106 loss: 0.8481548428535461, accuracy: 0.6009783744812012\n",
      "Batch 107 loss: 1.207475185394287, accuracy: 0.6004050970077515\n",
      "Batch 108 loss: 1.0180835723876953, accuracy: 0.6008457541465759\n",
      "Batch 109 loss: 1.0692187547683716, accuracy: 0.6003550887107849\n",
      "Batch 110 loss: 1.0062286853790283, accuracy: 0.6002252101898193\n",
      "Batch 111 loss: 1.0247812271118164, accuracy: 0.6003069281578064\n",
      "Batch 112 loss: 1.083642601966858, accuracy: 0.6001797318458557\n",
      "Batch 113 loss: 1.0833977460861206, accuracy: 0.5997806787490845\n",
      "Batch 114 loss: 0.902519166469574, accuracy: 0.6005434989929199\n",
      "Batch 115 loss: 1.1567586660385132, accuracy: 0.599946141242981\n",
      "Batch 116 loss: 1.0162889957427979, accuracy: 0.6002270579338074\n",
      "Batch 117 loss: 1.0533533096313477, accuracy: 0.5999073386192322\n",
      "Batch 118 loss: 0.9629643559455872, accuracy: 0.5996586084365845\n",
      "Batch 119 loss: 1.089316487312317, accuracy: 0.5997396111488342\n",
      "Batch 120 loss: 1.0680352449417114, accuracy: 0.5993672609329224\n",
      "Batch 121 loss: 1.0185655355453491, accuracy: 0.5993852615356445\n",
      "Batch 122 loss: 0.9578731060028076, accuracy: 0.5997840166091919\n",
      "Batch 123 loss: 1.1094930171966553, accuracy: 0.5997353792190552\n",
      "Batch 124 loss: 1.0449694395065308, accuracy: 0.5997499823570251\n",
      "Batch 125 loss: 1.128951072692871, accuracy: 0.5995163917541504\n",
      "Batch 126 loss: 0.9321413040161133, accuracy: 0.6000860929489136\n",
      "Batch 127 loss: 1.0922322273254395, accuracy: 0.60028076171875\n",
      "Batch 128 loss: 1.1200239658355713, accuracy: 0.6001695990562439\n",
      "Batch 129 loss: 0.938080370426178, accuracy: 0.6004807949066162\n",
      "Batch 130 loss: 1.060031771659851, accuracy: 0.6001908183097839\n",
      "Batch 131 loss: 0.98823082447052, accuracy: 0.6004379987716675\n",
      "Batch 132 loss: 0.997379720211029, accuracy: 0.6005638837814331\n",
      "Batch 133 loss: 1.1587443351745605, accuracy: 0.6004547476768494\n",
      "Batch 134 loss: 0.966361939907074, accuracy: 0.6005787253379822\n",
      "Batch 135 loss: 1.0299040079116821, accuracy: 0.6009880304336548\n",
      "Batch 136 loss: 1.073491096496582, accuracy: 0.6007071137428284\n",
      "Batch 137 loss: 1.1026232242584229, accuracy: 0.6002604365348816\n",
      "Batch 138 loss: 0.8697493076324463, accuracy: 0.6010004281997681\n",
      "Batch 139 loss: 1.1434065103530884, accuracy: 0.6008370518684387\n",
      "Batch 140 loss: 0.9940258860588074, accuracy: 0.6008976101875305\n",
      "Batch 141 loss: 0.9969527721405029, accuracy: 0.6011773943901062\n",
      "Batch 142 loss: 1.1445515155792236, accuracy: 0.6009069085121155\n",
      "Batch 143 loss: 1.0665258169174194, accuracy: 0.6008029580116272\n",
      "Batch 144 loss: 1.1041162014007568, accuracy: 0.6006465554237366\n",
      "Batch 145 loss: 1.1132670640945435, accuracy: 0.6004387736320496\n",
      "Batch 146 loss: 1.0612188577651978, accuracy: 0.6003932952880859\n",
      "Batch 147 loss: 1.0612846612930298, accuracy: 0.6004011631011963\n",
      "Batch 148 loss: 1.0332940816879272, accuracy: 0.6003565192222595\n",
      "Batch 149 loss: 1.0549129247665405, accuracy: 0.6004687547683716\n",
      "Batch 150 loss: 1.0332121849060059, accuracy: 0.6001138091087341\n",
      "Batch 151 loss: 1.0001662969589233, accuracy: 0.6004831194877625\n",
      "Batch 152 loss: 1.063675045967102, accuracy: 0.6004391312599182\n",
      "Batch 153 loss: 1.2142771482467651, accuracy: 0.5998376607894897\n",
      "Batch 154 loss: 1.1110159158706665, accuracy: 0.5997983813285828\n",
      "Batch 155 loss: 1.0755723714828491, accuracy: 0.5998597741127014\n",
      "Batch 156 loss: 1.0330780744552612, accuracy: 0.6001691818237305\n",
      "Batch 157 loss: 1.0413742065429688, accuracy: 0.6000296473503113\n",
      "Batch 158 loss: 0.9589731693267822, accuracy: 0.6000884175300598\n",
      "Batch 159 loss: 1.0661494731903076, accuracy: 0.600341796875\n",
      "Batch 160 loss: 1.001093864440918, accuracy: 0.6004949808120728\n",
      "Batch 161 loss: 0.9498958587646484, accuracy: 0.6007909178733826\n",
      "Batch 162 loss: 1.136117935180664, accuracy: 0.6004121899604797\n",
      "Batch 163 loss: 1.0626548528671265, accuracy: 0.6006574034690857\n",
      "Batch 164 loss: 1.1432620286941528, accuracy: 0.6005681753158569\n",
      "Batch 165 loss: 1.2072287797927856, accuracy: 0.600338876247406\n",
      "Batch 166 loss: 1.122450590133667, accuracy: 0.6000187397003174\n",
      "Batch 167 loss: 1.1223115921020508, accuracy: 0.5997489094734192\n",
      "Batch 168 loss: 0.9638022184371948, accuracy: 0.5998058319091797\n",
      "Batch 169 loss: 1.2151962518692017, accuracy: 0.5994485020637512\n",
      "Batch 170 loss: 1.081932544708252, accuracy: 0.5993695259094238\n",
      "Batch 171 loss: 1.0999592542648315, accuracy: 0.599064290523529\n",
      "Batch 172 loss: 0.9356505870819092, accuracy: 0.5994400382041931\n",
      "Batch 173 loss: 1.022226095199585, accuracy: 0.5994073152542114\n",
      "Batch 174 loss: 1.0143250226974487, accuracy: 0.5996428728103638\n",
      "Batch 175 loss: 0.9528998732566833, accuracy: 0.5996537804603577\n",
      "Batch 176 loss: 0.8224407434463501, accuracy: 0.6002383232116699\n",
      "Batch 177 loss: 0.9691759347915649, accuracy: 0.600553035736084\n",
      "Batch 178 loss: 1.060059666633606, accuracy: 0.6005586385726929\n",
      "Batch 179 loss: 1.0327105522155762, accuracy: 0.6006510257720947\n",
      "Batch 180 loss: 1.0709539651870728, accuracy: 0.6005265712738037\n",
      "Batch 181 loss: 1.250772476196289, accuracy: 0.6002318263053894\n",
      "Batch 182 loss: 1.3308430910110474, accuracy: 0.5997267961502075\n",
      "Batch 183 loss: 0.8731096982955933, accuracy: 0.6003736257553101\n",
      "Batch 184 loss: 0.9985884428024292, accuracy: 0.6006757020950317\n",
      "Batch 185 loss: 1.047151803970337, accuracy: 0.6006384491920471\n",
      "Batch 186 loss: 1.2055432796478271, accuracy: 0.6002256274223328\n",
      "Batch 187 loss: 1.1686375141143799, accuracy: 0.5998587012290955\n",
      "Batch 188 loss: 1.1648871898651123, accuracy: 0.5996196866035461\n",
      "Batch 189 loss: 1.1950575113296509, accuracy: 0.5993421077728271\n",
      "Batch 190 loss: 0.9250905513763428, accuracy: 0.5993946194648743\n",
      "Batch 191 loss: 1.0845675468444824, accuracy: 0.599365234375\n",
      "Batch 192 loss: 1.064592957496643, accuracy: 0.5994170904159546\n",
      "Batch 193 loss: 1.0136141777038574, accuracy: 0.5995892286300659\n",
      "Batch 194 loss: 1.0234031677246094, accuracy: 0.5997195243835449\n",
      "Batch 195 loss: 1.0144754648208618, accuracy: 0.5996890664100647\n",
      "Batch 196 loss: 1.0161687135696411, accuracy: 0.6000158786773682\n",
      "Batch 197 loss: 1.154063105583191, accuracy: 0.6001026034355164\n",
      "Batch 198 loss: 0.9670889377593994, accuracy: 0.6001491546630859\n",
      "Batch 199 loss: 0.9192160367965698, accuracy: 0.6006249785423279\n",
      "Batch 200 loss: 1.0902447700500488, accuracy: 0.6005130410194397\n",
      "Batch 201 loss: 1.0863852500915527, accuracy: 0.600363552570343\n",
      "Batch 202 loss: 1.035244345664978, accuracy: 0.6005618572235107\n",
      "Batch 203 loss: 0.9669363498687744, accuracy: 0.6008731722831726\n",
      "Batch 204 loss: 1.0141841173171997, accuracy: 0.6011433005332947\n",
      "Batch 205 loss: 1.1567188501358032, accuracy: 0.601145327091217\n",
      "Batch 206 loss: 0.9388511776924133, accuracy: 0.6013360619544983\n",
      "Batch 207 loss: 1.1118288040161133, accuracy: 0.601487398147583\n",
      "Batch 208 loss: 1.0081396102905273, accuracy: 0.6014129519462585\n",
      "Batch 209 loss: 1.1487925052642822, accuracy: 0.6011532545089722\n",
      "Batch 210 loss: 1.0637065172195435, accuracy: 0.6011182069778442\n",
      "Batch 211 loss: 0.9949320554733276, accuracy: 0.6014150977134705\n",
      "Batch 212 loss: 0.9523341059684753, accuracy: 0.6015258431434631\n",
      "Batch 213 loss: 1.1863653659820557, accuracy: 0.6012704372406006\n",
      "Batch 214 loss: 1.0927644968032837, accuracy: 0.6013444662094116\n",
      "Batch 215 loss: 1.0954855680465698, accuracy: 0.6011646389961243\n",
      "Batch 216 loss: 1.0377498865127563, accuracy: 0.6013104915618896\n",
      "Batch 217 loss: 0.9624856114387512, accuracy: 0.6014549732208252\n",
      "Batch 218 loss: 1.0794117450714111, accuracy: 0.6013484597206116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 219 loss: 0.9411225318908691, accuracy: 0.6015980243682861\n",
      "Batch 220 loss: 0.9862971305847168, accuracy: 0.6019160151481628\n",
      "Batch 221 loss: 1.0576834678649902, accuracy: 0.6019496321678162\n",
      "Batch 222 loss: 1.090323567390442, accuracy: 0.6018778085708618\n",
      "Batch 223 loss: 1.2527731657028198, accuracy: 0.6014578938484192\n",
      "Batch 224 loss: 0.9577362537384033, accuracy: 0.6014490127563477\n",
      "Training epoch: 11, train accuracy: 60.144901275634766, train loss: 1.053472105132209, valid accuracy: 60.51824951171875, valid loss: 1.0502310785753974 \n",
      "Batch 0 loss: 1.0060961246490479, accuracy: 0.6328125\n",
      "Batch 1 loss: 1.0453940629959106, accuracy: 0.61328125\n",
      "Batch 2 loss: 1.1639869213104248, accuracy: 0.5911458134651184\n",
      "Batch 3 loss: 1.1371359825134277, accuracy: 0.580078125\n",
      "Batch 4 loss: 0.9222695827484131, accuracy: 0.6000000238418579\n",
      "Batch 5 loss: 0.95491623878479, accuracy: 0.6067708134651184\n",
      "Batch 6 loss: 1.1052995920181274, accuracy: 0.6049107313156128\n",
      "Batch 7 loss: 1.1211416721343994, accuracy: 0.60546875\n",
      "Batch 8 loss: 0.9887586832046509, accuracy: 0.6041666865348816\n",
      "Batch 9 loss: 1.0059738159179688, accuracy: 0.60546875\n",
      "Batch 10 loss: 0.9828141927719116, accuracy: 0.6072443127632141\n",
      "Batch 11 loss: 1.0812150239944458, accuracy: 0.6048176884651184\n",
      "Batch 12 loss: 1.0753636360168457, accuracy: 0.606370210647583\n",
      "Batch 13 loss: 1.0035607814788818, accuracy: 0.6060267686843872\n",
      "Batch 14 loss: 0.9476636648178101, accuracy: 0.6098958253860474\n",
      "Batch 15 loss: 0.9270707368850708, accuracy: 0.61279296875\n",
      "Batch 16 loss: 1.0753931999206543, accuracy: 0.6116728186607361\n",
      "Batch 17 loss: 1.05992591381073, accuracy: 0.6089409589767456\n",
      "Batch 18 loss: 1.0824313163757324, accuracy: 0.6097862124443054\n",
      "Batch 19 loss: 1.0815699100494385, accuracy: 0.604296863079071\n",
      "Batch 20 loss: 0.9553616046905518, accuracy: 0.6082589030265808\n",
      "Batch 21 loss: 0.9925653338432312, accuracy: 0.6100852489471436\n",
      "Batch 22 loss: 1.0042877197265625, accuracy: 0.6114130616188049\n",
      "Batch 23 loss: 1.010603666305542, accuracy: 0.6087239384651184\n",
      "Batch 24 loss: 0.966848611831665, accuracy: 0.612500011920929\n",
      "Batch 25 loss: 0.9982675313949585, accuracy: 0.6120793223381042\n",
      "Batch 26 loss: 1.1178396940231323, accuracy: 0.6105324029922485\n",
      "Batch 27 loss: 1.0459096431732178, accuracy: 0.611328125\n",
      "Batch 28 loss: 0.9561173319816589, accuracy: 0.6115301847457886\n",
      "Batch 29 loss: 1.0592268705368042, accuracy: 0.6109374761581421\n",
      "Batch 30 loss: 0.9902482628822327, accuracy: 0.6126512289047241\n",
      "Batch 31 loss: 1.1143794059753418, accuracy: 0.610595703125\n",
      "Batch 32 loss: 1.0937570333480835, accuracy: 0.6103219985961914\n",
      "Batch 33 loss: 0.988805890083313, accuracy: 0.6123621463775635\n",
      "Batch 34 loss: 1.0301403999328613, accuracy: 0.6131696701049805\n",
      "Batch 35 loss: 1.0042295455932617, accuracy: 0.6128472089767456\n",
      "Batch 36 loss: 1.0910849571228027, accuracy: 0.6138091087341309\n",
      "Batch 37 loss: 1.0365976095199585, accuracy: 0.6126644611358643\n",
      "Batch 38 loss: 1.0773347616195679, accuracy: 0.6125801205635071\n",
      "Batch 39 loss: 1.0896978378295898, accuracy: 0.6126953363418579\n",
      "Batch 40 loss: 0.9941878914833069, accuracy: 0.6126143336296082\n",
      "Batch 41 loss: 1.0749491453170776, accuracy: 0.6116071343421936\n",
      "Batch 42 loss: 1.0906198024749756, accuracy: 0.609920084476471\n",
      "Batch 43 loss: 0.8669916391372681, accuracy: 0.6125710010528564\n",
      "Batch 44 loss: 0.934800922870636, accuracy: 0.6135416626930237\n",
      "Batch 45 loss: 1.2458668947219849, accuracy: 0.61328125\n",
      "Batch 46 loss: 1.2787501811981201, accuracy: 0.611535906791687\n",
      "Batch 47 loss: 0.9470524191856384, accuracy: 0.6131184697151184\n",
      "Batch 48 loss: 1.1398838758468628, accuracy: 0.6122449040412903\n",
      "Batch 49 loss: 0.9447523951530457, accuracy: 0.6131250262260437\n",
      "Batch 50 loss: 1.1116949319839478, accuracy: 0.6121323704719543\n",
      "Batch 51 loss: 1.0770882368087769, accuracy: 0.6117788553237915\n",
      "Batch 52 loss: 1.2188825607299805, accuracy: 0.6107016801834106\n",
      "Batch 53 loss: 1.1166945695877075, accuracy: 0.6095196604728699\n",
      "Batch 54 loss: 1.0988496541976929, accuracy: 0.6092329621315002\n",
      "Batch 55 loss: 1.0241148471832275, accuracy: 0.6090959906578064\n",
      "Batch 56 loss: 0.9936404824256897, accuracy: 0.6092379093170166\n",
      "Batch 57 loss: 0.9411182999610901, accuracy: 0.609375\n",
      "Batch 58 loss: 0.9893385171890259, accuracy: 0.609375\n",
      "Batch 59 loss: 1.1019314527511597, accuracy: 0.6087239384651184\n",
      "Batch 60 loss: 1.0553182363510132, accuracy: 0.6082223653793335\n",
      "Batch 61 loss: 1.1298500299453735, accuracy: 0.6071068644523621\n",
      "Batch 62 loss: 1.0195499658584595, accuracy: 0.6073908805847168\n",
      "Batch 63 loss: 1.0449113845825195, accuracy: 0.6075439453125\n",
      "Batch 64 loss: 1.0557246208190918, accuracy: 0.607692301273346\n",
      "Batch 65 loss: 1.0454144477844238, accuracy: 0.607362687587738\n",
      "Batch 66 loss: 0.9542325735092163, accuracy: 0.6080923676490784\n",
      "Batch 67 loss: 0.9421486854553223, accuracy: 0.6084558963775635\n",
      "Batch 68 loss: 1.0010526180267334, accuracy: 0.6083559989929199\n",
      "Batch 69 loss: 1.0392721891403198, accuracy: 0.6084821224212646\n",
      "Batch 70 loss: 0.9112656116485596, accuracy: 0.6097050905227661\n",
      "Batch 71 loss: 1.1457362174987793, accuracy: 0.6090494990348816\n",
      "Batch 72 loss: 1.2266873121261597, accuracy: 0.6077696681022644\n",
      "Batch 73 loss: 0.8939709067344666, accuracy: 0.6086359620094299\n",
      "Batch 74 loss: 0.8702751398086548, accuracy: 0.60958331823349\n",
      "Batch 75 loss: 1.083531141281128, accuracy: 0.6092721819877625\n",
      "Batch 76 loss: 0.9165738821029663, accuracy: 0.6100852489471436\n",
      "Batch 77 loss: 0.9589225649833679, accuracy: 0.6103765964508057\n",
      "Batch 78 loss: 0.8789031505584717, accuracy: 0.6110561490058899\n",
      "Batch 79 loss: 1.0369371175765991, accuracy: 0.611132800579071\n",
      "Batch 80 loss: 1.0469937324523926, accuracy: 0.6112075448036194\n",
      "Batch 81 loss: 1.1579862833023071, accuracy: 0.6105182766914368\n",
      "Batch 82 loss: 0.780771017074585, accuracy: 0.6117281913757324\n",
      "Batch 83 loss: 1.2840845584869385, accuracy: 0.6104910969734192\n",
      "Batch 84 loss: 0.9718766212463379, accuracy: 0.6106617450714111\n",
      "Batch 85 loss: 1.1403937339782715, accuracy: 0.6095566749572754\n",
      "Batch 86 loss: 1.1171048879623413, accuracy: 0.608476996421814\n",
      "Batch 87 loss: 1.0063652992248535, accuracy: 0.6087535619735718\n",
      "Batch 88 loss: 0.982352077960968, accuracy: 0.6092872023582458\n",
      "Batch 89 loss: 1.0607953071594238, accuracy: 0.6087673902511597\n",
      "Batch 90 loss: 1.0358980894088745, accuracy: 0.6094608306884766\n",
      "Batch 91 loss: 1.1126859188079834, accuracy: 0.60988450050354\n",
      "Batch 92 loss: 0.9247711896896362, accuracy: 0.6102990508079529\n",
      "Batch 93 loss: 0.9779319167137146, accuracy: 0.6104554533958435\n",
      "Batch 94 loss: 1.1390190124511719, accuracy: 0.6097039580345154\n",
      "Batch 95 loss: 0.9821951389312744, accuracy: 0.6097819209098816\n",
      "Batch 96 loss: 1.0048903226852417, accuracy: 0.6097776889801025\n",
      "Batch 97 loss: 1.0436811447143555, accuracy: 0.609773576259613\n",
      "Batch 98 loss: 0.8629437685012817, accuracy: 0.6102430820465088\n",
      "Batch 99 loss: 1.0037180185317993, accuracy: 0.6100000143051147\n",
      "Batch 100 loss: 0.890677273273468, accuracy: 0.6108446717262268\n",
      "Batch 101 loss: 0.9641093015670776, accuracy: 0.6109068393707275\n",
      "Batch 102 loss: 1.0564016103744507, accuracy: 0.6105886101722717\n",
      "Batch 103 loss: 1.112255573272705, accuracy: 0.6103515625\n",
      "Batch 104 loss: 0.9079924821853638, accuracy: 0.6109374761581421\n",
      "Batch 105 loss: 0.9301116466522217, accuracy: 0.6108490824699402\n",
      "Batch 106 loss: 0.994404137134552, accuracy: 0.6106892228126526\n",
      "Batch 107 loss: 1.0301827192306519, accuracy: 0.6104600429534912\n",
      "Batch 108 loss: 1.0963423252105713, accuracy: 0.6096616983413696\n",
      "Batch 109 loss: 1.0910805463790894, accuracy: 0.6094460487365723\n",
      "Batch 110 loss: 0.955216646194458, accuracy: 0.6099380850791931\n",
      "Batch 111 loss: 1.045479655265808, accuracy: 0.6099330186843872\n",
      "Batch 112 loss: 0.9770535826683044, accuracy: 0.6105503439903259\n",
      "Batch 113 loss: 1.1819016933441162, accuracy: 0.6097862124443054\n",
      "Batch 114 loss: 0.9498589634895325, accuracy: 0.610190212726593\n",
      "Batch 115 loss: 1.028319239616394, accuracy: 0.6101158261299133\n",
      "Batch 116 loss: 1.042734146118164, accuracy: 0.6097756624221802\n",
      "Batch 117 loss: 1.1391050815582275, accuracy: 0.609375\n",
      "Batch 118 loss: 0.9749836325645447, accuracy: 0.6098345518112183\n",
      "Batch 119 loss: 1.0741510391235352, accuracy: 0.6100911498069763\n",
      "Batch 120 loss: 0.9395443201065063, accuracy: 0.6101498007774353\n",
      "Batch 121 loss: 1.0862953662872314, accuracy: 0.6101434230804443\n",
      "Batch 122 loss: 1.0799747705459595, accuracy: 0.6097561120986938\n",
      "Batch 123 loss: 1.0319550037384033, accuracy: 0.609375\n",
      "Batch 124 loss: 0.8471332788467407, accuracy: 0.6098750233650208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 125 loss: 1.1755008697509766, accuracy: 0.609375\n",
      "Batch 126 loss: 1.089096188545227, accuracy: 0.6093134880065918\n",
      "Batch 127 loss: 0.9991284608840942, accuracy: 0.60894775390625\n",
      "Batch 128 loss: 0.9707427620887756, accuracy: 0.609375\n",
      "Batch 129 loss: 1.1089646816253662, accuracy: 0.6094350814819336\n",
      "Batch 130 loss: 1.0061042308807373, accuracy: 0.6094942688941956\n",
      "Batch 131 loss: 1.0043468475341797, accuracy: 0.6099076867103577\n",
      "Batch 132 loss: 0.9702250361442566, accuracy: 0.6103148460388184\n",
      "Batch 133 loss: 1.1557276248931885, accuracy: 0.6097831130027771\n",
      "Batch 134 loss: 0.9544428586959839, accuracy: 0.6097221970558167\n",
      "Batch 135 loss: 1.1142877340316772, accuracy: 0.6097196936607361\n",
      "Batch 136 loss: 0.9763369560241699, accuracy: 0.6099452376365662\n",
      "Batch 137 loss: 0.9185944199562073, accuracy: 0.6103374361991882\n",
      "Batch 138 loss: 0.8819332122802734, accuracy: 0.6107801198959351\n",
      "Batch 139 loss: 0.937065064907074, accuracy: 0.6107701063156128\n",
      "Batch 140 loss: 1.091568946838379, accuracy: 0.6109263896942139\n",
      "Batch 141 loss: 1.0594102144241333, accuracy: 0.6106404066085815\n",
      "Batch 142 loss: 0.9296919703483582, accuracy: 0.610686182975769\n",
      "Batch 143 loss: 1.0503010749816895, accuracy: 0.6103515625\n",
      "Batch 144 loss: 1.0459316968917847, accuracy: 0.6100215315818787\n",
      "Batch 145 loss: 1.2366597652435303, accuracy: 0.6095890402793884\n",
      "Batch 146 loss: 1.1816508769989014, accuracy: 0.6092155575752258\n",
      "Batch 147 loss: 0.9508329033851624, accuracy: 0.6096389293670654\n",
      "Batch 148 loss: 1.0635087490081787, accuracy: 0.6098993420600891\n",
      "Batch 149 loss: 1.044216275215149, accuracy: 0.609947919845581\n",
      "Batch 150 loss: 0.9416337609291077, accuracy: 0.6097888946533203\n",
      "Batch 151 loss: 1.0402179956436157, accuracy: 0.6095805764198303\n",
      "Batch 152 loss: 0.7832430005073547, accuracy: 0.6104983687400818\n",
      "Batch 153 loss: 1.1281903982162476, accuracy: 0.6100345253944397\n",
      "Batch 154 loss: 0.9916957020759583, accuracy: 0.6100302338600159\n",
      "Batch 155 loss: 0.9001849293708801, accuracy: 0.6102263331413269\n",
      "Batch 156 loss: 1.122462272644043, accuracy: 0.6102706789970398\n",
      "Batch 157 loss: 1.0047088861465454, accuracy: 0.6104628443717957\n",
      "Batch 158 loss: 1.0804897546768188, accuracy: 0.6104068160057068\n",
      "Batch 159 loss: 1.0166374444961548, accuracy: 0.61083984375\n",
      "Batch 160 loss: 1.0820618867874146, accuracy: 0.6104425191879272\n",
      "Batch 161 loss: 0.9637579917907715, accuracy: 0.6106288433074951\n",
      "Batch 162 loss: 1.1644892692565918, accuracy: 0.6100939512252808\n",
      "Batch 163 loss: 1.035471796989441, accuracy: 0.6099466681480408\n",
      "Batch 164 loss: 1.0242619514465332, accuracy: 0.6097537875175476\n",
      "Batch 165 loss: 1.0325512886047363, accuracy: 0.6096574068069458\n",
      "Batch 166 loss: 1.008674144744873, accuracy: 0.610029935836792\n",
      "Batch 167 loss: 1.0853372812271118, accuracy: 0.6097005009651184\n",
      "Batch 168 loss: 0.9552924036979675, accuracy: 0.6097910404205322\n",
      "Batch 169 loss: 1.0848445892333984, accuracy: 0.6098345518112183\n",
      "Batch 170 loss: 1.054752230644226, accuracy: 0.6101059913635254\n",
      "Batch 171 loss: 1.1117452383041382, accuracy: 0.610192596912384\n",
      "Batch 172 loss: 0.9909532070159912, accuracy: 0.6100072264671326\n",
      "Batch 173 loss: 1.0103230476379395, accuracy: 0.6098688840866089\n",
      "Batch 174 loss: 1.0226516723632812, accuracy: 0.6098214387893677\n",
      "Batch 175 loss: 0.8688153028488159, accuracy: 0.6101740002632141\n",
      "Batch 176 loss: 0.9784295558929443, accuracy: 0.6103901863098145\n",
      "Batch 177 loss: 1.227692723274231, accuracy: 0.6099455952644348\n",
      "Batch 178 loss: 1.1510345935821533, accuracy: 0.6097241640090942\n",
      "Batch 179 loss: 0.9425039887428284, accuracy: 0.6099392175674438\n",
      "Batch 180 loss: 1.1215437650680542, accuracy: 0.6097634434700012\n",
      "Batch 181 loss: 1.210374355316162, accuracy: 0.609718382358551\n",
      "Batch 182 loss: 0.9483495950698853, accuracy: 0.6099726557731628\n",
      "Batch 183 loss: 1.1587759256362915, accuracy: 0.609714686870575\n",
      "Batch 184 loss: 1.0671955347061157, accuracy: 0.6097128391265869\n",
      "Batch 185 loss: 1.0411630868911743, accuracy: 0.6097950339317322\n",
      "Batch 186 loss: 0.9069028496742249, accuracy: 0.6102105379104614\n",
      "Batch 187 loss: 1.1220868825912476, accuracy: 0.6098321080207825\n",
      "Batch 188 loss: 1.046674370765686, accuracy: 0.6100363731384277\n",
      "Batch 189 loss: 1.1656726598739624, accuracy: 0.6099506616592407\n",
      "Batch 190 loss: 1.0925419330596924, accuracy: 0.6098658442497253\n",
      "Batch 191 loss: 0.9759259819984436, accuracy: 0.6097819209098816\n",
      "Batch 192 loss: 1.0466536283493042, accuracy: 0.6095774173736572\n",
      "Batch 193 loss: 0.9680144786834717, accuracy: 0.6097776889801025\n",
      "Batch 194 loss: 1.0831955671310425, accuracy: 0.6097355484962463\n",
      "Batch 195 loss: 0.9595056176185608, accuracy: 0.6096938848495483\n",
      "Batch 196 loss: 1.1575456857681274, accuracy: 0.609375\n",
      "Batch 197 loss: 1.2198240756988525, accuracy: 0.6089804172515869\n",
      "Batch 198 loss: 1.02467679977417, accuracy: 0.6091394424438477\n",
      "Batch 199 loss: 1.0138907432556152, accuracy: 0.6091015338897705\n",
      "Batch 200 loss: 0.9047607183456421, accuracy: 0.6092195510864258\n",
      "Batch 201 loss: 1.0006746053695679, accuracy: 0.6094136834144592\n",
      "Batch 202 loss: 1.0919110774993896, accuracy: 0.609182596206665\n",
      "Batch 203 loss: 1.0907453298568726, accuracy: 0.6090686321258545\n",
      "Batch 204 loss: 0.9186749458312988, accuracy: 0.6090320348739624\n",
      "Batch 205 loss: 0.8897781372070312, accuracy: 0.6091853976249695\n",
      "Batch 206 loss: 0.9910619854927063, accuracy: 0.6092617511749268\n",
      "Batch 207 loss: 0.9884124994277954, accuracy: 0.609450101852417\n",
      "Batch 208 loss: 0.9744386076927185, accuracy: 0.6094871163368225\n",
      "Batch 209 loss: 1.0095683336257935, accuracy: 0.609747052192688\n",
      "Batch 210 loss: 1.159834861755371, accuracy: 0.6095601320266724\n",
      "Batch 211 loss: 1.0696948766708374, accuracy: 0.6094855666160583\n",
      "Batch 212 loss: 1.0002318620681763, accuracy: 0.6093383431434631\n",
      "Batch 213 loss: 1.043460726737976, accuracy: 0.609703540802002\n",
      "Batch 214 loss: 1.0643380880355835, accuracy: 0.6096656918525696\n",
      "Batch 215 loss: 0.9027634859085083, accuracy: 0.6098813414573669\n",
      "Batch 216 loss: 0.9848257303237915, accuracy: 0.6097710132598877\n",
      "Batch 217 loss: 1.0169801712036133, accuracy: 0.6101633906364441\n",
      "Batch 218 loss: 1.0174628496170044, accuracy: 0.610195517539978\n",
      "Batch 219 loss: 1.0657107830047607, accuracy: 0.6102628111839294\n",
      "Batch 220 loss: 1.1503050327301025, accuracy: 0.6099405884742737\n",
      "Batch 221 loss: 1.1521053314208984, accuracy: 0.6097269058227539\n",
      "Batch 222 loss: 1.046170711517334, accuracy: 0.6095501780509949\n",
      "Batch 223 loss: 1.1173381805419922, accuracy: 0.6094447374343872\n",
      "Batch 224 loss: 1.149601697921753, accuracy: 0.6093559265136719\n",
      "Training epoch: 12, train accuracy: 60.93559646606445, train loss: 1.0342632677819994, valid accuracy: 60.21175765991211, valid loss: 1.0480987964005306 \n",
      "Batch 0 loss: 0.8463160991668701, accuracy: 0.6796875\n",
      "Batch 1 loss: 0.926037609577179, accuracy: 0.66015625\n",
      "Batch 2 loss: 1.099328875541687, accuracy: 0.6223958134651184\n",
      "Batch 3 loss: 1.0631961822509766, accuracy: 0.6171875\n",
      "Batch 4 loss: 0.8732516765594482, accuracy: 0.629687488079071\n",
      "Batch 5 loss: 0.9526867270469666, accuracy: 0.6236979365348816\n",
      "Batch 6 loss: 1.0418636798858643, accuracy: 0.6194196343421936\n",
      "Batch 7 loss: 1.0258488655090332, accuracy: 0.6162109375\n",
      "Batch 8 loss: 0.9449310898780823, accuracy: 0.6180555820465088\n",
      "Batch 9 loss: 1.0929583311080933, accuracy: 0.617968738079071\n",
      "Batch 10 loss: 1.206943154335022, accuracy: 0.6086647510528564\n",
      "Batch 11 loss: 0.8515680432319641, accuracy: 0.6139323115348816\n",
      "Batch 12 loss: 1.0694869756698608, accuracy: 0.6117788553237915\n",
      "Batch 13 loss: 1.0558143854141235, accuracy: 0.6116071343421936\n",
      "Batch 14 loss: 0.9725441932678223, accuracy: 0.6130208373069763\n",
      "Batch 15 loss: 0.9897827506065369, accuracy: 0.61474609375\n",
      "Batch 16 loss: 0.9807942509651184, accuracy: 0.6158088445663452\n",
      "Batch 17 loss: 0.8777762651443481, accuracy: 0.6197916865348816\n",
      "Batch 18 loss: 0.9998806715011597, accuracy: 0.6221216917037964\n",
      "Batch 19 loss: 0.9215048551559448, accuracy: 0.623828113079071\n",
      "Batch 20 loss: 1.0383737087249756, accuracy: 0.6220238208770752\n",
      "Batch 21 loss: 0.9521879553794861, accuracy: 0.6242897510528564\n",
      "Batch 22 loss: 1.0865697860717773, accuracy: 0.6239809989929199\n",
      "Batch 23 loss: 1.0278211832046509, accuracy: 0.623046875\n",
      "Batch 24 loss: 0.8928983211517334, accuracy: 0.6225000023841858\n",
      "Batch 25 loss: 1.081182837486267, accuracy: 0.6225961446762085\n",
      "Batch 26 loss: 0.8916457295417786, accuracy: 0.6241319179534912\n",
      "Batch 27 loss: 1.0546278953552246, accuracy: 0.6244419813156128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28 loss: 0.9944434762001038, accuracy: 0.623652994632721\n",
      "Batch 29 loss: 0.9473854899406433, accuracy: 0.6260416507720947\n",
      "Batch 30 loss: 1.1797235012054443, accuracy: 0.6237398982048035\n",
      "Batch 31 loss: 0.9391319751739502, accuracy: 0.6240234375\n",
      "Batch 32 loss: 1.1252762079238892, accuracy: 0.6228693127632141\n",
      "Batch 33 loss: 1.034578800201416, accuracy: 0.6213235259056091\n",
      "Batch 34 loss: 0.9604682326316833, accuracy: 0.6214285492897034\n",
      "Batch 35 loss: 1.0383827686309814, accuracy: 0.6221787929534912\n",
      "Batch 36 loss: 1.0503655672073364, accuracy: 0.6218327879905701\n",
      "Batch 37 loss: 0.9307299852371216, accuracy: 0.6215049624443054\n",
      "Batch 38 loss: 1.104972243309021, accuracy: 0.620192289352417\n",
      "Batch 39 loss: 1.061293601989746, accuracy: 0.618945300579071\n",
      "Batch 40 loss: 0.8935157060623169, accuracy: 0.6204268336296082\n",
      "Batch 41 loss: 1.0542280673980713, accuracy: 0.6197916865348816\n",
      "Batch 42 loss: 0.8095529079437256, accuracy: 0.6210029125213623\n",
      "Batch 43 loss: 0.9587827920913696, accuracy: 0.6212713122367859\n",
      "Batch 44 loss: 0.9257382154464722, accuracy: 0.6215277910232544\n",
      "Batch 45 loss: 1.013408899307251, accuracy: 0.6200747489929199\n",
      "Batch 46 loss: 1.0384173393249512, accuracy: 0.6200132966041565\n",
      "Batch 47 loss: 1.063442349433899, accuracy: 0.6194661259651184\n",
      "Batch 48 loss: 1.070260763168335, accuracy: 0.6198979616165161\n",
      "Batch 49 loss: 1.062439203262329, accuracy: 0.6187499761581421\n",
      "Batch 50 loss: 0.9970746040344238, accuracy: 0.6196385025978088\n",
      "Batch 51 loss: 0.9447028040885925, accuracy: 0.6194410920143127\n",
      "Batch 52 loss: 0.9483917951583862, accuracy: 0.6202830076217651\n",
      "Batch 53 loss: 1.0581632852554321, accuracy: 0.6203703880310059\n",
      "Batch 54 loss: 1.032084345817566, accuracy: 0.6201704740524292\n",
      "Batch 55 loss: 1.0938925743103027, accuracy: 0.6196986436843872\n",
      "Batch 56 loss: 0.9956945180892944, accuracy: 0.6193804740905762\n",
      "Batch 57 loss: 0.9891055822372437, accuracy: 0.619746744632721\n",
      "Batch 58 loss: 1.0873984098434448, accuracy: 0.6193061470985413\n",
      "Batch 59 loss: 0.9606251120567322, accuracy: 0.6190103888511658\n",
      "Batch 60 loss: 0.9536451101303101, accuracy: 0.6192367076873779\n",
      "Batch 61 loss: 0.9030399322509766, accuracy: 0.6194556355476379\n",
      "Batch 62 loss: 0.9952383637428284, accuracy: 0.619667649269104\n",
      "Batch 63 loss: 0.9471532106399536, accuracy: 0.6197509765625\n",
      "Batch 64 loss: 1.0076680183410645, accuracy: 0.6199519038200378\n",
      "Batch 65 loss: 0.9307834506034851, accuracy: 0.6197916865348816\n",
      "Batch 66 loss: 1.0540590286254883, accuracy: 0.6191697716712952\n",
      "Batch 67 loss: 1.0068252086639404, accuracy: 0.619140625\n",
      "Batch 68 loss: 1.0650349855422974, accuracy: 0.6192255616188049\n",
      "Batch 69 loss: 1.0503464937210083, accuracy: 0.6188616156578064\n",
      "Batch 70 loss: 1.0318818092346191, accuracy: 0.6186179518699646\n",
      "Batch 71 loss: 1.1520057916641235, accuracy: 0.6180555820465088\n",
      "Batch 72 loss: 1.052007794380188, accuracy: 0.6177226305007935\n",
      "Batch 73 loss: 0.9752903580665588, accuracy: 0.6180320978164673\n",
      "Batch 74 loss: 1.1805391311645508, accuracy: 0.6178125143051147\n",
      "Batch 75 loss: 1.038571834564209, accuracy: 0.6171875\n",
      "Batch 76 loss: 0.9191656708717346, accuracy: 0.6177962422370911\n",
      "Batch 77 loss: 0.9355290532112122, accuracy: 0.6178886294364929\n",
      "Batch 78 loss: 1.0483404397964478, accuracy: 0.617780864238739\n",
      "Batch 79 loss: 1.1116337776184082, accuracy: 0.6172851324081421\n",
      "Batch 80 loss: 0.913344144821167, accuracy: 0.618248462677002\n",
      "Batch 81 loss: 1.0614053010940552, accuracy: 0.6182355284690857\n",
      "Batch 82 loss: 1.048570990562439, accuracy: 0.6183170080184937\n",
      "Batch 83 loss: 1.2338154315948486, accuracy: 0.617001473903656\n",
      "Batch 84 loss: 1.141263484954834, accuracy: 0.616911768913269\n",
      "Batch 85 loss: 1.0500253438949585, accuracy: 0.6171875\n",
      "Batch 86 loss: 0.966698169708252, accuracy: 0.6171875\n",
      "Batch 87 loss: 0.959257185459137, accuracy: 0.6174538135528564\n",
      "Batch 88 loss: 1.1564431190490723, accuracy: 0.6168363690376282\n",
      "Batch 89 loss: 1.0680172443389893, accuracy: 0.6166666746139526\n",
      "Batch 90 loss: 1.0769754648208618, accuracy: 0.6164148449897766\n",
      "Batch 91 loss: 0.9994745850563049, accuracy: 0.6165081262588501\n",
      "Batch 92 loss: 0.9852451086044312, accuracy: 0.6169354915618896\n",
      "Batch 93 loss: 1.059460163116455, accuracy: 0.616771936416626\n",
      "Batch 94 loss: 1.17361319065094, accuracy: 0.6156250238418579\n",
      "Batch 95 loss: 1.0088369846343994, accuracy: 0.6151530146598816\n",
      "Batch 96 loss: 1.0134156942367554, accuracy: 0.6152545213699341\n",
      "Batch 97 loss: 1.0213888883590698, accuracy: 0.6151148080825806\n",
      "Batch 98 loss: 1.0655863285064697, accuracy: 0.6151357293128967\n",
      "Batch 99 loss: 1.0148898363113403, accuracy: 0.6153125166893005\n",
      "Batch 100 loss: 1.0868752002716064, accuracy: 0.6147122383117676\n",
      "Batch 101 loss: 0.9867000579833984, accuracy: 0.6148130893707275\n",
      "Batch 102 loss: 0.9513658285140991, accuracy: 0.6149120330810547\n",
      "Batch 103 loss: 0.8633660078048706, accuracy: 0.6156100034713745\n",
      "Batch 104 loss: 0.9991175532341003, accuracy: 0.6148809790611267\n",
      "Batch 105 loss: 0.959979772567749, accuracy: 0.6156397461891174\n",
      "Batch 106 loss: 1.0492042303085327, accuracy: 0.6159462332725525\n",
      "Batch 107 loss: 0.8763993978500366, accuracy: 0.6163194179534912\n",
      "Batch 108 loss: 1.128401517868042, accuracy: 0.615610659122467\n",
      "Batch 109 loss: 0.863227128982544, accuracy: 0.6165482997894287\n",
      "Batch 110 loss: 0.9870497584342957, accuracy: 0.6169059872627258\n",
      "Batch 111 loss: 1.0836178064346313, accuracy: 0.6162806749343872\n",
      "Batch 112 loss: 1.1875051259994507, accuracy: 0.6158047318458557\n",
      "Batch 113 loss: 1.034121036529541, accuracy: 0.6152686476707458\n",
      "Batch 114 loss: 0.9350563883781433, accuracy: 0.61548912525177\n",
      "Batch 115 loss: 1.0610437393188477, accuracy: 0.6155037879943848\n",
      "Batch 116 loss: 0.9872311353683472, accuracy: 0.6156517267227173\n",
      "Batch 117 loss: 0.9043092727661133, accuracy: 0.6161282062530518\n",
      "Batch 118 loss: 0.9640198945999146, accuracy: 0.6163997054100037\n",
      "Batch 119 loss: 0.8843048214912415, accuracy: 0.6169270873069763\n",
      "Batch 120 loss: 1.1279728412628174, accuracy: 0.61699378490448\n",
      "Batch 121 loss: 0.9729286432266235, accuracy: 0.6175717115402222\n",
      "Batch 122 loss: 0.8770037889480591, accuracy: 0.6179497241973877\n",
      "Batch 123 loss: 1.1027480363845825, accuracy: 0.6177545189857483\n",
      "Batch 124 loss: 1.2239408493041992, accuracy: 0.617312490940094\n",
      "Batch 125 loss: 0.9729494452476501, accuracy: 0.6176215410232544\n",
      "Batch 126 loss: 1.083471417427063, accuracy: 0.617495059967041\n",
      "Batch 127 loss: 1.0414189100265503, accuracy: 0.61773681640625\n",
      "Batch 128 loss: 0.9885026216506958, accuracy: 0.6175508499145508\n",
      "Batch 129 loss: 0.9004123210906982, accuracy: 0.618088960647583\n",
      "Batch 130 loss: 0.9004507660865784, accuracy: 0.6184399127960205\n",
      "Batch 131 loss: 1.2175456285476685, accuracy: 0.6183120012283325\n",
      "Batch 132 loss: 1.138122320175171, accuracy: 0.6181861162185669\n",
      "Batch 133 loss: 0.9188681840896606, accuracy: 0.6184118390083313\n",
      "Batch 134 loss: 1.0192129611968994, accuracy: 0.6179977059364319\n",
      "Batch 135 loss: 1.0173420906066895, accuracy: 0.6178193688392639\n",
      "Batch 136 loss: 1.1322191953659058, accuracy: 0.6174726486206055\n",
      "Batch 137 loss: 0.9417155981063843, accuracy: 0.6178668737411499\n",
      "Batch 138 loss: 0.8896303176879883, accuracy: 0.6183677911758423\n",
      "Batch 139 loss: 1.1129961013793945, accuracy: 0.6178571581840515\n",
      "Batch 140 loss: 1.0864075422286987, accuracy: 0.6175199747085571\n",
      "Batch 141 loss: 1.0385875701904297, accuracy: 0.6177376508712769\n",
      "Batch 142 loss: 1.0239471197128296, accuracy: 0.6181162595748901\n",
      "Batch 143 loss: 0.9862511157989502, accuracy: 0.6181098222732544\n",
      "Batch 144 loss: 0.9716389775276184, accuracy: 0.6181034445762634\n",
      "Batch 145 loss: 0.9551491141319275, accuracy: 0.6179901361465454\n",
      "Batch 146 loss: 1.1536033153533936, accuracy: 0.6178252696990967\n",
      "Batch 147 loss: 1.005881428718567, accuracy: 0.6179265379905701\n",
      "Batch 148 loss: 1.016130805015564, accuracy: 0.6179739832878113\n",
      "Batch 149 loss: 0.9063940644264221, accuracy: 0.6182812452316284\n",
      "Batch 150 loss: 0.9828117489814758, accuracy: 0.6182222962379456\n",
      "Batch 151 loss: 0.9852158427238464, accuracy: 0.6185238361358643\n",
      "Batch 152 loss: 1.0512760877609253, accuracy: 0.618515133857727\n",
      "Batch 153 loss: 1.1678930521011353, accuracy: 0.6182020902633667\n",
      "Batch 154 loss: 1.0463786125183105, accuracy: 0.617943525314331\n",
      "Batch 155 loss: 1.0183007717132568, accuracy: 0.6179386973381042\n",
      "Batch 156 loss: 0.8477684855461121, accuracy: 0.618630588054657\n",
      "Batch 157 loss: 1.055265188217163, accuracy: 0.6188686490058899\n",
      "Batch 158 loss: 0.973331093788147, accuracy: 0.6187106966972351\n",
      "Batch 159 loss: 0.9745157957077026, accuracy: 0.6187988519668579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 160 loss: 1.1131482124328613, accuracy: 0.6182065010070801\n",
      "Batch 161 loss: 0.9468435645103455, accuracy: 0.6183931231498718\n",
      "Batch 162 loss: 1.1735551357269287, accuracy: 0.6183857321739197\n",
      "Batch 163 loss: 1.041513442993164, accuracy: 0.6179973483085632\n",
      "Batch 164 loss: 1.0180212259292603, accuracy: 0.6179450750350952\n",
      "Batch 165 loss: 1.0407483577728271, accuracy: 0.618128776550293\n",
      "Batch 166 loss: 0.9799137115478516, accuracy: 0.6182166934013367\n",
      "Batch 167 loss: 0.8432140946388245, accuracy: 0.6186290979385376\n",
      "Batch 168 loss: 1.0094504356384277, accuracy: 0.618805468082428\n",
      "Batch 169 loss: 1.0892456769943237, accuracy: 0.6186121106147766\n",
      "Batch 170 loss: 0.9372637867927551, accuracy: 0.6187408566474915\n",
      "Batch 171 loss: 0.9093774557113647, accuracy: 0.6186864376068115\n",
      "Batch 172 loss: 0.9555819034576416, accuracy: 0.6186777353286743\n",
      "Batch 173 loss: 1.0097322463989258, accuracy: 0.6188936829566956\n",
      "Batch 174 loss: 0.963496208190918, accuracy: 0.6188392639160156\n",
      "Batch 175 loss: 1.0943927764892578, accuracy: 0.6188299059867859\n",
      "Batch 176 loss: 1.0456212759017944, accuracy: 0.6184675097465515\n",
      "Batch 177 loss: 1.0491811037063599, accuracy: 0.6185480952262878\n",
      "Batch 178 loss: 1.0478817224502563, accuracy: 0.6184532046318054\n",
      "Batch 179 loss: 1.0906788110733032, accuracy: 0.6184461712837219\n",
      "Batch 180 loss: 1.0739128589630127, accuracy: 0.6181802749633789\n",
      "Batch 181 loss: 0.9506199359893799, accuracy: 0.6181747913360596\n",
      "Batch 182 loss: 0.9262211322784424, accuracy: 0.6183828711509705\n",
      "Batch 183 loss: 1.0479408502578735, accuracy: 0.618291437625885\n",
      "Batch 184 loss: 0.9724161028862, accuracy: 0.6184965968132019\n",
      "Batch 185 loss: 1.0658541917800903, accuracy: 0.6182795763015747\n",
      "Batch 186 loss: 0.8881652355194092, accuracy: 0.6186915040016174\n",
      "Batch 187 loss: 1.021668791770935, accuracy: 0.6188081502914429\n",
      "Batch 188 loss: 0.9640885591506958, accuracy: 0.6187582612037659\n",
      "Batch 189 loss: 1.0895055532455444, accuracy: 0.6185032725334167\n",
      "Batch 190 loss: 1.1167490482330322, accuracy: 0.618373692035675\n",
      "Batch 191 loss: 0.9500770568847656, accuracy: 0.61865234375\n",
      "Batch 192 loss: 0.8586772680282593, accuracy: 0.6189686059951782\n",
      "Batch 193 loss: 1.0782535076141357, accuracy: 0.6188386082649231\n",
      "Batch 194 loss: 1.0475367307662964, accuracy: 0.6189503073692322\n",
      "Batch 195 loss: 0.9896407127380371, accuracy: 0.6189413070678711\n",
      "Batch 196 loss: 1.1165322065353394, accuracy: 0.6187341213226318\n",
      "Batch 197 loss: 1.0848934650421143, accuracy: 0.6184895634651184\n",
      "Batch 198 loss: 1.0328556299209595, accuracy: 0.6184437870979309\n",
      "Batch 199 loss: 1.11993408203125, accuracy: 0.6180468797683716\n",
      "Batch 200 loss: 0.8994485139846802, accuracy: 0.6184312701225281\n",
      "Batch 201 loss: 1.0884451866149902, accuracy: 0.6184251308441162\n",
      "Batch 202 loss: 0.9474482536315918, accuracy: 0.6184960007667542\n",
      "Batch 203 loss: 0.9575842618942261, accuracy: 0.618527889251709\n",
      "Batch 204 loss: 0.9148628115653992, accuracy: 0.6187499761581421\n",
      "Batch 205 loss: 1.1136305332183838, accuracy: 0.6184390187263489\n",
      "Batch 206 loss: 1.0514730215072632, accuracy: 0.618395209312439\n",
      "Batch 207 loss: 0.9487990736961365, accuracy: 0.6185020804405212\n",
      "Batch 208 loss: 0.9480035901069641, accuracy: 0.6186827421188354\n",
      "Batch 209 loss: 0.8832643032073975, accuracy: 0.6189360022544861\n",
      "Batch 210 loss: 0.9949910640716553, accuracy: 0.6188536882400513\n",
      "Batch 211 loss: 0.8520709276199341, accuracy: 0.6190300583839417\n",
      "Batch 212 loss: 0.9095650315284729, accuracy: 0.6190580725669861\n",
      "Batch 213 loss: 1.017399549484253, accuracy: 0.6188303232192993\n",
      "Batch 214 loss: 0.9504783153533936, accuracy: 0.6189680099487305\n",
      "Batch 215 loss: 1.0354328155517578, accuracy: 0.6190321445465088\n",
      "Batch 216 loss: 1.070222020149231, accuracy: 0.6188796162605286\n",
      "Batch 217 loss: 1.0830973386764526, accuracy: 0.618800163269043\n",
      "Batch 218 loss: 1.1691715717315674, accuracy: 0.6185787916183472\n",
      "Batch 219 loss: 0.9307928085327148, accuracy: 0.6186789870262146\n",
      "Batch 220 loss: 1.0299323797225952, accuracy: 0.6183187365531921\n",
      "Batch 221 loss: 0.8975690007209778, accuracy: 0.6184192299842834\n",
      "Batch 222 loss: 1.0055196285247803, accuracy: 0.6183435916900635\n",
      "Batch 223 loss: 1.0172113180160522, accuracy: 0.6182337999343872\n",
      "Batch 224 loss: 1.0111087560653687, accuracy: 0.6182382106781006\n",
      "Training epoch: 13, train accuracy: 61.82381820678711, train loss: 1.0118177819252014, valid accuracy: 60.88046646118164, valid loss: 1.0453797455491691 \n",
      "Batch 0 loss: 1.0332452058792114, accuracy: 0.609375\n",
      "Batch 1 loss: 1.0723069906234741, accuracy: 0.609375\n",
      "Batch 2 loss: 0.8664728403091431, accuracy: 0.6354166865348816\n",
      "Batch 3 loss: 0.9761770963668823, accuracy: 0.63671875\n",
      "Batch 4 loss: 1.0393387079238892, accuracy: 0.635937511920929\n",
      "Batch 5 loss: 1.1063662767410278, accuracy: 0.6393229365348816\n",
      "Batch 6 loss: 0.9666424989700317, accuracy: 0.6305803656578064\n",
      "Batch 7 loss: 1.069871425628662, accuracy: 0.630859375\n",
      "Batch 8 loss: 1.0039300918579102, accuracy: 0.6380208134651184\n",
      "Batch 9 loss: 0.8643072843551636, accuracy: 0.6421874761581421\n",
      "Batch 10 loss: 0.9977676272392273, accuracy: 0.6384943127632141\n",
      "Batch 11 loss: 1.0506765842437744, accuracy: 0.6321614384651184\n",
      "Batch 12 loss: 1.0595959424972534, accuracy: 0.6310096383094788\n",
      "Batch 13 loss: 1.0850733518600464, accuracy: 0.6277901530265808\n",
      "Batch 14 loss: 1.035339593887329, accuracy: 0.6265624761581421\n",
      "Batch 15 loss: 0.891755998134613, accuracy: 0.62744140625\n",
      "Batch 16 loss: 0.9906388521194458, accuracy: 0.6259191036224365\n",
      "Batch 17 loss: 1.0608839988708496, accuracy: 0.6219618320465088\n",
      "Batch 18 loss: 0.838020384311676, accuracy: 0.625\n",
      "Batch 19 loss: 0.9887964129447937, accuracy: 0.6234375238418579\n",
      "Batch 20 loss: 1.09331214427948, accuracy: 0.6227678656578064\n",
      "Batch 21 loss: 1.0261939764022827, accuracy: 0.6235795617103577\n",
      "Batch 22 loss: 1.0254712104797363, accuracy: 0.6229619383811951\n",
      "Batch 23 loss: 0.9954245090484619, accuracy: 0.6233723759651184\n",
      "Batch 24 loss: 1.0203992128372192, accuracy: 0.6234375238418579\n",
      "Batch 25 loss: 1.0830317735671997, accuracy: 0.6216946840286255\n",
      "Batch 26 loss: 1.0041688680648804, accuracy: 0.6197916865348816\n",
      "Batch 27 loss: 1.1357382535934448, accuracy: 0.6183035969734192\n",
      "Batch 28 loss: 1.136655569076538, accuracy: 0.618534505367279\n",
      "Batch 29 loss: 1.0936245918273926, accuracy: 0.6171875\n",
      "Batch 30 loss: 0.9635705947875977, accuracy: 0.6174395084381104\n",
      "Batch 31 loss: 1.0938997268676758, accuracy: 0.618408203125\n",
      "Batch 32 loss: 0.9407946467399597, accuracy: 0.6186079382896423\n",
      "Batch 33 loss: 0.9119464159011841, accuracy: 0.6194853186607361\n",
      "Batch 34 loss: 1.134443759918213, accuracy: 0.6200892925262451\n",
      "Batch 35 loss: 1.0106935501098633, accuracy: 0.6195746660232544\n",
      "Batch 36 loss: 0.8591631650924683, accuracy: 0.6205658912658691\n",
      "Batch 37 loss: 0.9973376989364624, accuracy: 0.6215049624443054\n",
      "Batch 38 loss: 0.9904339909553528, accuracy: 0.6213942170143127\n",
      "Batch 39 loss: 0.8541774153709412, accuracy: 0.6226562261581421\n",
      "Batch 40 loss: 0.953639566898346, accuracy: 0.6234756112098694\n",
      "Batch 41 loss: 1.0507296323776245, accuracy: 0.6220238208770752\n",
      "Batch 42 loss: 0.9276407957077026, accuracy: 0.621911346912384\n",
      "Batch 43 loss: 1.1837269067764282, accuracy: 0.6205610632896423\n",
      "Batch 44 loss: 0.9998658299446106, accuracy: 0.6199652552604675\n",
      "Batch 45 loss: 0.9118831753730774, accuracy: 0.62058424949646\n",
      "Batch 46 loss: 0.806034505367279, accuracy: 0.621509313583374\n",
      "Batch 47 loss: 0.9892059564590454, accuracy: 0.62158203125\n",
      "Batch 48 loss: 0.9979482293128967, accuracy: 0.6221300959587097\n",
      "Batch 49 loss: 0.8560317158699036, accuracy: 0.6231250166893005\n",
      "Batch 50 loss: 0.9319488406181335, accuracy: 0.6245404481887817\n",
      "Batch 51 loss: 0.9179739356040955, accuracy: 0.6248497366905212\n",
      "Batch 52 loss: 0.9082354307174683, accuracy: 0.6248525977134705\n",
      "Batch 53 loss: 1.0842084884643555, accuracy: 0.6238425970077515\n",
      "Batch 54 loss: 0.8788363337516785, accuracy: 0.6251420378684998\n",
      "Batch 55 loss: 1.2056623697280884, accuracy: 0.623046875\n",
      "Batch 56 loss: 0.954870343208313, accuracy: 0.6233552694320679\n",
      "Batch 57 loss: 1.0342025756835938, accuracy: 0.623383641242981\n",
      "Batch 58 loss: 1.0749262571334839, accuracy: 0.6231461763381958\n",
      "Batch 59 loss: 1.1356513500213623, accuracy: 0.6204426884651184\n",
      "Batch 60 loss: 1.1312886476516724, accuracy: 0.6192367076873779\n",
      "Batch 61 loss: 0.9601201415061951, accuracy: 0.6181955933570862\n",
      "Batch 62 loss: 0.9091241955757141, accuracy: 0.6185516119003296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 63 loss: 0.8771227598190308, accuracy: 0.6195068359375\n",
      "Batch 64 loss: 1.0025259256362915, accuracy: 0.6189903616905212\n",
      "Batch 65 loss: 0.9145697355270386, accuracy: 0.6196733117103577\n",
      "Batch 66 loss: 1.1571072340011597, accuracy: 0.619752824306488\n",
      "Batch 67 loss: 1.0053731203079224, accuracy: 0.6194853186607361\n",
      "Batch 68 loss: 0.8851907253265381, accuracy: 0.620244562625885\n",
      "Batch 69 loss: 0.9161132574081421, accuracy: 0.62109375\n",
      "Batch 70 loss: 1.003891110420227, accuracy: 0.6209287047386169\n",
      "Batch 71 loss: 0.911888837814331, accuracy: 0.6221787929534912\n",
      "Batch 72 loss: 0.8633341193199158, accuracy: 0.6226455569267273\n",
      "Batch 73 loss: 1.0111323595046997, accuracy: 0.6227829456329346\n",
      "Batch 74 loss: 0.9629806876182556, accuracy: 0.6231250166893005\n",
      "Batch 75 loss: 0.8623493313789368, accuracy: 0.6232524514198303\n",
      "Batch 76 loss: 0.9434892535209656, accuracy: 0.6232751607894897\n",
      "Batch 77 loss: 0.9983761310577393, accuracy: 0.6228966116905212\n",
      "Batch 78 loss: 1.0551624298095703, accuracy: 0.6219343543052673\n",
      "Batch 79 loss: 1.0858205556869507, accuracy: 0.6219726800918579\n",
      "Batch 80 loss: 0.9599106907844543, accuracy: 0.6227816343307495\n",
      "Batch 81 loss: 0.8767489790916443, accuracy: 0.6236661672592163\n",
      "Batch 82 loss: 0.9797118306159973, accuracy: 0.6234939694404602\n",
      "Batch 83 loss: 1.0001212358474731, accuracy: 0.6231398582458496\n",
      "Batch 84 loss: 0.9971840977668762, accuracy: 0.6233456134796143\n",
      "Batch 85 loss: 1.046448826789856, accuracy: 0.6226381063461304\n",
      "Batch 86 loss: 1.2434715032577515, accuracy: 0.6215876340866089\n",
      "Batch 87 loss: 0.8374825716018677, accuracy: 0.6226029992103577\n",
      "Batch 88 loss: 0.9769228100776672, accuracy: 0.6231566071510315\n",
      "Batch 89 loss: 0.9964568018913269, accuracy: 0.6226562261581421\n",
      "Batch 90 loss: 1.0171152353286743, accuracy: 0.6224244236946106\n",
      "Batch 91 loss: 0.8706114292144775, accuracy: 0.6232166886329651\n",
      "Batch 92 loss: 0.906591534614563, accuracy: 0.6230678558349609\n",
      "Batch 93 loss: 0.990418553352356, accuracy: 0.623254656791687\n",
      "Batch 94 loss: 0.9534158110618591, accuracy: 0.6240131855010986\n",
      "Batch 95 loss: 0.884288489818573, accuracy: 0.6243489384651184\n",
      "Batch 96 loss: 0.9226839542388916, accuracy: 0.6242751479148865\n",
      "Batch 97 loss: 0.8490116000175476, accuracy: 0.625\n",
      "Batch 98 loss: 0.881324827671051, accuracy: 0.6254734992980957\n",
      "Batch 99 loss: 1.0211173295974731, accuracy: 0.6260156035423279\n",
      "Batch 100 loss: 0.9427326917648315, accuracy: 0.6261602640151978\n",
      "Batch 101 loss: 1.1974756717681885, accuracy: 0.6256893277168274\n",
      "Batch 102 loss: 1.0154553651809692, accuracy: 0.6258343458175659\n",
      "Batch 103 loss: 0.9263811707496643, accuracy: 0.6265023946762085\n",
      "Batch 104 loss: 0.9762176871299744, accuracy: 0.6267856955528259\n",
      "Batch 105 loss: 0.8568344712257385, accuracy: 0.6274321675300598\n",
      "Batch 106 loss: 0.9599010944366455, accuracy: 0.6277015209197998\n",
      "Batch 107 loss: 1.0503246784210205, accuracy: 0.6274594664573669\n",
      "Batch 108 loss: 1.0330371856689453, accuracy: 0.6275085806846619\n",
      "Batch 109 loss: 1.0137310028076172, accuracy: 0.6272016763687134\n",
      "Batch 110 loss: 1.0212836265563965, accuracy: 0.627181887626648\n",
      "Batch 111 loss: 0.9810823798179626, accuracy: 0.6270228624343872\n",
      "Batch 112 loss: 1.026004672050476, accuracy: 0.6271432638168335\n",
      "Batch 113 loss: 0.8909711837768555, accuracy: 0.6272615194320679\n",
      "Batch 114 loss: 1.1269041299819946, accuracy: 0.6262907385826111\n",
      "Batch 115 loss: 1.022422432899475, accuracy: 0.6264143586158752\n",
      "Batch 116 loss: 1.0469119548797607, accuracy: 0.6260015964508057\n",
      "Batch 117 loss: 1.0884414911270142, accuracy: 0.6260592937469482\n",
      "Batch 118 loss: 0.9989986419677734, accuracy: 0.6257221698760986\n",
      "Batch 119 loss: 0.6999423503875732, accuracy: 0.6263672113418579\n",
      "Batch 120 loss: 0.9507831931114197, accuracy: 0.6262913346290588\n",
      "Batch 121 loss: 0.9329899549484253, accuracy: 0.6267290115356445\n",
      "Batch 122 loss: 1.0748295783996582, accuracy: 0.6265243887901306\n",
      "Batch 123 loss: 1.0497044324874878, accuracy: 0.6260080933570862\n",
      "Batch 124 loss: 1.0064219236373901, accuracy: 0.6258749961853027\n",
      "Batch 125 loss: 1.024012804031372, accuracy: 0.626054048538208\n",
      "Batch 126 loss: 0.9844928979873657, accuracy: 0.6257382035255432\n",
      "Batch 127 loss: 0.9653328657150269, accuracy: 0.62548828125\n",
      "Batch 128 loss: 0.998712420463562, accuracy: 0.6257873177528381\n",
      "Batch 129 loss: 0.8787522315979004, accuracy: 0.6263822317123413\n",
      "Batch 130 loss: 0.98977130651474, accuracy: 0.6263120174407959\n",
      "Batch 131 loss: 1.0501891374588013, accuracy: 0.6259469985961914\n",
      "Batch 132 loss: 0.9400089979171753, accuracy: 0.6261160969734192\n",
      "Batch 133 loss: 0.9052793979644775, accuracy: 0.6265158653259277\n",
      "Batch 134 loss: 1.1034026145935059, accuracy: 0.6259837746620178\n",
      "Batch 135 loss: 0.9528990983963013, accuracy: 0.6260340213775635\n",
      "Batch 136 loss: 1.0876214504241943, accuracy: 0.6258553862571716\n",
      "Batch 137 loss: 0.8954287767410278, accuracy: 0.6261322498321533\n",
      "Batch 138 loss: 0.9932506680488586, accuracy: 0.6259554624557495\n",
      "Batch 139 loss: 1.1100889444351196, accuracy: 0.6258370280265808\n",
      "Batch 140 loss: 1.0111092329025269, accuracy: 0.6259973645210266\n",
      "Batch 141 loss: 0.9073012471199036, accuracy: 0.6259903311729431\n",
      "Batch 142 loss: 1.1175700426101685, accuracy: 0.6254916787147522\n",
      "Batch 143 loss: 0.9517155289649963, accuracy: 0.6255425214767456\n",
      "Batch 144 loss: 1.0369964838027954, accuracy: 0.6254310607910156\n",
      "Batch 145 loss: 0.9912325739860535, accuracy: 0.6253745555877686\n",
      "Batch 146 loss: 1.0528351068496704, accuracy: 0.6252657175064087\n",
      "Batch 147 loss: 0.9288514852523804, accuracy: 0.6256862282752991\n",
      "Batch 148 loss: 0.9661793112754822, accuracy: 0.6257864832878113\n",
      "Batch 149 loss: 0.9147505164146423, accuracy: 0.6257291436195374\n",
      "Batch 150 loss: 0.9404324889183044, accuracy: 0.6259830594062805\n",
      "Batch 151 loss: 1.0422728061676025, accuracy: 0.6260279417037964\n",
      "Batch 152 loss: 0.8648813366889954, accuracy: 0.6262765526771545\n",
      "Batch 153 loss: 0.9642890095710754, accuracy: 0.6264204382896423\n",
      "Batch 154 loss: 1.098212718963623, accuracy: 0.6264616847038269\n",
      "Batch 155 loss: 0.8856012225151062, accuracy: 0.6266526579856873\n",
      "Batch 156 loss: 0.8454580903053284, accuracy: 0.6268908977508545\n",
      "Batch 157 loss: 0.8720465898513794, accuracy: 0.627126157283783\n",
      "Batch 158 loss: 1.0651592016220093, accuracy: 0.6267688870429993\n",
      "Batch 159 loss: 0.8766418099403381, accuracy: 0.626904308795929\n",
      "Batch 160 loss: 1.0353238582611084, accuracy: 0.6268439292907715\n",
      "Batch 161 loss: 0.9837676882743835, accuracy: 0.6268807649612427\n",
      "Batch 162 loss: 1.0140222311019897, accuracy: 0.6266775131225586\n",
      "Batch 163 loss: 0.7762822508811951, accuracy: 0.6270007491111755\n",
      "Batch 164 loss: 0.906331479549408, accuracy: 0.6272727251052856\n",
      "Batch 165 loss: 1.090833067893982, accuracy: 0.6268354654312134\n",
      "Batch 166 loss: 0.9364760518074036, accuracy: 0.6265905499458313\n",
      "Batch 167 loss: 1.204124093055725, accuracy: 0.6262090802192688\n",
      "Batch 168 loss: 0.8967912197113037, accuracy: 0.6264330744743347\n",
      "Batch 169 loss: 0.9801098108291626, accuracy: 0.6263786554336548\n",
      "Batch 170 loss: 0.957868754863739, accuracy: 0.6263249516487122\n",
      "Batch 171 loss: 1.0448362827301025, accuracy: 0.6262717843055725\n",
      "Batch 172 loss: 0.9401193857192993, accuracy: 0.6263999342918396\n",
      "Batch 173 loss: 1.0808448791503906, accuracy: 0.626167356967926\n",
      "Batch 174 loss: 1.176566481590271, accuracy: 0.6259375214576721\n",
      "Batch 175 loss: 0.9896166920661926, accuracy: 0.6261097192764282\n",
      "Batch 176 loss: 1.0017516613006592, accuracy: 0.6260592937469482\n",
      "Batch 177 loss: 0.8420020341873169, accuracy: 0.6265361905097961\n",
      "Batch 178 loss: 1.17965829372406, accuracy: 0.6260474920272827\n",
      "Batch 179 loss: 0.9322218298912048, accuracy: 0.6260850429534912\n",
      "Batch 180 loss: 1.0828009843826294, accuracy: 0.6258201003074646\n",
      "Batch 181 loss: 0.9381973147392273, accuracy: 0.6260731220245361\n",
      "Batch 182 loss: 0.9773639440536499, accuracy: 0.6260672807693481\n",
      "Batch 183 loss: 0.9262524843215942, accuracy: 0.62648606300354\n",
      "Batch 184 loss: 0.9571523666381836, accuracy: 0.6267313957214355\n",
      "Batch 185 loss: 0.9913256168365479, accuracy: 0.6268481016159058\n",
      "Batch 186 loss: 0.8736068606376648, accuracy: 0.6271306872367859\n",
      "Batch 187 loss: 0.9886630773544312, accuracy: 0.6271193623542786\n",
      "Batch 188 loss: 1.009970784187317, accuracy: 0.6269841194152832\n",
      "Batch 189 loss: 1.0549612045288086, accuracy: 0.6267681121826172\n",
      "Batch 190 loss: 0.9656276106834412, accuracy: 0.6267997622489929\n",
      "Batch 191 loss: 1.00156569480896, accuracy: 0.6268717646598816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 192 loss: 1.0127211809158325, accuracy: 0.6269025206565857\n",
      "Batch 193 loss: 0.8726955652236938, accuracy: 0.6268524527549744\n",
      "Batch 194 loss: 1.0423283576965332, accuracy: 0.6268429756164551\n",
      "Batch 195 loss: 1.052004337310791, accuracy: 0.6266741156578064\n",
      "Batch 196 loss: 1.0407345294952393, accuracy: 0.6265863180160522\n",
      "Batch 197 loss: 1.095867395401001, accuracy: 0.6264204382896423\n",
      "Batch 198 loss: 1.0269503593444824, accuracy: 0.6263347864151001\n",
      "Batch 199 loss: 1.0884438753128052, accuracy: 0.625781238079071\n",
      "Batch 200 loss: 0.9962863922119141, accuracy: 0.6259716749191284\n",
      "Batch 201 loss: 1.0730044841766357, accuracy: 0.6258508563041687\n",
      "Batch 202 loss: 0.9460049867630005, accuracy: 0.6261545419692993\n",
      "Batch 203 loss: 0.8438618779182434, accuracy: 0.6261872053146362\n",
      "Batch 204 loss: 1.0663081407546997, accuracy: 0.6259908676147461\n",
      "Batch 205 loss: 0.9808949828147888, accuracy: 0.6259860396385193\n",
      "Batch 206 loss: 0.9334335327148438, accuracy: 0.626207709312439\n",
      "Batch 207 loss: 1.158333420753479, accuracy: 0.6259765625\n",
      "Batch 208 loss: 0.9632636904716492, accuracy: 0.6260092854499817\n",
      "Batch 209 loss: 1.1173632144927979, accuracy: 0.6258556842803955\n",
      "Batch 210 loss: 0.9617993235588074, accuracy: 0.6255553960800171\n",
      "Batch 211 loss: 1.0304162502288818, accuracy: 0.6257001757621765\n",
      "Batch 212 loss: 1.016457438468933, accuracy: 0.6254034638404846\n",
      "Batch 213 loss: 0.9814282655715942, accuracy: 0.6254015564918518\n",
      "Batch 214 loss: 0.9607061147689819, accuracy: 0.6254360675811768\n",
      "Batch 215 loss: 1.0501513481140137, accuracy: 0.6256148815155029\n",
      "Batch 216 loss: 1.0434057712554932, accuracy: 0.6255040168762207\n",
      "Batch 217 loss: 1.0315003395080566, accuracy: 0.6252149939537048\n",
      "Batch 218 loss: 0.9770466089248657, accuracy: 0.6252853870391846\n",
      "Batch 219 loss: 0.8735338449478149, accuracy: 0.6254971623420715\n",
      "Batch 220 loss: 1.0209193229675293, accuracy: 0.625318169593811\n",
      "Batch 221 loss: 1.0419942140579224, accuracy: 0.6250703930854797\n",
      "Batch 222 loss: 0.9862000346183777, accuracy: 0.625105082988739\n",
      "Batch 223 loss: 0.9189873933792114, accuracy: 0.6252790093421936\n",
      "Batch 224 loss: 1.0781540870666504, accuracy: 0.6253787875175476\n",
      "Training epoch: 14, train accuracy: 62.537879943847656, train loss: 0.9909409297837152, valid accuracy: 61.85567092895508, valid loss: 1.0449735768910111 \n",
      "Batch 0 loss: 0.8477600812911987, accuracy: 0.65625\n",
      "Batch 1 loss: 0.8882431983947754, accuracy: 0.6640625\n",
      "Batch 2 loss: 0.889117419719696, accuracy: 0.6510416865348816\n",
      "Batch 3 loss: 1.0942211151123047, accuracy: 0.63671875\n",
      "Batch 4 loss: 0.9550469517707825, accuracy: 0.645312488079071\n",
      "Batch 5 loss: 0.9363758563995361, accuracy: 0.6484375\n",
      "Batch 6 loss: 1.0018112659454346, accuracy: 0.640625\n",
      "Batch 7 loss: 0.9720981121063232, accuracy: 0.6396484375\n",
      "Batch 8 loss: 1.0026558637619019, accuracy: 0.6328125\n",
      "Batch 9 loss: 1.0616375207901, accuracy: 0.628125011920929\n",
      "Batch 10 loss: 1.0043952465057373, accuracy: 0.6271306872367859\n",
      "Batch 11 loss: 0.9366235733032227, accuracy: 0.6328125\n",
      "Batch 12 loss: 0.9781310558319092, accuracy: 0.6340144276618958\n",
      "Batch 13 loss: 1.1879042387008667, accuracy: 0.6300223469734192\n",
      "Batch 14 loss: 0.9024696350097656, accuracy: 0.6307291388511658\n",
      "Batch 15 loss: 0.9528769254684448, accuracy: 0.62939453125\n",
      "Batch 16 loss: 1.079587697982788, accuracy: 0.6227021813392639\n",
      "Batch 17 loss: 0.8674997091293335, accuracy: 0.6263020634651184\n",
      "Batch 18 loss: 0.9864221811294556, accuracy: 0.6241776347160339\n",
      "Batch 19 loss: 0.9181530475616455, accuracy: 0.6265624761581421\n",
      "Batch 20 loss: 0.8734853267669678, accuracy: 0.6320684552192688\n",
      "Batch 21 loss: 1.0054292678833008, accuracy: 0.6324573755264282\n",
      "Batch 22 loss: 0.8925607800483704, accuracy: 0.6351901888847351\n",
      "Batch 23 loss: 0.86394202709198, accuracy: 0.634765625\n",
      "Batch 24 loss: 0.8705638647079468, accuracy: 0.6356250047683716\n",
      "Batch 25 loss: 0.8310927152633667, accuracy: 0.6373196840286255\n",
      "Batch 26 loss: 1.2321897745132446, accuracy: 0.6328125\n",
      "Batch 27 loss: 0.7782201766967773, accuracy: 0.6353236436843872\n",
      "Batch 28 loss: 1.042836308479309, accuracy: 0.6346982717514038\n",
      "Batch 29 loss: 1.0314379930496216, accuracy: 0.6343749761581421\n",
      "Batch 30 loss: 0.9723582863807678, accuracy: 0.6340726017951965\n",
      "Batch 31 loss: 1.2340377569198608, accuracy: 0.6318359375\n",
      "Batch 32 loss: 1.1016488075256348, accuracy: 0.6299715638160706\n",
      "Batch 33 loss: 0.9868228435516357, accuracy: 0.6298253536224365\n",
      "Batch 34 loss: 1.0107531547546387, accuracy: 0.6292410492897034\n",
      "Batch 35 loss: 0.8498697280883789, accuracy: 0.6310763955116272\n",
      "Batch 36 loss: 0.8414731621742249, accuracy: 0.6317567825317383\n",
      "Batch 37 loss: 1.0092726945877075, accuracy: 0.6324012875556946\n",
      "Batch 38 loss: 1.144215703010559, accuracy: 0.6316105723381042\n",
      "Batch 39 loss: 1.0075111389160156, accuracy: 0.630859375\n",
      "Batch 40 loss: 0.9939501285552979, accuracy: 0.6305258870124817\n",
      "Batch 41 loss: 0.915081799030304, accuracy: 0.6309523582458496\n",
      "Batch 42 loss: 0.9609241485595703, accuracy: 0.6320857405662537\n",
      "Batch 43 loss: 0.8645840287208557, accuracy: 0.6321022510528564\n",
      "Batch 44 loss: 0.8506274819374084, accuracy: 0.6335069537162781\n",
      "Batch 45 loss: 1.0059561729431152, accuracy: 0.63332200050354\n",
      "Batch 46 loss: 1.0521472692489624, accuracy: 0.631482720375061\n",
      "Batch 47 loss: 1.078856110572815, accuracy: 0.6298828125\n",
      "Batch 48 loss: 0.9192348718643188, accuracy: 0.6301020383834839\n",
      "Batch 49 loss: 0.9434720873832703, accuracy: 0.6304687261581421\n",
      "Batch 50 loss: 0.9739011526107788, accuracy: 0.6299019455909729\n",
      "Batch 51 loss: 1.127246379852295, accuracy: 0.62890625\n",
      "Batch 52 loss: 1.1083093881607056, accuracy: 0.6280955076217651\n",
      "Batch 53 loss: 0.9020265340805054, accuracy: 0.6290509104728699\n",
      "Batch 54 loss: 1.1869584321975708, accuracy: 0.6279829740524292\n",
      "Batch 55 loss: 1.1648943424224854, accuracy: 0.6270926594734192\n",
      "Batch 56 loss: 1.0679030418395996, accuracy: 0.6262335777282715\n",
      "Batch 57 loss: 1.0485860109329224, accuracy: 0.6258081793785095\n",
      "Batch 58 loss: 0.9622227549552917, accuracy: 0.6259269118309021\n",
      "Batch 59 loss: 0.8787428736686707, accuracy: 0.6263020634651184\n",
      "Batch 60 loss: 1.062004566192627, accuracy: 0.6262807250022888\n",
      "Batch 61 loss: 1.0106045007705688, accuracy: 0.6272681355476379\n",
      "Batch 62 loss: 0.9707146883010864, accuracy: 0.6269841194152832\n",
      "Batch 63 loss: 1.0287139415740967, accuracy: 0.626220703125\n",
      "Batch 64 loss: 0.972588300704956, accuracy: 0.6253605484962463\n",
      "Batch 65 loss: 1.1006659269332886, accuracy: 0.6245265007019043\n",
      "Batch 66 loss: 0.9079141616821289, accuracy: 0.6254664063453674\n",
      "Batch 67 loss: 0.9358372688293457, accuracy: 0.6266084313392639\n",
      "Batch 68 loss: 0.877498209476471, accuracy: 0.6273776888847351\n",
      "Batch 69 loss: 0.9172370433807373, accuracy: 0.6276785731315613\n",
      "Batch 70 loss: 0.970211923122406, accuracy: 0.6270906925201416\n",
      "Batch 71 loss: 1.050459623336792, accuracy: 0.6270616054534912\n",
      "Batch 72 loss: 1.0471726655960083, accuracy: 0.6266053318977356\n",
      "Batch 73 loss: 1.0683342218399048, accuracy: 0.6258445978164673\n",
      "Batch 74 loss: 0.961793065071106, accuracy: 0.6266666650772095\n",
      "Batch 75 loss: 1.0405806303024292, accuracy: 0.6268503069877625\n",
      "Batch 76 loss: 1.0125935077667236, accuracy: 0.6267248392105103\n",
      "Batch 77 loss: 0.903388500213623, accuracy: 0.6275039911270142\n",
      "Batch 78 loss: 0.9426499605178833, accuracy: 0.6276701092720032\n",
      "Batch 79 loss: 0.7110689878463745, accuracy: 0.6292968988418579\n",
      "Batch 80 loss: 0.9012131690979004, accuracy: 0.6295331716537476\n",
      "Batch 81 loss: 0.9306703209877014, accuracy: 0.6299542784690857\n",
      "Batch 82 loss: 1.0128676891326904, accuracy: 0.6291415691375732\n",
      "Batch 83 loss: 1.1042362451553345, accuracy: 0.6281622052192688\n",
      "Batch 84 loss: 0.917879045009613, accuracy: 0.6285845637321472\n",
      "Batch 85 loss: 0.9111467599868774, accuracy: 0.62890625\n",
      "Batch 86 loss: 0.9156762361526489, accuracy: 0.6291307210922241\n",
      "Batch 87 loss: 0.9695938229560852, accuracy: 0.6290838122367859\n",
      "Batch 88 loss: 0.8791846036911011, accuracy: 0.6294768452644348\n",
      "Batch 89 loss: 0.8390206098556519, accuracy: 0.6300347447395325\n",
      "Batch 90 loss: 0.8866971135139465, accuracy: 0.6304945349693298\n",
      "Batch 91 loss: 0.9087094664573669, accuracy: 0.630264937877655\n",
      "Batch 92 loss: 0.9681313633918762, accuracy: 0.6305443644523621\n",
      "Batch 93 loss: 0.8647330403327942, accuracy: 0.6309009194374084\n",
      "Batch 94 loss: 1.0379866361618042, accuracy: 0.6307565569877625\n",
      "Batch 95 loss: 0.9764886498451233, accuracy: 0.6300455927848816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 96 loss: 0.8372488617897034, accuracy: 0.6303962469100952\n",
      "Batch 97 loss: 1.0995979309082031, accuracy: 0.629862904548645\n",
      "Batch 98 loss: 1.140870451927185, accuracy: 0.6291824579238892\n",
      "Batch 99 loss: 1.0839179754257202, accuracy: 0.6289843916893005\n",
      "Batch 100 loss: 1.0456591844558716, accuracy: 0.6288675665855408\n",
      "Batch 101 loss: 1.0505080223083496, accuracy: 0.6281403303146362\n",
      "Batch 102 loss: 0.7683185338973999, accuracy: 0.6288683414459229\n",
      "Batch 103 loss: 0.8570032119750977, accuracy: 0.6295823454856873\n",
      "Batch 104 loss: 1.0905251502990723, accuracy: 0.6290178298950195\n",
      "Batch 105 loss: 0.9410831332206726, accuracy: 0.6287588477134705\n",
      "Batch 106 loss: 1.1875097751617432, accuracy: 0.6280665993690491\n",
      "Batch 107 loss: 1.129119634628296, accuracy: 0.6270254850387573\n",
      "Batch 108 loss: 0.9150647521018982, accuracy: 0.627293586730957\n",
      "Batch 109 loss: 0.9572560787200928, accuracy: 0.6272016763687134\n",
      "Batch 110 loss: 0.9160967469215393, accuracy: 0.6273226141929626\n",
      "Batch 111 loss: 1.0096567869186401, accuracy: 0.62744140625\n",
      "Batch 112 loss: 0.961491584777832, accuracy: 0.6276963353157043\n",
      "Batch 113 loss: 0.8994027376174927, accuracy: 0.628358006477356\n",
      "Batch 114 loss: 0.9928510189056396, accuracy: 0.6285325884819031\n",
      "Batch 115 loss: 1.1107261180877686, accuracy: 0.6278960108757019\n",
      "Batch 116 loss: 0.9641522169113159, accuracy: 0.6276041865348816\n",
      "Batch 117 loss: 0.9461289644241333, accuracy: 0.6281779408454895\n",
      "Batch 118 loss: 1.0047671794891357, accuracy: 0.6280199289321899\n",
      "Batch 119 loss: 0.9652660489082336, accuracy: 0.6283203363418579\n",
      "Batch 120 loss: 1.0730504989624023, accuracy: 0.6282928586006165\n",
      "Batch 121 loss: 0.9329003691673279, accuracy: 0.6285220384597778\n",
      "Batch 122 loss: 0.9621813297271729, accuracy: 0.6290014982223511\n",
      "Batch 123 loss: 1.0136293172836304, accuracy: 0.6286542415618896\n",
      "Batch 124 loss: 1.040302038192749, accuracy: 0.6286249756813049\n",
      "Batch 125 loss: 1.078871726989746, accuracy: 0.6283482313156128\n",
      "Batch 126 loss: 1.2214298248291016, accuracy: 0.6278912425041199\n",
      "Batch 127 loss: 0.9766501784324646, accuracy: 0.62823486328125\n",
      "Batch 128 loss: 1.0077732801437378, accuracy: 0.6283308863639832\n",
      "Batch 129 loss: 1.095953345298767, accuracy: 0.6278846263885498\n",
      "Batch 130 loss: 1.0914032459259033, accuracy: 0.6275644302368164\n",
      "Batch 131 loss: 1.0506573915481567, accuracy: 0.6275449991226196\n",
      "Batch 132 loss: 0.8571038246154785, accuracy: 0.6282307505607605\n",
      "Batch 133 loss: 1.0610618591308594, accuracy: 0.6280317306518555\n",
      "Batch 134 loss: 0.9055792689323425, accuracy: 0.6284143328666687\n",
      "Batch 135 loss: 1.0168516635894775, accuracy: 0.6282743811607361\n",
      "Batch 136 loss: 1.0682647228240967, accuracy: 0.6278512477874756\n",
      "Batch 137 loss: 0.962834894657135, accuracy: 0.6279438138008118\n",
      "Batch 138 loss: 0.9270530343055725, accuracy: 0.6282036900520325\n",
      "Batch 139 loss: 0.9099351763725281, accuracy: 0.6282365918159485\n",
      "Batch 140 loss: 0.8483208417892456, accuracy: 0.6284352540969849\n",
      "Batch 141 loss: 0.9418142437934875, accuracy: 0.62890625\n",
      "Batch 142 loss: 0.9017360210418701, accuracy: 0.6292067170143127\n",
      "Batch 143 loss: 0.9983677864074707, accuracy: 0.6292860507965088\n",
      "Batch 144 loss: 0.8959203958511353, accuracy: 0.629687488079071\n",
      "Batch 145 loss: 0.9968129992485046, accuracy: 0.6296554207801819\n",
      "Batch 146 loss: 0.9752550721168518, accuracy: 0.6295174360275269\n",
      "Batch 147 loss: 0.8946883678436279, accuracy: 0.6299619674682617\n",
      "Batch 148 loss: 0.854202926158905, accuracy: 0.6302433013916016\n",
      "Batch 149 loss: 1.1247973442077637, accuracy: 0.6302083134651184\n",
      "Batch 150 loss: 0.909735918045044, accuracy: 0.6301738619804382\n",
      "Batch 151 loss: 1.0190725326538086, accuracy: 0.6300883889198303\n",
      "Batch 152 loss: 0.9772465229034424, accuracy: 0.6301062107086182\n",
      "Batch 153 loss: 0.9790450930595398, accuracy: 0.629819393157959\n",
      "Batch 154 loss: 1.0104737281799316, accuracy: 0.6296371221542358\n",
      "Batch 155 loss: 1.017111897468567, accuracy: 0.6297575831413269\n",
      "Batch 156 loss: 0.8452668786048889, accuracy: 0.6301751732826233\n",
      "Batch 157 loss: 1.0912178754806519, accuracy: 0.6299940943717957\n",
      "Batch 158 loss: 0.816076397895813, accuracy: 0.6304048895835876\n",
      "Batch 159 loss: 0.7358994483947754, accuracy: 0.630908191204071\n",
      "Batch 160 loss: 0.9658147692680359, accuracy: 0.6307259202003479\n",
      "Batch 161 loss: 1.1072567701339722, accuracy: 0.6306423544883728\n",
      "Batch 162 loss: 1.0295604467391968, accuracy: 0.6308953166007996\n",
      "Batch 163 loss: 0.9206846356391907, accuracy: 0.6313357353210449\n",
      "Batch 164 loss: 0.9321903586387634, accuracy: 0.6316288113594055\n",
      "Batch 165 loss: 0.9975351691246033, accuracy: 0.6314947009086609\n",
      "Batch 166 loss: 1.0055606365203857, accuracy: 0.63117516040802\n",
      "Batch 167 loss: 1.0530061721801758, accuracy: 0.630859375\n",
      "Batch 168 loss: 0.8920858502388, accuracy: 0.631194531917572\n",
      "Batch 169 loss: 0.8131662011146545, accuracy: 0.6316636204719543\n",
      "Batch 170 loss: 1.0122016668319702, accuracy: 0.6314418911933899\n",
      "Batch 171 loss: 0.9773293137550354, accuracy: 0.6313135623931885\n",
      "Batch 172 loss: 0.9411153197288513, accuracy: 0.6315480470657349\n",
      "Batch 173 loss: 0.9542685747146606, accuracy: 0.631465494632721\n",
      "Batch 174 loss: 0.9610379934310913, accuracy: 0.6315178275108337\n",
      "Batch 175 loss: 1.1306654214859009, accuracy: 0.6309925317764282\n",
      "Batch 176 loss: 1.1706451177597046, accuracy: 0.6303407549858093\n",
      "Batch 177 loss: 1.041202187538147, accuracy: 0.6302229762077332\n",
      "Batch 178 loss: 0.9017463326454163, accuracy: 0.6301937699317932\n",
      "Batch 179 loss: 1.0166969299316406, accuracy: 0.6301215291023254\n",
      "Batch 180 loss: 1.039196252822876, accuracy: 0.6299206018447876\n",
      "Batch 181 loss: 1.0072364807128906, accuracy: 0.6298935413360596\n",
      "Batch 182 loss: 1.0318629741668701, accuracy: 0.6298241019248962\n",
      "Batch 183 loss: 1.0254203081130981, accuracy: 0.6298403739929199\n",
      "Batch 184 loss: 0.9500299692153931, accuracy: 0.6301097869873047\n",
      "Batch 185 loss: 0.9418137073516846, accuracy: 0.6299563050270081\n",
      "Batch 186 loss: 0.964799702167511, accuracy: 0.6300551295280457\n",
      "Batch 187 loss: 0.8869639039039612, accuracy: 0.6301944851875305\n",
      "Batch 188 loss: 0.8777831196784973, accuracy: 0.6304563283920288\n",
      "Batch 189 loss: 0.9925491809844971, accuracy: 0.6305921077728271\n",
      "Batch 190 loss: 1.0108082294464111, accuracy: 0.6306037306785583\n",
      "Batch 191 loss: 0.9874429702758789, accuracy: 0.6310221552848816\n",
      "Batch 192 loss: 0.8899072408676147, accuracy: 0.631314754486084\n",
      "Batch 193 loss: 0.9678298830986023, accuracy: 0.6314030289649963\n",
      "Batch 194 loss: 0.9581501483917236, accuracy: 0.6314502954483032\n",
      "Batch 195 loss: 0.9857133030891418, accuracy: 0.6313377022743225\n",
      "Batch 196 loss: 0.8636158108711243, accuracy: 0.6316624283790588\n",
      "Batch 197 loss: 0.9535432457923889, accuracy: 0.6315893530845642\n",
      "Batch 198 loss: 0.9330053329467773, accuracy: 0.6317524909973145\n",
      "Batch 199 loss: 1.1363496780395508, accuracy: 0.6316015720367432\n",
      "Batch 200 loss: 1.0995845794677734, accuracy: 0.6314910054206848\n",
      "Batch 201 loss: 1.043438196182251, accuracy: 0.6314201951026917\n",
      "Batch 202 loss: 1.1220347881317139, accuracy: 0.6309652328491211\n",
      "Batch 203 loss: 0.992136538028717, accuracy: 0.6307061910629272\n",
      "Batch 204 loss: 1.027276873588562, accuracy: 0.6307164430618286\n",
      "Batch 205 loss: 1.0374183654785156, accuracy: 0.6306508183479309\n",
      "Batch 206 loss: 1.0317139625549316, accuracy: 0.6305857300758362\n",
      "Batch 207 loss: 1.005002737045288, accuracy: 0.6304837465286255\n",
      "Batch 208 loss: 1.038513422012329, accuracy: 0.630382776260376\n",
      "Batch 209 loss: 0.9232764840126038, accuracy: 0.6304687261581421\n",
      "Batch 210 loss: 0.8777247071266174, accuracy: 0.630776047706604\n",
      "Batch 211 loss: 1.017736554145813, accuracy: 0.6305645704269409\n",
      "Batch 212 loss: 0.9460222721099854, accuracy: 0.6304650902748108\n",
      "Batch 213 loss: 1.1551107168197632, accuracy: 0.6303300261497498\n",
      "Batch 214 loss: 0.881051778793335, accuracy: 0.6302688717842102\n",
      "Batch 215 loss: 1.0878995656967163, accuracy: 0.6299912929534912\n",
      "Batch 216 loss: 1.10512113571167, accuracy: 0.6298602819442749\n",
      "Batch 217 loss: 0.9097744226455688, accuracy: 0.6300530433654785\n",
      "Batch 218 loss: 0.9275184869766235, accuracy: 0.6301369667053223\n",
      "Batch 219 loss: 1.0011392831802368, accuracy: 0.6299005746841431\n",
      "Batch 220 loss: 0.9139024019241333, accuracy: 0.6299490928649902\n",
      "Batch 221 loss: 1.1108708381652832, accuracy: 0.6295748949050903\n",
      "Batch 222 loss: 0.9802413582801819, accuracy: 0.6296594738960266\n",
      "Batch 223 loss: 1.076790690422058, accuracy: 0.6294293999671936\n",
      "Batch 224 loss: 0.9935243725776672, accuracy: 0.6294193267822266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 15, train accuracy: 62.941932678222656, train loss: 0.9824801373481751, valid accuracy: 61.52131652832031, valid loss: 1.0194464839737991 \n",
      "Batch 0 loss: 0.9706017971038818, accuracy: 0.625\n",
      "Batch 1 loss: 0.9134933352470398, accuracy: 0.63671875\n",
      "Batch 2 loss: 1.0375559329986572, accuracy: 0.6276041865348816\n",
      "Batch 3 loss: 0.8296087384223938, accuracy: 0.650390625\n",
      "Batch 4 loss: 0.7700896859169006, accuracy: 0.6656249761581421\n",
      "Batch 5 loss: 0.9627329111099243, accuracy: 0.65625\n",
      "Batch 6 loss: 1.0408005714416504, accuracy: 0.6372767686843872\n",
      "Batch 7 loss: 0.8860538005828857, accuracy: 0.634765625\n",
      "Batch 8 loss: 1.121355652809143, accuracy: 0.625\n",
      "Batch 9 loss: 1.0647248029708862, accuracy: 0.621874988079071\n",
      "Batch 10 loss: 0.890352725982666, accuracy: 0.6257102489471436\n",
      "Batch 11 loss: 0.9071326851844788, accuracy: 0.6276041865348816\n",
      "Batch 12 loss: 1.1181925535202026, accuracy: 0.628004789352417\n",
      "Batch 13 loss: 0.8729988932609558, accuracy: 0.6300223469734192\n",
      "Batch 14 loss: 0.8940845727920532, accuracy: 0.6338541507720947\n",
      "Batch 15 loss: 0.9107046723365784, accuracy: 0.63525390625\n",
      "Batch 16 loss: 0.819953441619873, accuracy: 0.6374080777168274\n",
      "Batch 17 loss: 1.0484397411346436, accuracy: 0.6354166865348816\n",
      "Batch 18 loss: 0.8719184994697571, accuracy: 0.6369243264198303\n",
      "Batch 19 loss: 0.9326629042625427, accuracy: 0.637890636920929\n",
      "Batch 20 loss: 0.9932210445404053, accuracy: 0.6372767686843872\n",
      "Batch 21 loss: 0.9802281856536865, accuracy: 0.6360085010528564\n",
      "Batch 22 loss: 0.9683939814567566, accuracy: 0.634171187877655\n",
      "Batch 23 loss: 0.9729920029640198, accuracy: 0.6354166865348816\n",
      "Batch 24 loss: 0.9874435067176819, accuracy: 0.6346874833106995\n",
      "Batch 25 loss: 0.9723469614982605, accuracy: 0.633713960647583\n",
      "Batch 26 loss: 0.9443016052246094, accuracy: 0.6354166865348816\n",
      "Batch 27 loss: 0.9726583361625671, accuracy: 0.6353236436843872\n",
      "Batch 28 loss: 0.9335837364196777, accuracy: 0.6371228694915771\n",
      "Batch 29 loss: 0.9997765421867371, accuracy: 0.6377604007720947\n",
      "Batch 30 loss: 0.984167218208313, accuracy: 0.6358367204666138\n",
      "Batch 31 loss: 1.0153416395187378, accuracy: 0.635986328125\n",
      "Batch 32 loss: 1.0418719053268433, accuracy: 0.634706437587738\n",
      "Batch 33 loss: 1.0391528606414795, accuracy: 0.6332720518112183\n",
      "Batch 34 loss: 0.9876419305801392, accuracy: 0.6334821581840515\n",
      "Batch 35 loss: 0.9975579977035522, accuracy: 0.6317274570465088\n",
      "Batch 36 loss: 0.8134233355522156, accuracy: 0.6332347989082336\n",
      "Batch 37 loss: 1.0349653959274292, accuracy: 0.6326069235801697\n",
      "Batch 38 loss: 1.0157017707824707, accuracy: 0.6334134340286255\n",
      "Batch 39 loss: 0.9345453977584839, accuracy: 0.634570300579071\n",
      "Batch 40 loss: 0.8409757614135742, accuracy: 0.6349085569381714\n",
      "Batch 41 loss: 1.0608636140823364, accuracy: 0.6344866156578064\n",
      "Batch 42 loss: 0.9534539580345154, accuracy: 0.6349927186965942\n",
      "Batch 43 loss: 0.9587894678115845, accuracy: 0.6358309388160706\n",
      "Batch 44 loss: 0.9385868906974792, accuracy: 0.6369791626930237\n",
      "Batch 45 loss: 0.956416130065918, accuracy: 0.63620924949646\n",
      "Batch 46 loss: 0.8734576106071472, accuracy: 0.6366356611251831\n",
      "Batch 47 loss: 0.9909095764160156, accuracy: 0.6357421875\n",
      "Batch 48 loss: 1.0088468790054321, accuracy: 0.6352040767669678\n",
      "Batch 49 loss: 0.9257290363311768, accuracy: 0.6351562738418579\n",
      "Batch 50 loss: 0.9795963764190674, accuracy: 0.6351103186607361\n",
      "Batch 51 loss: 0.958540141582489, accuracy: 0.6353665590286255\n",
      "Batch 52 loss: 1.09189772605896, accuracy: 0.6345813870429993\n",
      "Batch 53 loss: 0.9710673689842224, accuracy: 0.6351273059844971\n",
      "Batch 54 loss: 1.0479984283447266, accuracy: 0.6343749761581421\n",
      "Batch 55 loss: 0.9628949761390686, accuracy: 0.6346260905265808\n",
      "Batch 56 loss: 0.9773409366607666, accuracy: 0.6354166865348816\n",
      "Batch 57 loss: 1.006845235824585, accuracy: 0.635102391242981\n",
      "Batch 58 loss: 0.9627626538276672, accuracy: 0.6354607939720154\n",
      "Batch 59 loss: 0.9846742153167725, accuracy: 0.6356770992279053\n",
      "Batch 60 loss: 1.0255206823349, accuracy: 0.6355020403862\n",
      "Batch 61 loss: 0.9893444180488586, accuracy: 0.6354586482048035\n",
      "Batch 62 loss: 0.929980456829071, accuracy: 0.6357886791229248\n",
      "Batch 63 loss: 0.9716210961341858, accuracy: 0.635986328125\n",
      "Batch 64 loss: 1.014512300491333, accuracy: 0.6355769038200378\n",
      "Batch 65 loss: 1.0162651538848877, accuracy: 0.6352983117103577\n",
      "Batch 66 loss: 0.8998754620552063, accuracy: 0.6350280046463013\n",
      "Batch 67 loss: 0.9623566269874573, accuracy: 0.6356847286224365\n",
      "Batch 68 loss: 0.8478273749351501, accuracy: 0.6360960006713867\n",
      "Batch 69 loss: 1.0065844058990479, accuracy: 0.6352678537368774\n",
      "Batch 70 loss: 0.9220253229141235, accuracy: 0.6358934640884399\n",
      "Batch 71 loss: 0.8764021992683411, accuracy: 0.6362847089767456\n",
      "Batch 72 loss: 0.9084091186523438, accuracy: 0.6363441944122314\n",
      "Batch 73 loss: 0.9693963527679443, accuracy: 0.6368243098258972\n",
      "Batch 74 loss: 0.7897108197212219, accuracy: 0.6373958587646484\n",
      "Batch 75 loss: 0.9461150765419006, accuracy: 0.6377466917037964\n",
      "Batch 76 loss: 0.9338985681533813, accuracy: 0.6373782753944397\n",
      "Batch 77 loss: 1.032347321510315, accuracy: 0.6373196840286255\n",
      "Batch 78 loss: 0.8963022232055664, accuracy: 0.6376582384109497\n",
      "Batch 79 loss: 0.8822072148323059, accuracy: 0.63818359375\n",
      "Batch 80 loss: 0.9095735549926758, accuracy: 0.6381173133850098\n",
      "Batch 81 loss: 0.811916708946228, accuracy: 0.6389100551605225\n",
      "Batch 82 loss: 0.9314026832580566, accuracy: 0.639401376247406\n",
      "Batch 83 loss: 0.9811237454414368, accuracy: 0.638671875\n",
      "Batch 84 loss: 1.003588318824768, accuracy: 0.6379595398902893\n",
      "Batch 85 loss: 0.9931253790855408, accuracy: 0.6379905343055725\n",
      "Batch 86 loss: 0.8410889506340027, accuracy: 0.6380208134651184\n",
      "Batch 87 loss: 0.9675693511962891, accuracy: 0.6378728747367859\n",
      "Batch 88 loss: 1.1211152076721191, accuracy: 0.6373770833015442\n",
      "Batch 89 loss: 0.9400342106819153, accuracy: 0.6376736164093018\n",
      "Batch 90 loss: 0.964510977268219, accuracy: 0.6378777623176575\n",
      "Batch 91 loss: 1.0612289905548096, accuracy: 0.637313187122345\n",
      "Batch 92 loss: 0.8737925887107849, accuracy: 0.6371808052062988\n",
      "Batch 93 loss: 0.8628654479980469, accuracy: 0.6378822922706604\n",
      "Batch 94 loss: 1.0121986865997314, accuracy: 0.6376644968986511\n",
      "Batch 95 loss: 0.9313002228736877, accuracy: 0.6376953125\n",
      "Batch 96 loss: 1.1445153951644897, accuracy: 0.6365174055099487\n",
      "Batch 97 loss: 0.999965250492096, accuracy: 0.6359215378761292\n",
      "Batch 98 loss: 0.9687362909317017, accuracy: 0.6366792917251587\n",
      "Batch 99 loss: 0.9546002745628357, accuracy: 0.63671875\n",
      "Batch 100 loss: 0.9395000338554382, accuracy: 0.6366800665855408\n",
      "Batch 101 loss: 1.0226356983184814, accuracy: 0.6366421580314636\n",
      "Batch 102 loss: 1.0264760255813599, accuracy: 0.6359223127365112\n",
      "Batch 103 loss: 1.0384987592697144, accuracy: 0.6355919241905212\n",
      "Batch 104 loss: 0.870077908039093, accuracy: 0.6356399059295654\n",
      "Batch 105 loss: 0.9149280786514282, accuracy: 0.6357606053352356\n",
      "Batch 106 loss: 0.8690513372421265, accuracy: 0.636390209197998\n",
      "Batch 107 loss: 1.0198590755462646, accuracy: 0.6361400485038757\n",
      "Batch 108 loss: 0.9813662767410278, accuracy: 0.6357511281967163\n",
      "Batch 109 loss: 0.9392176270484924, accuracy: 0.6357244253158569\n",
      "Batch 110 loss: 1.0433361530303955, accuracy: 0.6354870200157166\n",
      "Batch 111 loss: 1.0080788135528564, accuracy: 0.63525390625\n",
      "Batch 112 loss: 0.9536580443382263, accuracy: 0.6350940465927124\n",
      "Batch 113 loss: 1.0166970491409302, accuracy: 0.6348684430122375\n",
      "Batch 114 loss: 0.8509694337844849, accuracy: 0.635326087474823\n",
      "Batch 115 loss: 1.0834617614746094, accuracy: 0.6348329782485962\n",
      "Batch 116 loss: 0.8650953769683838, accuracy: 0.6348824501037598\n",
      "Batch 117 loss: 0.907392144203186, accuracy: 0.6347325444221497\n",
      "Batch 118 loss: 1.0953316688537598, accuracy: 0.6343881487846375\n",
      "Batch 119 loss: 1.0440913438796997, accuracy: 0.6342447996139526\n",
      "Batch 120 loss: 0.9079889059066772, accuracy: 0.6344912052154541\n",
      "Batch 121 loss: 0.9258257150650024, accuracy: 0.6344134211540222\n",
      "Batch 122 loss: 0.901654839515686, accuracy: 0.6346544623374939\n",
      "Batch 123 loss: 0.8640185594558716, accuracy: 0.6346396207809448\n",
      "Batch 124 loss: 0.9774245619773865, accuracy: 0.6345624923706055\n",
      "Batch 125 loss: 0.8847032189369202, accuracy: 0.6349206566810608\n",
      "Batch 126 loss: 0.8602765202522278, accuracy: 0.6353961825370789\n",
      "Batch 127 loss: 0.8975814580917358, accuracy: 0.63543701171875\n",
      "Batch 128 loss: 0.8227691650390625, accuracy: 0.635598361492157\n",
      "Batch 129 loss: 0.984432578086853, accuracy: 0.6353966593742371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 130 loss: 0.9271836876869202, accuracy: 0.6355558037757874\n",
      "Batch 131 loss: 0.8834810853004456, accuracy: 0.6355942487716675\n",
      "Batch 132 loss: 0.8140115737915039, accuracy: 0.6356320381164551\n",
      "Batch 133 loss: 0.8947057127952576, accuracy: 0.6354944109916687\n",
      "Batch 134 loss: 0.9212009906768799, accuracy: 0.6355902552604675\n",
      "Batch 135 loss: 1.0053424835205078, accuracy: 0.6356847286224365\n",
      "Batch 136 loss: 0.9336506128311157, accuracy: 0.6355497241020203\n",
      "Batch 137 loss: 1.0307106971740723, accuracy: 0.6353034377098083\n",
      "Batch 138 loss: 0.7971012592315674, accuracy: 0.6355665326118469\n",
      "Batch 139 loss: 0.9767816066741943, accuracy: 0.635546863079071\n",
      "Batch 140 loss: 0.999326765537262, accuracy: 0.6354720592498779\n",
      "Batch 141 loss: 0.9250695705413818, accuracy: 0.6355633735656738\n",
      "Batch 142 loss: 0.7868970632553101, accuracy: 0.6355441212654114\n",
      "Batch 143 loss: 1.0044666528701782, accuracy: 0.6356337070465088\n",
      "Batch 144 loss: 1.1935313940048218, accuracy: 0.6350754499435425\n",
      "Batch 145 loss: 0.85821932554245, accuracy: 0.6350599527359009\n",
      "Batch 146 loss: 0.7905431389808655, accuracy: 0.6354697942733765\n",
      "Batch 147 loss: 0.9221725463867188, accuracy: 0.6352934837341309\n",
      "Batch 148 loss: 0.875031590461731, accuracy: 0.635591447353363\n",
      "Batch 149 loss: 0.9171395897865295, accuracy: 0.6358333230018616\n",
      "Batch 150 loss: 0.9739347696304321, accuracy: 0.6358650922775269\n",
      "Batch 151 loss: 1.053237795829773, accuracy: 0.6352282166481018\n",
      "Batch 152 loss: 1.0507874488830566, accuracy: 0.6348039507865906\n",
      "Batch 153 loss: 0.897088348865509, accuracy: 0.6351460814476013\n",
      "Batch 154 loss: 1.0478434562683105, accuracy: 0.6347278356552124\n",
      "Batch 155 loss: 0.885880172252655, accuracy: 0.6349659562110901\n",
      "Batch 156 loss: 0.939598023891449, accuracy: 0.6349024772644043\n",
      "Batch 157 loss: 1.0024524927139282, accuracy: 0.6350376009941101\n",
      "Batch 158 loss: 1.0031185150146484, accuracy: 0.6350235939025879\n",
      "Batch 159 loss: 0.9465115666389465, accuracy: 0.634960949420929\n",
      "Batch 160 loss: 0.9275891780853271, accuracy: 0.635238766670227\n",
      "Batch 161 loss: 1.062943458557129, accuracy: 0.634789764881134\n",
      "Batch 162 loss: 0.9373018741607666, accuracy: 0.6349213719367981\n",
      "Batch 163 loss: 1.0530104637145996, accuracy: 0.6345750689506531\n",
      "Batch 164 loss: 1.0382713079452515, accuracy: 0.6348958611488342\n",
      "Batch 165 loss: 1.135340690612793, accuracy: 0.6346008777618408\n",
      "Batch 166 loss: 0.9452979564666748, accuracy: 0.6346837282180786\n",
      "Batch 167 loss: 0.964832067489624, accuracy: 0.6348121166229248\n",
      "Batch 168 loss: 0.9766401052474976, accuracy: 0.6348003149032593\n",
      "Batch 169 loss: 0.9928647875785828, accuracy: 0.6349724531173706\n",
      "Batch 170 loss: 1.0011956691741943, accuracy: 0.6345943212509155\n",
      "Batch 171 loss: 0.9708269238471985, accuracy: 0.6347202062606812\n",
      "Batch 172 loss: 0.8941820859909058, accuracy: 0.6347543597221375\n",
      "Batch 173 loss: 0.9861361384391785, accuracy: 0.6346533894538879\n",
      "Batch 174 loss: 1.1968494653701782, accuracy: 0.6342856884002686\n",
      "Batch 175 loss: 1.029807209968567, accuracy: 0.6342329382896423\n",
      "Batch 176 loss: 0.9973041415214539, accuracy: 0.6342690587043762\n",
      "Batch 177 loss: 0.9175621271133423, accuracy: 0.6342170238494873\n",
      "Batch 178 loss: 0.9868777394294739, accuracy: 0.634209156036377\n",
      "Batch 179 loss: 1.118377923965454, accuracy: 0.6338541507720947\n",
      "Batch 180 loss: 0.9479513168334961, accuracy: 0.6340210437774658\n",
      "Batch 181 loss: 1.064563274383545, accuracy: 0.6337568759918213\n",
      "Batch 182 loss: 0.8253197073936462, accuracy: 0.6339651346206665\n",
      "Batch 183 loss: 0.939815878868103, accuracy: 0.63404381275177\n",
      "Batch 184 loss: 1.1351604461669922, accuracy: 0.6336992979049683\n",
      "Batch 185 loss: 0.9437217712402344, accuracy: 0.6336945295333862\n",
      "Batch 186 loss: 0.9100534319877625, accuracy: 0.6337316036224365\n",
      "Batch 187 loss: 0.8854292035102844, accuracy: 0.634142279624939\n",
      "Batch 188 loss: 1.0404912233352661, accuracy: 0.6338045597076416\n",
      "Batch 189 loss: 1.0777878761291504, accuracy: 0.6337993144989014\n",
      "Batch 190 loss: 0.9219059348106384, accuracy: 0.6336714625358582\n",
      "Batch 191 loss: 0.8824276328086853, accuracy: 0.6335856318473816\n",
      "Batch 192 loss: 0.8404534459114075, accuracy: 0.6337435245513916\n",
      "Batch 193 loss: 0.8161875605583191, accuracy: 0.634101152420044\n",
      "Batch 194 loss: 0.8282508850097656, accuracy: 0.634495198726654\n",
      "Batch 195 loss: 1.013900637626648, accuracy: 0.6342872977256775\n",
      "Batch 196 loss: 0.9242275953292847, accuracy: 0.6343194842338562\n",
      "Batch 197 loss: 0.88658607006073, accuracy: 0.6346275210380554\n",
      "Batch 198 loss: 1.032649040222168, accuracy: 0.6345791220664978\n",
      "Batch 199 loss: 1.0848979949951172, accuracy: 0.6344531178474426\n",
      "Batch 200 loss: 1.1345363855361938, accuracy: 0.6339396834373474\n",
      "Batch 201 loss: 0.8292118906974792, accuracy: 0.6340888142585754\n",
      "Batch 202 loss: 0.9071840047836304, accuracy: 0.6342364549636841\n",
      "Batch 203 loss: 1.0698026418685913, accuracy: 0.6341145634651184\n",
      "Batch 204 loss: 1.1218043565750122, accuracy: 0.6336508989334106\n",
      "Batch 205 loss: 1.0456242561340332, accuracy: 0.6336468458175659\n",
      "Batch 206 loss: 0.8891627788543701, accuracy: 0.6337183117866516\n",
      "Batch 207 loss: 0.8837208151817322, accuracy: 0.6339768767356873\n",
      "Batch 208 loss: 0.9558057188987732, accuracy: 0.6339339017868042\n",
      "Batch 209 loss: 0.9367808699607849, accuracy: 0.6340029835700989\n",
      "Batch 210 loss: 0.9010186195373535, accuracy: 0.6342565417289734\n",
      "Batch 211 loss: 0.9083488583564758, accuracy: 0.6344708204269409\n",
      "Batch 212 loss: 0.9900757074356079, accuracy: 0.6343163251876831\n",
      "Batch 213 loss: 0.921884298324585, accuracy: 0.6343092918395996\n",
      "Batch 214 loss: 0.8586667776107788, accuracy: 0.6344476938247681\n",
      "Batch 215 loss: 0.8500388860702515, accuracy: 0.6346571445465088\n",
      "Batch 216 loss: 0.8553996086120605, accuracy: 0.6348286271095276\n",
      "Batch 217 loss: 1.0251765251159668, accuracy: 0.6347835659980774\n",
      "Batch 218 loss: 0.9265226721763611, accuracy: 0.6347388625144958\n",
      "Batch 219 loss: 1.017167329788208, accuracy: 0.6344815492630005\n",
      "Batch 220 loss: 0.9085251092910767, accuracy: 0.634650707244873\n",
      "Batch 221 loss: 0.9622933864593506, accuracy: 0.634642481803894\n",
      "Batch 222 loss: 0.9397525787353516, accuracy: 0.6345641613006592\n",
      "Batch 223 loss: 0.9055635929107666, accuracy: 0.6346260905265808\n",
      "Batch 224 loss: 1.0904898643493652, accuracy: 0.6345745325088501\n",
      "Training epoch: 16, train accuracy: 63.45745086669922, train loss: 0.9588548556963603, valid accuracy: 61.9949836730957, valid loss: 1.0671699149855252 \n",
      "Batch 0 loss: 1.0288722515106201, accuracy: 0.6484375\n",
      "Batch 1 loss: 0.7708993554115295, accuracy: 0.68359375\n",
      "Batch 2 loss: 0.922746479511261, accuracy: 0.6692708134651184\n",
      "Batch 3 loss: 0.8990128636360168, accuracy: 0.662109375\n",
      "Batch 4 loss: 1.0313282012939453, accuracy: 0.660937488079071\n",
      "Batch 5 loss: 1.0761737823486328, accuracy: 0.65625\n",
      "Batch 6 loss: 0.9724733829498291, accuracy: 0.6540178656578064\n",
      "Batch 7 loss: 1.0946499109268188, accuracy: 0.64453125\n",
      "Batch 8 loss: 0.9381938576698303, accuracy: 0.6510416865348816\n",
      "Batch 9 loss: 0.8796120882034302, accuracy: 0.659375011920929\n",
      "Batch 10 loss: 1.0724821090698242, accuracy: 0.6598011255264282\n",
      "Batch 11 loss: 0.8885027766227722, accuracy: 0.66015625\n",
      "Batch 12 loss: 0.9423966407775879, accuracy: 0.6604567170143127\n",
      "Batch 13 loss: 0.9855844974517822, accuracy: 0.6590401530265808\n",
      "Batch 14 loss: 0.742011308670044, accuracy: 0.6630208492279053\n",
      "Batch 15 loss: 1.0640498399734497, accuracy: 0.65576171875\n",
      "Batch 16 loss: 0.9126942157745361, accuracy: 0.654411792755127\n",
      "Batch 17 loss: 0.8591305017471313, accuracy: 0.6540798544883728\n",
      "Batch 18 loss: 0.8948473930358887, accuracy: 0.6566612124443054\n",
      "Batch 19 loss: 0.9503587484359741, accuracy: 0.6546875238418579\n",
      "Batch 20 loss: 1.12833571434021, accuracy: 0.6502976417541504\n",
      "Batch 21 loss: 0.9623069763183594, accuracy: 0.6484375\n",
      "Batch 22 loss: 0.9774664640426636, accuracy: 0.6457201242446899\n",
      "Batch 23 loss: 0.8168414831161499, accuracy: 0.6484375\n",
      "Batch 24 loss: 1.0534930229187012, accuracy: 0.6478124856948853\n",
      "Batch 25 loss: 0.9291173815727234, accuracy: 0.6487379670143127\n",
      "Batch 26 loss: 1.0335547924041748, accuracy: 0.6475694179534912\n",
      "Batch 27 loss: 0.8005243539810181, accuracy: 0.6498326063156128\n",
      "Batch 28 loss: 1.0878931283950806, accuracy: 0.647090494632721\n",
      "Batch 29 loss: 0.9398666620254517, accuracy: 0.6481770873069763\n",
      "Batch 30 loss: 0.8886794447898865, accuracy: 0.6481854915618896\n",
      "Batch 31 loss: 0.9894585609436035, accuracy: 0.64697265625\n",
      "Batch 32 loss: 0.8280936479568481, accuracy: 0.6479640007019043\n",
      "Batch 33 loss: 1.1292446851730347, accuracy: 0.6461396813392639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34 loss: 1.0154039859771729, accuracy: 0.6462053656578064\n",
      "Batch 35 loss: 0.9760640263557434, accuracy: 0.6456162929534912\n",
      "Batch 36 loss: 0.8863592147827148, accuracy: 0.6461148858070374\n",
      "Batch 37 loss: 1.0645148754119873, accuracy: 0.6449424624443054\n",
      "Batch 38 loss: 0.7772982716560364, accuracy: 0.6460336446762085\n",
      "Batch 39 loss: 0.8581132888793945, accuracy: 0.6451171636581421\n",
      "Batch 40 loss: 0.9693915843963623, accuracy: 0.6444360017776489\n",
      "Batch 41 loss: 1.0479660034179688, accuracy: 0.6437872052192688\n",
      "Batch 42 loss: 0.9313154220581055, accuracy: 0.6440770626068115\n",
      "Batch 43 loss: 0.9138805270195007, accuracy: 0.6438210010528564\n",
      "Batch 44 loss: 1.0278950929641724, accuracy: 0.6439236402511597\n",
      "Batch 45 loss: 0.846262514591217, accuracy: 0.6443614363670349\n",
      "Batch 46 loss: 0.7538609504699707, accuracy: 0.6459441781044006\n",
      "Batch 47 loss: 0.9674408435821533, accuracy: 0.6456705927848816\n",
      "Batch 48 loss: 0.8508143424987793, accuracy: 0.6465242505073547\n",
      "Batch 49 loss: 0.9584339261054993, accuracy: 0.6465625166893005\n",
      "Batch 50 loss: 0.9479184746742249, accuracy: 0.6467524766921997\n",
      "Batch 51 loss: 0.9036256074905396, accuracy: 0.6472355723381042\n",
      "Batch 52 loss: 0.8592819571495056, accuracy: 0.6474056839942932\n",
      "Batch 53 loss: 0.8811752200126648, accuracy: 0.6477141380310059\n",
      "Batch 54 loss: 0.8584926724433899, accuracy: 0.6475852131843567\n",
      "Batch 55 loss: 1.02662992477417, accuracy: 0.6474609375\n",
      "Batch 56 loss: 1.0728579759597778, accuracy: 0.6473410129547119\n",
      "Batch 57 loss: 0.9453325867652893, accuracy: 0.646821141242981\n",
      "Batch 58 loss: 0.8826305270195007, accuracy: 0.6472457647323608\n",
      "Batch 59 loss: 0.8973983526229858, accuracy: 0.6477864384651184\n",
      "Batch 60 loss: 0.8106846809387207, accuracy: 0.6485655903816223\n",
      "Batch 61 loss: 0.7942493557929993, accuracy: 0.6498236060142517\n",
      "Batch 62 loss: 0.9523853659629822, accuracy: 0.6504216194152832\n",
      "Batch 63 loss: 0.835940420627594, accuracy: 0.6505126953125\n",
      "Batch 64 loss: 1.0296963453292847, accuracy: 0.650120198726654\n",
      "Batch 65 loss: 1.0619477033615112, accuracy: 0.6497395634651184\n",
      "Batch 66 loss: 0.9842604398727417, accuracy: 0.6491371393203735\n",
      "Batch 67 loss: 0.9482689499855042, accuracy: 0.6487821936607361\n",
      "Batch 68 loss: 0.9707266688346863, accuracy: 0.6483242511749268\n",
      "Batch 69 loss: 0.954075038433075, accuracy: 0.6478794813156128\n",
      "Batch 70 loss: 1.0342072248458862, accuracy: 0.6472271084785461\n",
      "Batch 71 loss: 1.0583816766738892, accuracy: 0.6471354365348816\n",
      "Batch 72 loss: 0.9350387454032898, accuracy: 0.6474742889404297\n",
      "Batch 73 loss: 0.8480268120765686, accuracy: 0.6481207609176636\n",
      "Batch 74 loss: 0.8644908666610718, accuracy: 0.6485416889190674\n",
      "Batch 75 loss: 1.0625944137573242, accuracy: 0.6479235291481018\n",
      "Batch 76 loss: 0.9631909132003784, accuracy: 0.6479302048683167\n",
      "Batch 77 loss: 1.024903655052185, accuracy: 0.6469351053237915\n",
      "Batch 78 loss: 0.8750441074371338, accuracy: 0.647844135761261\n",
      "Batch 79 loss: 1.0814592838287354, accuracy: 0.646679699420929\n",
      "Batch 80 loss: 1.036546230316162, accuracy: 0.6467013955116272\n",
      "Batch 81 loss: 0.7925775647163391, accuracy: 0.6472942233085632\n",
      "Batch 82 loss: 0.9606507420539856, accuracy: 0.6468373537063599\n",
      "Batch 83 loss: 1.0655951499938965, accuracy: 0.6463913917541504\n",
      "Batch 84 loss: 0.9943398237228394, accuracy: 0.6459558606147766\n",
      "Batch 85 loss: 0.9456993341445923, accuracy: 0.6458030343055725\n",
      "Batch 86 loss: 0.9919995665550232, accuracy: 0.6455639600753784\n",
      "Batch 87 loss: 0.9581284523010254, accuracy: 0.6462180614471436\n",
      "Batch 88 loss: 0.841262698173523, accuracy: 0.6465941071510315\n",
      "Batch 89 loss: 0.9528087377548218, accuracy: 0.6467881798744202\n",
      "Batch 90 loss: 0.9392562508583069, accuracy: 0.6467204689979553\n",
      "Batch 91 loss: 0.9284310936927795, accuracy: 0.64673912525177\n",
      "Batch 92 loss: 0.8750007152557373, accuracy: 0.6477654576301575\n",
      "Batch 93 loss: 0.8551760315895081, accuracy: 0.6481050252914429\n",
      "Batch 94 loss: 0.9516655802726746, accuracy: 0.6478618383407593\n",
      "Batch 95 loss: 0.9995959997177124, accuracy: 0.6474609375\n",
      "Batch 96 loss: 1.0900598764419556, accuracy: 0.647148847579956\n",
      "Batch 97 loss: 1.0567777156829834, accuracy: 0.6469228267669678\n",
      "Batch 98 loss: 1.0600703954696655, accuracy: 0.6462278962135315\n",
      "Batch 99 loss: 0.878645658493042, accuracy: 0.6464062333106995\n",
      "Batch 100 loss: 0.8576153516769409, accuracy: 0.6468130946159363\n",
      "Batch 101 loss: 1.0092827081680298, accuracy: 0.6465227007865906\n",
      "Batch 102 loss: 1.0220094919204712, accuracy: 0.6461620330810547\n",
      "Batch 103 loss: 1.0160609483718872, accuracy: 0.6458834409713745\n",
      "Batch 104 loss: 1.0906310081481934, accuracy: 0.6455357074737549\n",
      "Batch 105 loss: 0.9727725386619568, accuracy: 0.6457841992378235\n",
      "Batch 106 loss: 0.8600437641143799, accuracy: 0.64580899477005\n",
      "Batch 107 loss: 0.9275563359260559, accuracy: 0.6458333134651184\n",
      "Batch 108 loss: 1.0283063650131226, accuracy: 0.6457138657569885\n",
      "Batch 109 loss: 0.8626086711883545, accuracy: 0.6458806991577148\n",
      "Batch 110 loss: 0.9179354906082153, accuracy: 0.646255612373352\n",
      "Batch 111 loss: 0.913272500038147, accuracy: 0.6461356282234192\n",
      "Batch 112 loss: 0.9859773516654968, accuracy: 0.646225094795227\n",
      "Batch 113 loss: 0.826901376247406, accuracy: 0.6466556787490845\n",
      "Batch 114 loss: 0.9085046648979187, accuracy: 0.6469429135322571\n",
      "Batch 115 loss: 0.945087730884552, accuracy: 0.6468884944915771\n",
      "Batch 116 loss: 1.002148151397705, accuracy: 0.6469684839248657\n",
      "Batch 117 loss: 1.1898906230926514, accuracy: 0.6460540294647217\n",
      "Batch 118 loss: 0.8872923254966736, accuracy: 0.6462709903717041\n",
      "Batch 119 loss: 0.9637202024459839, accuracy: 0.6457682251930237\n",
      "Batch 120 loss: 1.0232040882110596, accuracy: 0.6455320119857788\n",
      "Batch 121 loss: 0.7571742534637451, accuracy: 0.6460681557655334\n",
      "Batch 122 loss: 0.9451838731765747, accuracy: 0.6463414430618286\n",
      "Batch 123 loss: 0.9199436902999878, accuracy: 0.6463583707809448\n",
      "Batch 124 loss: 0.9498085379600525, accuracy: 0.6464999914169312\n",
      "Batch 125 loss: 0.876265287399292, accuracy: 0.6463293433189392\n",
      "Batch 126 loss: 0.8391790390014648, accuracy: 0.6466535329818726\n",
      "Batch 127 loss: 1.0432236194610596, accuracy: 0.64599609375\n",
      "Batch 128 loss: 1.1859580278396606, accuracy: 0.6455305218696594\n",
      "Batch 129 loss: 0.9398717284202576, accuracy: 0.6451923251152039\n",
      "Batch 130 loss: 0.9706900119781494, accuracy: 0.6452767252922058\n",
      "Batch 131 loss: 1.0225541591644287, accuracy: 0.6450047492980957\n",
      "Batch 132 loss: 0.9405632615089417, accuracy: 0.6454417109489441\n",
      "Batch 133 loss: 1.1733675003051758, accuracy: 0.64453125\n",
      "Batch 134 loss: 0.8452829122543335, accuracy: 0.6449074149131775\n",
      "Batch 135 loss: 0.9720163941383362, accuracy: 0.6448184847831726\n",
      "Batch 136 loss: 1.0786908864974976, accuracy: 0.6444457173347473\n",
      "Batch 137 loss: 0.9314141869544983, accuracy: 0.6447576880455017\n",
      "Batch 138 loss: 0.9197630882263184, accuracy: 0.6443345546722412\n",
      "Batch 139 loss: 0.9107695817947388, accuracy: 0.6450334787368774\n",
      "Batch 140 loss: 0.9937564134597778, accuracy: 0.644669771194458\n",
      "Batch 141 loss: 0.9840952157974243, accuracy: 0.6446412801742554\n",
      "Batch 142 loss: 0.9584723114967346, accuracy: 0.6448317170143127\n",
      "Batch 143 loss: 0.9133559465408325, accuracy: 0.6449652910232544\n",
      "Batch 144 loss: 0.9848494529724121, accuracy: 0.6446120738983154\n",
      "Batch 145 loss: 0.9858748912811279, accuracy: 0.6445847749710083\n",
      "Batch 146 loss: 0.9974924921989441, accuracy: 0.6448235511779785\n",
      "Batch 147 loss: 1.0071675777435303, accuracy: 0.6444256901741028\n",
      "Batch 148 loss: 1.065351963043213, accuracy: 0.6442428827285767\n",
      "Batch 149 loss: 1.052857756614685, accuracy: 0.6440625190734863\n",
      "Batch 150 loss: 1.0365179777145386, accuracy: 0.6437293291091919\n",
      "Batch 151 loss: 0.913837730884552, accuracy: 0.6438117027282715\n",
      "Batch 152 loss: 0.8684557676315308, accuracy: 0.6441993713378906\n",
      "Batch 153 loss: 1.0162153244018555, accuracy: 0.6441761255264282\n",
      "Batch 154 loss: 1.0373783111572266, accuracy: 0.6439515948295593\n",
      "Batch 155 loss: 0.8739382028579712, accuracy: 0.6441305875778198\n",
      "Batch 156 loss: 0.8975779414176941, accuracy: 0.643909215927124\n",
      "Batch 157 loss: 1.0521705150604248, accuracy: 0.6436412334442139\n",
      "Batch 158 loss: 1.0604280233383179, accuracy: 0.643425703048706\n",
      "Batch 159 loss: 0.953515887260437, accuracy: 0.643261730670929\n",
      "Batch 160 loss: 0.8638060092926025, accuracy: 0.6433423757553101\n",
      "Batch 161 loss: 0.9117727875709534, accuracy: 0.6435185074806213\n",
      "Batch 162 loss: 0.8518368601799011, accuracy: 0.6439321041107178\n",
      "Batch 163 loss: 1.03860342502594, accuracy: 0.6435308456420898\n",
      "Batch 164 loss: 0.829348087310791, accuracy: 0.6436079740524292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 165 loss: 0.9699262976646423, accuracy: 0.6438723802566528\n",
      "Batch 166 loss: 0.9482815265655518, accuracy: 0.644040048122406\n",
      "Batch 167 loss: 0.9968807101249695, accuracy: 0.6438336968421936\n",
      "Batch 168 loss: 0.8973333835601807, accuracy: 0.6439071893692017\n",
      "Batch 169 loss: 0.8894787430763245, accuracy: 0.6438419222831726\n",
      "Batch 170 loss: 0.9721795916557312, accuracy: 0.6436403393745422\n",
      "Batch 171 loss: 0.9310160875320435, accuracy: 0.6437590718269348\n",
      "Batch 172 loss: 0.904623806476593, accuracy: 0.6437861323356628\n",
      "Batch 173 loss: 0.9828433990478516, accuracy: 0.6437230706214905\n",
      "Batch 174 loss: 0.8268135786056519, accuracy: 0.6438839435577393\n",
      "Batch 175 loss: 0.9385830163955688, accuracy: 0.64404296875\n",
      "Batch 176 loss: 0.9919645190238953, accuracy: 0.6438912153244019\n",
      "Batch 177 loss: 0.9204838275909424, accuracy: 0.6440484523773193\n",
      "Batch 178 loss: 1.051252007484436, accuracy: 0.6437237858772278\n",
      "Batch 179 loss: 0.9732718467712402, accuracy: 0.6436197757720947\n",
      "Batch 180 loss: 0.9228346943855286, accuracy: 0.6436464190483093\n",
      "Batch 181 loss: 0.983851432800293, accuracy: 0.643629789352417\n",
      "Batch 182 loss: 0.9261688590049744, accuracy: 0.6438695192337036\n",
      "Batch 183 loss: 0.9625672101974487, accuracy: 0.643427312374115\n",
      "Batch 184 loss: 0.9363330602645874, accuracy: 0.6431587934494019\n",
      "Batch 185 loss: 0.9483818411827087, accuracy: 0.6432291865348816\n",
      "Batch 186 loss: 0.9193077683448792, accuracy: 0.6432988047599792\n",
      "Batch 187 loss: 0.8153972029685974, accuracy: 0.6436585783958435\n",
      "Batch 188 loss: 0.920341432094574, accuracy: 0.6436838507652283\n",
      "Batch 189 loss: 1.0020389556884766, accuracy: 0.6435033082962036\n",
      "Batch 190 loss: 0.9989972710609436, accuracy: 0.6428746581077576\n",
      "Batch 191 loss: 0.825648307800293, accuracy: 0.6432698369026184\n",
      "Batch 192 loss: 0.9927513599395752, accuracy: 0.6431347131729126\n",
      "Batch 193 loss: 1.0350940227508545, accuracy: 0.6428399085998535\n",
      "Batch 194 loss: 1.005347728729248, accuracy: 0.6427884697914124\n",
      "Batch 195 loss: 1.0884963274002075, accuracy: 0.6424984335899353\n",
      "Batch 196 loss: 1.0626713037490845, accuracy: 0.6423302888870239\n",
      "Batch 197 loss: 1.0159251689910889, accuracy: 0.6422427296638489\n",
      "Batch 198 loss: 0.8765254616737366, accuracy: 0.642548680305481\n",
      "Batch 199 loss: 0.9593196511268616, accuracy: 0.6424609422683716\n",
      "Batch 200 loss: 0.9616092443466187, accuracy: 0.642451822757721\n",
      "Batch 201 loss: 0.8943837881088257, accuracy: 0.6425974369049072\n",
      "Batch 202 loss: 0.8355519771575928, accuracy: 0.6429340839385986\n",
      "Batch 203 loss: 0.9737375974655151, accuracy: 0.6430376768112183\n",
      "Batch 204 loss: 1.0489553213119507, accuracy: 0.6428734660148621\n",
      "Batch 205 loss: 0.8533158302307129, accuracy: 0.6430521607398987\n",
      "Batch 206 loss: 0.8628992438316345, accuracy: 0.6431536674499512\n",
      "Batch 207 loss: 1.050512433052063, accuracy: 0.642991304397583\n",
      "Batch 208 loss: 0.8585255742073059, accuracy: 0.6432042717933655\n",
      "Batch 209 loss: 0.7656625509262085, accuracy: 0.6436011791229248\n",
      "Batch 210 loss: 0.9882526397705078, accuracy: 0.6434760093688965\n",
      "Batch 211 loss: 0.9212340712547302, accuracy: 0.6432045698165894\n",
      "Batch 212 loss: 0.9057815074920654, accuracy: 0.643339216709137\n",
      "Batch 213 loss: 0.9938147068023682, accuracy: 0.6432899832725525\n",
      "Batch 214 loss: 0.9555832147598267, accuracy: 0.6431686282157898\n",
      "Batch 215 loss: 0.9515313506126404, accuracy: 0.6432291865348816\n",
      "Batch 216 loss: 1.0935040712356567, accuracy: 0.6428931355476379\n",
      "Batch 217 loss: 0.9454776048660278, accuracy: 0.6428110599517822\n",
      "Batch 218 loss: 0.8877512216567993, accuracy: 0.6427297592163086\n",
      "Batch 219 loss: 0.9819828271865845, accuracy: 0.6426491737365723\n",
      "Batch 220 loss: 0.9039320945739746, accuracy: 0.6427106857299805\n",
      "Batch 221 loss: 0.8460849523544312, accuracy: 0.6426661014556885\n",
      "Batch 222 loss: 0.7883368730545044, accuracy: 0.6428321003913879\n",
      "Batch 223 loss: 0.9906159043312073, accuracy: 0.6427176594734192\n",
      "Batch 224 loss: 0.8770041465759277, accuracy: 0.6427600979804993\n",
      "Training epoch: 17, train accuracy: 64.27600860595703, train loss: 0.9510458032290141, valid accuracy: 63.02591323852539, valid loss: 1.0157266341406723 \n",
      "Batch 0 loss: 0.8997636437416077, accuracy: 0.6953125\n",
      "Batch 1 loss: 0.9800072312355042, accuracy: 0.66796875\n",
      "Batch 2 loss: 0.8807533979415894, accuracy: 0.65625\n",
      "Batch 3 loss: 1.01571786403656, accuracy: 0.626953125\n",
      "Batch 4 loss: 1.0250258445739746, accuracy: 0.6234375238418579\n",
      "Batch 5 loss: 0.9022217988967896, accuracy: 0.6302083134651184\n",
      "Batch 6 loss: 0.8737658262252808, accuracy: 0.6350446343421936\n",
      "Batch 7 loss: 0.9710648655891418, accuracy: 0.6298828125\n",
      "Batch 8 loss: 1.0365439653396606, accuracy: 0.6310763955116272\n",
      "Batch 9 loss: 0.9208695292472839, accuracy: 0.6312500238418579\n",
      "Batch 10 loss: 0.9124860167503357, accuracy: 0.6349431872367859\n",
      "Batch 11 loss: 0.8706824779510498, accuracy: 0.6393229365348816\n",
      "Batch 12 loss: 0.8615149855613708, accuracy: 0.6400240659713745\n",
      "Batch 13 loss: 0.8637990951538086, accuracy: 0.6383928656578064\n",
      "Batch 14 loss: 1.1420562267303467, accuracy: 0.6354166865348816\n",
      "Batch 15 loss: 0.9939713478088379, accuracy: 0.63134765625\n",
      "Batch 16 loss: 0.8272125124931335, accuracy: 0.6332720518112183\n",
      "Batch 17 loss: 1.0573838949203491, accuracy: 0.6306423544883728\n",
      "Batch 18 loss: 1.0092873573303223, accuracy: 0.6307565569877625\n",
      "Batch 19 loss: 0.8801933526992798, accuracy: 0.632031261920929\n",
      "Batch 20 loss: 0.901512861251831, accuracy: 0.6335565447807312\n",
      "Batch 21 loss: 0.8215951919555664, accuracy: 0.6356534361839294\n",
      "Batch 22 loss: 0.7605056166648865, accuracy: 0.6382473111152649\n",
      "Batch 23 loss: 0.9538092613220215, accuracy: 0.6373698115348816\n",
      "Batch 24 loss: 0.8520601391792297, accuracy: 0.640625\n",
      "Batch 25 loss: 0.8844441175460815, accuracy: 0.6427283883094788\n",
      "Batch 26 loss: 1.0234612226486206, accuracy: 0.6414930820465088\n",
      "Batch 27 loss: 0.8483856320381165, accuracy: 0.6431361436843872\n",
      "Batch 28 loss: 1.1121399402618408, accuracy: 0.6403555870056152\n",
      "Batch 29 loss: 1.0418211221694946, accuracy: 0.639843761920929\n",
      "Batch 30 loss: 0.9764326810836792, accuracy: 0.6403729915618896\n",
      "Batch 31 loss: 1.0143389701843262, accuracy: 0.640869140625\n",
      "Batch 32 loss: 0.9163559675216675, accuracy: 0.6422821879386902\n",
      "Batch 33 loss: 1.0331777334213257, accuracy: 0.642463207244873\n",
      "Batch 34 loss: 0.9755657315254211, accuracy: 0.6435267925262451\n",
      "Batch 35 loss: 0.9711704850196838, accuracy: 0.6430121660232544\n",
      "Batch 36 loss: 0.7674939632415771, accuracy: 0.6459037065505981\n",
      "Batch 37 loss: 0.9285413026809692, accuracy: 0.6461759805679321\n",
      "Batch 38 loss: 1.0903548002243042, accuracy: 0.6460336446762085\n",
      "Batch 39 loss: 0.8586313724517822, accuracy: 0.647265613079071\n",
      "Batch 40 loss: 0.9550793766975403, accuracy: 0.6465319991111755\n",
      "Batch 41 loss: 0.8442778587341309, accuracy: 0.6467633843421936\n",
      "Batch 42 loss: 1.1615116596221924, accuracy: 0.6431686282157898\n",
      "Batch 43 loss: 1.0411124229431152, accuracy: 0.6432883739471436\n",
      "Batch 44 loss: 0.7763147354125977, accuracy: 0.643750011920929\n",
      "Batch 45 loss: 0.9307423830032349, accuracy: 0.6443614363670349\n",
      "Batch 46 loss: 0.9555254578590393, accuracy: 0.6451130509376526\n",
      "Batch 47 loss: 0.9343065023422241, accuracy: 0.64453125\n",
      "Batch 48 loss: 1.0391643047332764, accuracy: 0.6436543464660645\n",
      "Batch 49 loss: 0.9621166586875916, accuracy: 0.6431249976158142\n",
      "Batch 50 loss: 0.9599788784980774, accuracy: 0.6429228186607361\n",
      "Batch 51 loss: 1.0492137670516968, accuracy: 0.6431790590286255\n",
      "Batch 52 loss: 0.8449766039848328, accuracy: 0.6438679099082947\n",
      "Batch 53 loss: 0.9467237591743469, accuracy: 0.6433738470077515\n",
      "Batch 54 loss: 1.0307793617248535, accuracy: 0.6419034004211426\n",
      "Batch 55 loss: 1.0528850555419922, accuracy: 0.6403459906578064\n",
      "Batch 56 loss: 0.9978474378585815, accuracy: 0.6395285129547119\n",
      "Batch 57 loss: 0.9196324944496155, accuracy: 0.6400862336158752\n",
      "Batch 58 loss: 0.9308717250823975, accuracy: 0.6403601765632629\n",
      "Batch 59 loss: 0.9995785355567932, accuracy: 0.639843761920929\n",
      "Batch 60 loss: 0.7901701927185059, accuracy: 0.640625\n",
      "Batch 61 loss: 0.9705464839935303, accuracy: 0.6402469873428345\n",
      "Batch 62 loss: 1.0013422966003418, accuracy: 0.6391369104385376\n",
      "Batch 63 loss: 1.096722960472107, accuracy: 0.6385498046875\n",
      "Batch 64 loss: 0.8197590708732605, accuracy: 0.6397836804389954\n",
      "Batch 65 loss: 0.8990613222122192, accuracy: 0.6400331258773804\n",
      "Batch 66 loss: 1.0543419122695923, accuracy: 0.6393423676490784\n",
      "Batch 67 loss: 0.9987231492996216, accuracy: 0.6390165686607361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 68 loss: 0.8589376211166382, accuracy: 0.639266312122345\n",
      "Batch 69 loss: 0.9834115505218506, accuracy: 0.6386160850524902\n",
      "Batch 70 loss: 0.95392906665802, accuracy: 0.6383142471313477\n",
      "Batch 71 loss: 0.9800133109092712, accuracy: 0.638671875\n",
      "Batch 72 loss: 0.9954720735549927, accuracy: 0.6385915875434875\n",
      "Batch 73 loss: 0.9522199034690857, accuracy: 0.6388302445411682\n",
      "Batch 74 loss: 0.9417158961296082, accuracy: 0.6392708420753479\n",
      "Batch 75 loss: 0.924034595489502, accuracy: 0.6393914222717285\n",
      "Batch 76 loss: 1.0184946060180664, accuracy: 0.6386972665786743\n",
      "Batch 77 loss: 0.8430355787277222, accuracy: 0.6390224099159241\n",
      "Batch 78 loss: 0.9879899621009827, accuracy: 0.6392405033111572\n",
      "Batch 79 loss: 0.9469978213310242, accuracy: 0.638964831829071\n",
      "Batch 80 loss: 0.7616438865661621, accuracy: 0.6399498581886292\n",
      "Batch 81 loss: 0.837009608745575, accuracy: 0.6404344439506531\n",
      "Batch 82 loss: 0.8641042113304138, accuracy: 0.6404367685317993\n",
      "Batch 83 loss: 0.8582466244697571, accuracy: 0.640625\n",
      "Batch 84 loss: 0.9692341089248657, accuracy: 0.6402573585510254\n",
      "Batch 85 loss: 1.1187586784362793, accuracy: 0.639807403087616\n",
      "Batch 86 loss: 0.9664264917373657, accuracy: 0.639726996421814\n",
      "Batch 87 loss: 0.9120485782623291, accuracy: 0.6400035619735718\n",
      "Batch 88 loss: 0.9049474596977234, accuracy: 0.6403616666793823\n",
      "Batch 89 loss: 0.7966681718826294, accuracy: 0.6413194537162781\n",
      "Batch 90 loss: 0.9422740340232849, accuracy: 0.6414835453033447\n",
      "Batch 91 loss: 0.8598877191543579, accuracy: 0.6421535611152649\n",
      "Batch 92 loss: 0.8820824027061462, accuracy: 0.6418851017951965\n",
      "Batch 93 loss: 0.9599645733833313, accuracy: 0.6421210169792175\n",
      "Batch 94 loss: 1.0272886753082275, accuracy: 0.6417763233184814\n",
      "Batch 95 loss: 1.0517195463180542, accuracy: 0.6414387822151184\n",
      "Batch 96 loss: 0.9604349136352539, accuracy: 0.6411888003349304\n",
      "Batch 97 loss: 0.9391838908195496, accuracy: 0.6412627696990967\n",
      "Batch 98 loss: 0.8913227915763855, accuracy: 0.6411774158477783\n",
      "Batch 99 loss: 1.0356504917144775, accuracy: 0.6404687762260437\n",
      "Batch 100 loss: 0.8980445861816406, accuracy: 0.6408570408821106\n",
      "Batch 101 loss: 0.9108994603157043, accuracy: 0.6407781839370728\n",
      "Batch 102 loss: 0.8290624618530273, accuracy: 0.6413076519966125\n",
      "Batch 103 loss: 0.8179945945739746, accuracy: 0.6419020295143127\n",
      "Batch 104 loss: 1.0231654644012451, accuracy: 0.6415178775787354\n",
      "Batch 105 loss: 0.7522983551025391, accuracy: 0.6420990824699402\n",
      "Batch 106 loss: 1.000184416770935, accuracy: 0.6423043012619019\n",
      "Batch 107 loss: 0.9999306201934814, accuracy: 0.6425057649612427\n",
      "Batch 108 loss: 0.8920136094093323, accuracy: 0.6424885392189026\n",
      "Batch 109 loss: 0.8462734222412109, accuracy: 0.6428266763687134\n",
      "Batch 110 loss: 0.7606809139251709, accuracy: 0.6432291865348816\n",
      "Batch 111 loss: 0.7922685146331787, accuracy: 0.6435546875\n",
      "Batch 112 loss: 0.9613744020462036, accuracy: 0.6437361836433411\n",
      "Batch 113 loss: 1.0235811471939087, accuracy: 0.6434347629547119\n",
      "Batch 114 loss: 0.9987556338310242, accuracy: 0.6430026888847351\n",
      "Batch 115 loss: 0.9995256662368774, accuracy: 0.6427801847457886\n",
      "Batch 116 loss: 0.9186322093009949, accuracy: 0.6428285241127014\n",
      "Batch 117 loss: 0.9272668361663818, accuracy: 0.6428098678588867\n",
      "Batch 118 loss: 0.8321663737297058, accuracy: 0.6432510614395142\n",
      "Batch 119 loss: 0.8281246423721313, accuracy: 0.6438801884651184\n",
      "Batch 120 loss: 0.9049999117851257, accuracy: 0.6441761255264282\n",
      "Batch 121 loss: 0.788981020450592, accuracy: 0.6448514461517334\n",
      "Batch 122 loss: 0.9717796444892883, accuracy: 0.6446900367736816\n",
      "Batch 123 loss: 0.8768647909164429, accuracy: 0.6447832584381104\n",
      "Batch 124 loss: 0.9088759422302246, accuracy: 0.6451249718666077\n",
      "Batch 125 loss: 0.8316600322723389, accuracy: 0.6458333134651184\n",
      "Batch 126 loss: 1.0570532083511353, accuracy: 0.6455462574958801\n",
      "Batch 127 loss: 1.160613775253296, accuracy: 0.6444091796875\n",
      "Batch 128 loss: 0.9802467226982117, accuracy: 0.644016444683075\n",
      "Batch 129 loss: 0.8691791892051697, accuracy: 0.64453125\n",
      "Batch 130 loss: 1.050258994102478, accuracy: 0.6443225145339966\n",
      "Batch 131 loss: 0.9983532428741455, accuracy: 0.6439985632896423\n",
      "Batch 132 loss: 0.9235454797744751, accuracy: 0.6442669034004211\n",
      "Batch 133 loss: 0.8865340352058411, accuracy: 0.6442980170249939\n",
      "Batch 134 loss: 0.6939738392829895, accuracy: 0.6451967358589172\n",
      "Batch 135 loss: 0.8543413281440735, accuracy: 0.6454503536224365\n",
      "Batch 136 loss: 0.9111422300338745, accuracy: 0.6454151272773743\n",
      "Batch 137 loss: 0.999433159828186, accuracy: 0.64504075050354\n",
      "Batch 138 loss: 0.8072956204414368, accuracy: 0.6453462243080139\n",
      "Batch 139 loss: 1.2146034240722656, accuracy: 0.6450334787368774\n",
      "Batch 140 loss: 1.1429189443588257, accuracy: 0.6447805762290955\n",
      "Batch 141 loss: 0.8838528394699097, accuracy: 0.6446962952613831\n",
      "Batch 142 loss: 0.9379264116287231, accuracy: 0.6450502872467041\n",
      "Batch 143 loss: 0.8468753695487976, accuracy: 0.6452907919883728\n",
      "Batch 144 loss: 1.0751677751541138, accuracy: 0.6450430750846863\n",
      "Batch 145 loss: 0.9226275682449341, accuracy: 0.6451733708381653\n",
      "Batch 146 loss: 0.817099392414093, accuracy: 0.6454081535339355\n",
      "Batch 147 loss: 0.7698187232017517, accuracy: 0.6459037065505981\n",
      "Batch 148 loss: 0.9624130129814148, accuracy: 0.6460255980491638\n",
      "Batch 149 loss: 0.9669050574302673, accuracy: 0.6458854079246521\n",
      "Batch 150 loss: 1.0097596645355225, accuracy: 0.6457471251487732\n",
      "Batch 151 loss: 0.6851243376731873, accuracy: 0.6465357542037964\n",
      "Batch 152 loss: 0.9230950474739075, accuracy: 0.6461907625198364\n",
      "Batch 153 loss: 1.0146204233169556, accuracy: 0.6458502411842346\n",
      "Batch 154 loss: 0.9805351495742798, accuracy: 0.6458669304847717\n",
      "Batch 155 loss: 0.900928795337677, accuracy: 0.6462840437889099\n",
      "Batch 156 loss: 0.9190822839736938, accuracy: 0.6464470624923706\n",
      "Batch 157 loss: 0.8965560793876648, accuracy: 0.646657407283783\n",
      "Batch 158 loss: 0.9599589705467224, accuracy: 0.6466194987297058\n",
      "Batch 159 loss: 1.0107699632644653, accuracy: 0.6463378667831421\n",
      "Batch 160 loss: 0.9133737683296204, accuracy: 0.6464479565620422\n",
      "Batch 161 loss: 0.9241846799850464, accuracy: 0.646460235118866\n",
      "Batch 162 loss: 1.0461435317993164, accuracy: 0.6458972096443176\n",
      "Batch 163 loss: 0.8931850790977478, accuracy: 0.6460080146789551\n",
      "Batch 164 loss: 0.9210790395736694, accuracy: 0.6457386612892151\n",
      "Batch 165 loss: 0.9397523403167725, accuracy: 0.6457078456878662\n",
      "Batch 166 loss: 0.9099107980728149, accuracy: 0.6458645462989807\n",
      "Batch 167 loss: 0.9155899882316589, accuracy: 0.6458333134651184\n",
      "Batch 168 loss: 1.2155340909957886, accuracy: 0.6456176042556763\n",
      "Batch 169 loss: 0.93929523229599, accuracy: 0.645588219165802\n",
      "Batch 170 loss: 0.8865225911140442, accuracy: 0.6454221606254578\n",
      "Batch 171 loss: 0.8693830966949463, accuracy: 0.6453034281730652\n",
      "Batch 172 loss: 0.9501798152923584, accuracy: 0.6449151039123535\n",
      "Batch 173 loss: 1.012740969657898, accuracy: 0.64453125\n",
      "Batch 174 loss: 0.8633928894996643, accuracy: 0.644508957862854\n",
      "Batch 175 loss: 1.0104472637176514, accuracy: 0.6446644067764282\n",
      "Batch 176 loss: 0.913719117641449, accuracy: 0.6447740197181702\n",
      "Batch 177 loss: 1.0355883836746216, accuracy: 0.644311785697937\n",
      "Batch 178 loss: 0.7930981516838074, accuracy: 0.6446403861045837\n",
      "Batch 179 loss: 0.8028033971786499, accuracy: 0.6452690958976746\n",
      "Batch 180 loss: 0.9621973633766174, accuracy: 0.645588755607605\n",
      "Batch 181 loss: 0.9189208149909973, accuracy: 0.6454756259918213\n",
      "Batch 182 loss: 1.0424139499664307, accuracy: 0.6454064249992371\n",
      "Batch 183 loss: 0.9352527260780334, accuracy: 0.645380437374115\n",
      "Batch 184 loss: 0.9187269806861877, accuracy: 0.6451435685157776\n",
      "Batch 185 loss: 0.8414310216903687, accuracy: 0.6453713178634644\n",
      "Batch 186 loss: 0.8444660902023315, accuracy: 0.6456383466720581\n",
      "Batch 187 loss: 1.0157526731491089, accuracy: 0.6455701589584351\n",
      "Batch 188 loss: 0.9687350392341614, accuracy: 0.6455439925193787\n",
      "Batch 189 loss: 0.9360963702201843, accuracy: 0.6456003189086914\n",
      "Batch 190 loss: 0.8957343101501465, accuracy: 0.6454106569290161\n",
      "Batch 191 loss: 0.927902340888977, accuracy: 0.6455078125\n",
      "Batch 192 loss: 0.9225199222564697, accuracy: 0.6454825401306152\n",
      "Batch 193 loss: 1.1102744340896606, accuracy: 0.6449742317199707\n",
      "Batch 194 loss: 0.8057324290275574, accuracy: 0.645272433757782\n",
      "Batch 195 loss: 0.8583434224128723, accuracy: 0.6455675959587097\n",
      "Batch 196 loss: 0.8685005307197571, accuracy: 0.6459391117095947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 197 loss: 0.9293456077575684, accuracy: 0.6460700631141663\n",
      "Batch 198 loss: 0.9174591898918152, accuracy: 0.6460819840431213\n",
      "Batch 199 loss: 0.7931725382804871, accuracy: 0.6463281512260437\n",
      "Batch 200 loss: 0.9207468032836914, accuracy: 0.6461054086685181\n",
      "Batch 201 loss: 1.1289423704147339, accuracy: 0.6458848714828491\n",
      "Batch 202 loss: 0.9174882173538208, accuracy: 0.6461284160614014\n",
      "Batch 203 loss: 0.8756229281425476, accuracy: 0.6463311910629272\n",
      "Batch 204 loss: 0.9297562837600708, accuracy: 0.6462652683258057\n",
      "Batch 205 loss: 0.9308686852455139, accuracy: 0.6461999416351318\n",
      "Batch 206 loss: 0.9501832127571106, accuracy: 0.6463239789009094\n",
      "Batch 207 loss: 0.9413727521896362, accuracy: 0.6462965607643127\n",
      "Batch 208 loss: 0.9598943591117859, accuracy: 0.6464563608169556\n",
      "Batch 209 loss: 1.048384189605713, accuracy: 0.6458705067634583\n",
      "Batch 210 loss: 0.9100345969200134, accuracy: 0.6456605195999146\n",
      "Batch 211 loss: 0.9171093702316284, accuracy: 0.6457104682922363\n",
      "Batch 212 loss: 1.0008535385131836, accuracy: 0.6455765962600708\n",
      "Batch 213 loss: 0.8543121814727783, accuracy: 0.6457725167274475\n",
      "Batch 214 loss: 0.9071292877197266, accuracy: 0.645857572555542\n",
      "Batch 215 loss: 0.9118466377258301, accuracy: 0.6457971930503845\n",
      "Batch 216 loss: 0.9192925691604614, accuracy: 0.6457013487815857\n",
      "Batch 217 loss: 1.009583830833435, accuracy: 0.6456780433654785\n",
      "Batch 218 loss: 1.0633931159973145, accuracy: 0.6457619667053223\n",
      "Batch 219 loss: 0.9117356538772583, accuracy: 0.6456320881843567\n",
      "Batch 220 loss: 0.957280695438385, accuracy: 0.6458215713500977\n",
      "Batch 221 loss: 0.816971480846405, accuracy: 0.6459740996360779\n",
      "Batch 222 loss: 0.92952561378479, accuracy: 0.6457749605178833\n",
      "Batch 223 loss: 0.9312049150466919, accuracy: 0.6458565592765808\n",
      "Batch 224 loss: 1.0900288820266724, accuracy: 0.6457208395004272\n",
      "Training epoch: 18, train accuracy: 64.57208251953125, train loss: 0.9367780152956645, valid accuracy: 62.385066986083984, valid loss: 1.0654248615791058 \n",
      "Batch 0 loss: 0.8471394181251526, accuracy: 0.6484375\n",
      "Batch 1 loss: 1.008914828300476, accuracy: 0.6328125\n",
      "Batch 2 loss: 1.023770809173584, accuracy: 0.6223958134651184\n",
      "Batch 3 loss: 1.0191577672958374, accuracy: 0.6171875\n",
      "Batch 4 loss: 1.0732349157333374, accuracy: 0.6156250238418579\n",
      "Batch 5 loss: 0.80193692445755, accuracy: 0.6354166865348816\n",
      "Batch 6 loss: 0.9002915620803833, accuracy: 0.6417410969734192\n",
      "Batch 7 loss: 0.8142049312591553, accuracy: 0.64453125\n",
      "Batch 8 loss: 0.9211481213569641, accuracy: 0.6423611044883728\n",
      "Batch 9 loss: 1.0050976276397705, accuracy: 0.6421874761581421\n",
      "Batch 10 loss: 1.1162863969802856, accuracy: 0.6384943127632141\n",
      "Batch 11 loss: 0.8025072813034058, accuracy: 0.642578125\n",
      "Batch 12 loss: 0.9592828154563904, accuracy: 0.6418269276618958\n",
      "Batch 13 loss: 0.9233015775680542, accuracy: 0.6417410969734192\n",
      "Batch 14 loss: 0.8680183291435242, accuracy: 0.6468750238418579\n",
      "Batch 15 loss: 0.9768075346946716, accuracy: 0.64990234375\n",
      "Batch 16 loss: 0.8352739810943604, accuracy: 0.6539521813392639\n",
      "Batch 17 loss: 0.875397801399231, accuracy: 0.6566840410232544\n",
      "Batch 18 loss: 0.8886455297470093, accuracy: 0.6583059430122375\n",
      "Batch 19 loss: 0.9520213007926941, accuracy: 0.6558593511581421\n",
      "Batch 20 loss: 0.8874413371086121, accuracy: 0.6581101417541504\n",
      "Batch 21 loss: 0.9516264200210571, accuracy: 0.6576704382896423\n",
      "Batch 22 loss: 0.80340975522995, accuracy: 0.6589673757553101\n",
      "Batch 23 loss: 0.8610216975212097, accuracy: 0.6591796875\n",
      "Batch 24 loss: 0.9003786444664001, accuracy: 0.6600000262260437\n",
      "Batch 25 loss: 0.7532417178153992, accuracy: 0.6625601053237915\n",
      "Batch 26 loss: 0.8986506462097168, accuracy: 0.6634837985038757\n",
      "Batch 27 loss: 0.9743151664733887, accuracy: 0.6615513563156128\n",
      "Batch 28 loss: 0.8205485939979553, accuracy: 0.6613685488700867\n",
      "Batch 29 loss: 0.8957428336143494, accuracy: 0.6617187261581421\n",
      "Batch 30 loss: 0.8207005262374878, accuracy: 0.6622983813285828\n",
      "Batch 31 loss: 1.0126101970672607, accuracy: 0.66064453125\n",
      "Batch 32 loss: 0.9350823163986206, accuracy: 0.6619318127632141\n",
      "Batch 33 loss: 0.8386731147766113, accuracy: 0.6613051295280457\n",
      "Batch 34 loss: 0.7505909204483032, accuracy: 0.6618303656578064\n",
      "Batch 35 loss: 0.8439419865608215, accuracy: 0.6629774570465088\n",
      "Batch 36 loss: 0.8835680484771729, accuracy: 0.6636402010917664\n",
      "Batch 37 loss: 0.7218855619430542, accuracy: 0.6655016541481018\n",
      "Batch 38 loss: 0.8707695007324219, accuracy: 0.6666666865348816\n",
      "Batch 39 loss: 0.9897664189338684, accuracy: 0.666015625\n",
      "Batch 40 loss: 0.8803291320800781, accuracy: 0.6655868887901306\n",
      "Batch 41 loss: 0.7582414150238037, accuracy: 0.6666666865348816\n",
      "Batch 42 loss: 0.9335893392562866, accuracy: 0.6658793687820435\n",
      "Batch 43 loss: 0.8718733191490173, accuracy: 0.6658380627632141\n",
      "Batch 44 loss: 0.8650351762771606, accuracy: 0.6656249761581421\n",
      "Batch 45 loss: 0.8714715838432312, accuracy: 0.6652513742446899\n",
      "Batch 46 loss: 0.8821349143981934, accuracy: 0.6652260422706604\n",
      "Batch 47 loss: 0.841918408870697, accuracy: 0.6656901240348816\n",
      "Batch 48 loss: 0.8921688795089722, accuracy: 0.6658163070678711\n",
      "Batch 49 loss: 0.8464391231536865, accuracy: 0.6653125286102295\n",
      "Batch 50 loss: 1.0517698526382446, accuracy: 0.6642156839370728\n",
      "Batch 51 loss: 0.9483411908149719, accuracy: 0.6643629670143127\n",
      "Batch 52 loss: 0.7841103076934814, accuracy: 0.6646521091461182\n",
      "Batch 53 loss: 0.8285644054412842, accuracy: 0.6655092835426331\n",
      "Batch 54 loss: 1.0497764348983765, accuracy: 0.6643465757369995\n",
      "Batch 55 loss: 0.898179829120636, accuracy: 0.6650390625\n",
      "Batch 56 loss: 1.0332614183425903, accuracy: 0.663514256477356\n",
      "Batch 57 loss: 0.8649682998657227, accuracy: 0.6631196141242981\n",
      "Batch 58 loss: 0.9723730683326721, accuracy: 0.6618114113807678\n",
      "Batch 59 loss: 0.8885668516159058, accuracy: 0.6611979007720947\n",
      "Batch 60 loss: 0.8090860843658447, accuracy: 0.6617571711540222\n",
      "Batch 61 loss: 0.7852252721786499, accuracy: 0.6628023982048035\n",
      "Batch 62 loss: 1.1713788509368896, accuracy: 0.661210298538208\n",
      "Batch 63 loss: 0.9244143962860107, accuracy: 0.6610107421875\n",
      "Batch 64 loss: 1.0126081705093384, accuracy: 0.6604567170143127\n",
      "Batch 65 loss: 0.8617376089096069, accuracy: 0.6609848737716675\n",
      "Batch 66 loss: 0.9395408630371094, accuracy: 0.6602145433425903\n",
      "Batch 67 loss: 0.8519201874732971, accuracy: 0.660041332244873\n",
      "Batch 68 loss: 1.0582497119903564, accuracy: 0.6591938138008118\n",
      "Batch 69 loss: 0.7370822429656982, accuracy: 0.6608259081840515\n",
      "Batch 70 loss: 0.9146972298622131, accuracy: 0.6608715057373047\n",
      "Batch 71 loss: 1.0391780138015747, accuracy: 0.6606987714767456\n",
      "Batch 72 loss: 0.9432953596115112, accuracy: 0.660744845867157\n",
      "Batch 73 loss: 0.8460737466812134, accuracy: 0.6604729890823364\n",
      "Batch 74 loss: 0.9346877336502075, accuracy: 0.6604166626930237\n",
      "Batch 75 loss: 0.9125102758407593, accuracy: 0.6602590680122375\n",
      "Batch 76 loss: 1.0388089418411255, accuracy: 0.6594967246055603\n",
      "Batch 77 loss: 0.9326920509338379, accuracy: 0.6591546535491943\n",
      "Batch 78 loss: 0.78311687707901, accuracy: 0.6596123576164246\n",
      "Batch 79 loss: 0.9215357899665833, accuracy: 0.659375011920929\n",
      "Batch 80 loss: 0.798396110534668, accuracy: 0.6603009104728699\n",
      "Batch 81 loss: 1.0671234130859375, accuracy: 0.66015625\n",
      "Batch 82 loss: 0.8340615630149841, accuracy: 0.6604856848716736\n",
      "Batch 83 loss: 0.9128063917160034, accuracy: 0.6606212854385376\n",
      "Batch 84 loss: 0.8741391897201538, accuracy: 0.6600183844566345\n",
      "Batch 85 loss: 0.9890897274017334, accuracy: 0.6597929000854492\n",
      "Batch 86 loss: 0.8311832547187805, accuracy: 0.6601113677024841\n",
      "Batch 87 loss: 1.1025822162628174, accuracy: 0.6588245630264282\n",
      "Batch 88 loss: 0.7484747767448425, accuracy: 0.6594979166984558\n",
      "Batch 89 loss: 0.8157857060432434, accuracy: 0.66015625\n",
      "Batch 90 loss: 0.9802982211112976, accuracy: 0.6591689586639404\n",
      "Batch 91 loss: 0.897493839263916, accuracy: 0.6592221260070801\n",
      "Batch 92 loss: 0.875003457069397, accuracy: 0.6594421863555908\n",
      "Batch 93 loss: 0.9686688780784607, accuracy: 0.659740686416626\n",
      "Batch 94 loss: 1.0044080018997192, accuracy: 0.6597039699554443\n",
      "Batch 95 loss: 0.9880648255348206, accuracy: 0.659423828125\n",
      "Batch 96 loss: 0.9109796285629272, accuracy: 0.6589078903198242\n",
      "Batch 97 loss: 0.7664995193481445, accuracy: 0.659757673740387\n",
      "Batch 98 loss: 1.1198694705963135, accuracy: 0.6591698527336121\n",
      "Batch 99 loss: 0.9540201425552368, accuracy: 0.6585937738418579\n",
      "Batch 100 loss: 1.021830677986145, accuracy: 0.6581064462661743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 101 loss: 0.9099172353744507, accuracy: 0.6578584313392639\n",
      "Batch 102 loss: 0.8275736570358276, accuracy: 0.6586013436317444\n",
      "Batch 103 loss: 0.995372474193573, accuracy: 0.6579026579856873\n",
      "Batch 104 loss: 0.9786838293075562, accuracy: 0.6571428775787354\n",
      "Batch 105 loss: 0.8232186436653137, accuracy: 0.6572081446647644\n",
      "Batch 106 loss: 0.8513402938842773, accuracy: 0.6571261882781982\n",
      "Batch 107 loss: 0.9547649621963501, accuracy: 0.6568287014961243\n",
      "Batch 108 loss: 1.0546059608459473, accuracy: 0.65617835521698\n",
      "Batch 109 loss: 0.7967589497566223, accuracy: 0.6565340757369995\n",
      "Batch 110 loss: 0.8803167939186096, accuracy: 0.6566722989082336\n",
      "Batch 111 loss: 0.8694531917572021, accuracy: 0.6568080186843872\n",
      "Batch 112 loss: 0.8511261343955994, accuracy: 0.6572179198265076\n",
      "Batch 113 loss: 0.9908523559570312, accuracy: 0.6576206088066101\n",
      "Batch 114 loss: 0.8534920811653137, accuracy: 0.657880425453186\n",
      "Batch 115 loss: 0.9418115615844727, accuracy: 0.6574622988700867\n",
      "Batch 116 loss: 1.0364298820495605, accuracy: 0.6569177508354187\n",
      "Batch 117 loss: 0.9516816139221191, accuracy: 0.6569783091545105\n",
      "Batch 118 loss: 0.9077796339988708, accuracy: 0.6571034789085388\n",
      "Batch 119 loss: 0.9765843749046326, accuracy: 0.6566406488418579\n",
      "Batch 120 loss: 0.8995624780654907, accuracy: 0.6566373705863953\n",
      "Batch 121 loss: 0.942406177520752, accuracy: 0.6561219096183777\n",
      "Batch 122 loss: 0.8434305191040039, accuracy: 0.6564405560493469\n",
      "Batch 123 loss: 0.848957359790802, accuracy: 0.6564390063285828\n",
      "Batch 124 loss: 1.0388659238815308, accuracy: 0.656374990940094\n",
      "Batch 125 loss: 0.9315497279167175, accuracy: 0.656436026096344\n",
      "Batch 126 loss: 0.945469081401825, accuracy: 0.6561884880065918\n",
      "Batch 127 loss: 0.9823207855224609, accuracy: 0.656005859375\n",
      "Batch 128 loss: 0.8759662508964539, accuracy: 0.6560683250427246\n",
      "Batch 129 loss: 0.9465885162353516, accuracy: 0.6560696959495544\n",
      "Batch 130 loss: 0.9942654371261597, accuracy: 0.655593991279602\n",
      "Batch 131 loss: 1.05399489402771, accuracy: 0.6547111868858337\n",
      "Batch 132 loss: 0.9889610409736633, accuracy: 0.654429018497467\n",
      "Batch 133 loss: 0.9179060459136963, accuracy: 0.6543260216712952\n",
      "Batch 134 loss: 0.9442967176437378, accuracy: 0.653877317905426\n",
      "Batch 135 loss: 0.9350388050079346, accuracy: 0.6538373231887817\n",
      "Batch 136 loss: 0.8178882002830505, accuracy: 0.6543111205101013\n",
      "Batch 137 loss: 0.8823548555374146, accuracy: 0.6543251872062683\n",
      "Batch 138 loss: 0.9535518288612366, accuracy: 0.6541141867637634\n",
      "Batch 139 loss: 0.9259272217750549, accuracy: 0.6541852951049805\n",
      "Batch 140 loss: 0.8493996262550354, accuracy: 0.6543107032775879\n",
      "Batch 141 loss: 0.8245611190795898, accuracy: 0.65470951795578\n",
      "Batch 142 loss: 0.9028841853141785, accuracy: 0.6547749042510986\n",
      "Batch 143 loss: 0.7640587687492371, accuracy: 0.6553276777267456\n",
      "Batch 144 loss: 0.834027111530304, accuracy: 0.6554957032203674\n",
      "Batch 145 loss: 1.0391435623168945, accuracy: 0.6556078791618347\n",
      "Batch 146 loss: 0.9561051726341248, accuracy: 0.6557185649871826\n",
      "Batch 147 loss: 0.9222490191459656, accuracy: 0.6557748913764954\n",
      "Batch 148 loss: 0.9571598172187805, accuracy: 0.6555159687995911\n",
      "Batch 149 loss: 0.9694214463233948, accuracy: 0.6550520658493042\n",
      "Batch 150 loss: 0.9188525080680847, accuracy: 0.6551117300987244\n",
      "Batch 151 loss: 1.0067437887191772, accuracy: 0.6548622250556946\n",
      "Batch 152 loss: 0.8557324409484863, accuracy: 0.655024528503418\n",
      "Batch 153 loss: 0.8952066898345947, accuracy: 0.6549817323684692\n",
      "Batch 154 loss: 0.9317361116409302, accuracy: 0.6549395322799683\n",
      "Batch 155 loss: 0.8860465884208679, accuracy: 0.6548477411270142\n",
      "Batch 156 loss: 1.0898244380950928, accuracy: 0.6544585824012756\n",
      "Batch 157 loss: 0.9728829264640808, accuracy: 0.6543216109275818\n",
      "Batch 158 loss: 1.001204490661621, accuracy: 0.6541371941566467\n",
      "Batch 159 loss: 1.0373324155807495, accuracy: 0.6539062261581421\n",
      "Batch 160 loss: 0.8795839548110962, accuracy: 0.6540663838386536\n",
      "Batch 161 loss: 1.0163161754608154, accuracy: 0.6540316343307495\n",
      "Batch 162 loss: 0.7372697591781616, accuracy: 0.6547162532806396\n",
      "Batch 163 loss: 0.9300438165664673, accuracy: 0.6546779870986938\n",
      "Batch 164 loss: 0.8266054391860962, accuracy: 0.6551136374473572\n",
      "Batch 165 loss: 1.051196813583374, accuracy: 0.6548380851745605\n",
      "Batch 166 loss: 0.9987870454788208, accuracy: 0.65461266040802\n",
      "Batch 167 loss: 0.8515651226043701, accuracy: 0.6547154188156128\n",
      "Batch 168 loss: 0.9510148167610168, accuracy: 0.6544933319091797\n",
      "Batch 169 loss: 0.7763835191726685, accuracy: 0.654917299747467\n",
      "Batch 170 loss: 0.9723126888275146, accuracy: 0.6548793911933899\n",
      "Batch 171 loss: 1.00690495967865, accuracy: 0.6547056436538696\n",
      "Batch 172 loss: 1.1821945905685425, accuracy: 0.6541275382041931\n",
      "Batch 173 loss: 0.8931382894515991, accuracy: 0.6543193459510803\n",
      "Batch 174 loss: 0.9800613522529602, accuracy: 0.6541071534156799\n",
      "Batch 175 loss: 1.0523930788040161, accuracy: 0.6539861559867859\n",
      "Batch 176 loss: 0.831535816192627, accuracy: 0.6541755199432373\n",
      "Batch 177 loss: 1.0254327058792114, accuracy: 0.6539676785469055\n",
      "Batch 178 loss: 1.0295547246932983, accuracy: 0.6535003781318665\n",
      "Batch 179 loss: 0.8700109720230103, accuracy: 0.653515636920929\n",
      "Batch 180 loss: 0.9613258838653564, accuracy: 0.6535307168960571\n",
      "Batch 181 loss: 0.9973320960998535, accuracy: 0.6534598469734192\n",
      "Batch 182 loss: 1.0219743251800537, accuracy: 0.6530481576919556\n",
      "Batch 183 loss: 0.8980330228805542, accuracy: 0.6528957486152649\n",
      "Batch 184 loss: 0.8892178535461426, accuracy: 0.6529983282089233\n",
      "Batch 185 loss: 0.7832374572753906, accuracy: 0.6531838178634644\n",
      "Batch 186 loss: 0.8771993517875671, accuracy: 0.6532419919967651\n",
      "Batch 187 loss: 0.8416872620582581, accuracy: 0.6534242033958435\n",
      "Batch 188 loss: 0.9155414700508118, accuracy: 0.6536458134651184\n",
      "Batch 189 loss: 0.883055567741394, accuracy: 0.6538651585578918\n",
      "Batch 190 loss: 0.8987107276916504, accuracy: 0.6537958383560181\n",
      "Batch 191 loss: 0.83017498254776, accuracy: 0.654296875\n",
      "Batch 192 loss: 0.9846004843711853, accuracy: 0.6541045904159546\n",
      "Batch 193 loss: 0.8559127449989319, accuracy: 0.6541962027549744\n",
      "Batch 194 loss: 0.8972232937812805, accuracy: 0.6539262533187866\n",
      "Batch 195 loss: 0.94503253698349, accuracy: 0.6540178656578064\n",
      "Batch 196 loss: 0.8506837487220764, accuracy: 0.654187798500061\n",
      "Batch 197 loss: 0.8828386068344116, accuracy: 0.6541982293128967\n",
      "Batch 198 loss: 0.8315027952194214, accuracy: 0.6542870402336121\n",
      "Batch 199 loss: 0.8847858309745789, accuracy: 0.654296875\n",
      "Batch 200 loss: 0.8915220499038696, accuracy: 0.6542677283287048\n",
      "Batch 201 loss: 0.9428947567939758, accuracy: 0.6541228294372559\n",
      "Batch 202 loss: 1.0409249067306519, accuracy: 0.6538254022598267\n",
      "Batch 203 loss: 0.8689505457878113, accuracy: 0.6541053652763367\n",
      "Batch 204 loss: 0.9017290472984314, accuracy: 0.6542301774024963\n",
      "Batch 205 loss: 0.8620844483375549, accuracy: 0.6544296145439148\n",
      "Batch 206 loss: 1.112267255783081, accuracy: 0.6542119383811951\n",
      "Batch 207 loss: 1.034937858581543, accuracy: 0.653883695602417\n",
      "Batch 208 loss: 0.9660698175430298, accuracy: 0.6536707282066345\n",
      "Batch 209 loss: 0.9940947890281677, accuracy: 0.6537202596664429\n",
      "Batch 210 loss: 0.8196375370025635, accuracy: 0.6538803577423096\n",
      "Batch 211 loss: 0.9200591444969177, accuracy: 0.6540389060974121\n",
      "Batch 212 loss: 0.8908547759056091, accuracy: 0.6541226506233215\n",
      "Batch 213 loss: 0.8619422316551208, accuracy: 0.6542420983314514\n",
      "Batch 214 loss: 0.8936765789985657, accuracy: 0.6542878150939941\n",
      "Batch 215 loss: 0.9232053756713867, accuracy: 0.6545500755310059\n",
      "Batch 216 loss: 0.9981452226638794, accuracy: 0.6548098921775818\n",
      "Batch 217 loss: 1.0846906900405884, accuracy: 0.654601514339447\n",
      "Batch 218 loss: 1.0282130241394043, accuracy: 0.6542165875434875\n",
      "Batch 219 loss: 0.950773298740387, accuracy: 0.6542613506317139\n",
      "Batch 220 loss: 0.9236340522766113, accuracy: 0.6540936231613159\n",
      "Batch 221 loss: 0.9616022706031799, accuracy: 0.6538569927215576\n",
      "Batch 222 loss: 0.9802577495574951, accuracy: 0.6536925435066223\n",
      "Batch 223 loss: 1.1586642265319824, accuracy: 0.6532505750656128\n",
      "Batch 224 loss: 0.8244332671165466, accuracy: 0.6532446146011353\n",
      "Training epoch: 19, train accuracy: 65.324462890625, train loss: 0.9210285091400147, valid accuracy: 61.71635437011719, valid loss: 1.0147167629209057 \n",
      "Batch 0 loss: 0.8701654672622681, accuracy: 0.6796875\n",
      "Batch 1 loss: 0.8411943316459656, accuracy: 0.671875\n",
      "Batch 2 loss: 0.9073157906532288, accuracy: 0.6640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 loss: 0.9364869594573975, accuracy: 0.662109375\n",
      "Batch 4 loss: 0.9345478415489197, accuracy: 0.65625\n",
      "Batch 5 loss: 0.9039727449417114, accuracy: 0.6627604365348816\n",
      "Batch 6 loss: 0.8755301237106323, accuracy: 0.6629464030265808\n",
      "Batch 7 loss: 0.9544899463653564, accuracy: 0.6552734375\n",
      "Batch 8 loss: 0.8545222282409668, accuracy: 0.6579861044883728\n",
      "Batch 9 loss: 0.877860426902771, accuracy: 0.6507812738418579\n",
      "Batch 10 loss: 1.0342915058135986, accuracy: 0.6434659361839294\n",
      "Batch 11 loss: 0.9270085096359253, accuracy: 0.6458333134651184\n",
      "Batch 12 loss: 0.903835117816925, accuracy: 0.6448317170143127\n",
      "Batch 13 loss: 0.944321870803833, accuracy: 0.6484375\n",
      "Batch 14 loss: 0.8183912634849548, accuracy: 0.6520833373069763\n",
      "Batch 15 loss: 0.8265561461448669, accuracy: 0.65185546875\n",
      "Batch 16 loss: 0.8761602640151978, accuracy: 0.650275707244873\n",
      "Batch 17 loss: 0.8214234113693237, accuracy: 0.6532118320465088\n",
      "Batch 18 loss: 0.753032386302948, accuracy: 0.6574835777282715\n",
      "Batch 19 loss: 0.7799361944198608, accuracy: 0.6605468988418579\n",
      "Batch 20 loss: 1.0571626424789429, accuracy: 0.6595982313156128\n",
      "Batch 21 loss: 0.8070028424263, accuracy: 0.6598011255264282\n",
      "Batch 22 loss: 0.8518505096435547, accuracy: 0.66236412525177\n",
      "Batch 23 loss: 0.8241763114929199, accuracy: 0.6627604365348816\n",
      "Batch 24 loss: 0.7195230722427368, accuracy: 0.6662499904632568\n",
      "Batch 25 loss: 0.9623404145240784, accuracy: 0.6658653616905212\n",
      "Batch 26 loss: 0.8690510988235474, accuracy: 0.6666666865348816\n",
      "Batch 27 loss: 0.9344449043273926, accuracy: 0.6651785969734192\n",
      "Batch 28 loss: 0.8554061055183411, accuracy: 0.6648706793785095\n",
      "Batch 29 loss: 0.9192429780960083, accuracy: 0.663281261920929\n",
      "Batch 30 loss: 0.8873124718666077, accuracy: 0.6622983813285828\n",
      "Batch 31 loss: 0.9231257438659668, accuracy: 0.66259765625\n",
      "Batch 32 loss: 0.9355742931365967, accuracy: 0.6607481241226196\n",
      "Batch 33 loss: 1.0190556049346924, accuracy: 0.6610753536224365\n",
      "Batch 34 loss: 0.8903498649597168, accuracy: 0.6618303656578064\n",
      "Batch 35 loss: 0.9279904961585999, accuracy: 0.6616753339767456\n",
      "Batch 36 loss: 0.863760769367218, accuracy: 0.6617398858070374\n",
      "Batch 37 loss: 0.8088771104812622, accuracy: 0.6624177694320679\n",
      "Batch 38 loss: 1.1076371669769287, accuracy: 0.6596554517745972\n",
      "Batch 39 loss: 0.9661674499511719, accuracy: 0.6585937738418579\n",
      "Batch 40 loss: 0.9963138103485107, accuracy: 0.6577743887901306\n",
      "Batch 41 loss: 0.8819364905357361, accuracy: 0.6573660969734192\n",
      "Batch 42 loss: 0.7900378108024597, accuracy: 0.6580668687820435\n",
      "Batch 43 loss: 0.8416520357131958, accuracy: 0.6578480005264282\n",
      "Batch 44 loss: 0.9848179221153259, accuracy: 0.6571180820465088\n",
      "Batch 45 loss: 0.810795783996582, accuracy: 0.6581181883811951\n",
      "Batch 46 loss: 1.057024359703064, accuracy: 0.65625\n",
      "Batch 47 loss: 0.9534623622894287, accuracy: 0.6560872197151184\n",
      "Batch 48 loss: 0.7854310870170593, accuracy: 0.6567283272743225\n",
      "Batch 49 loss: 0.7833324670791626, accuracy: 0.6576562523841858\n",
      "Batch 50 loss: 1.013672113418579, accuracy: 0.6560968160629272\n",
      "Batch 51 loss: 0.8481430411338806, accuracy: 0.6560997366905212\n",
      "Batch 52 loss: 0.8922864198684692, accuracy: 0.65625\n",
      "Batch 53 loss: 0.8695606589317322, accuracy: 0.6572627425193787\n",
      "Batch 54 loss: 0.8264362812042236, accuracy: 0.6578124761581421\n",
      "Batch 55 loss: 1.0022050142288208, accuracy: 0.6580635905265808\n",
      "Batch 56 loss: 0.9491053819656372, accuracy: 0.656798243522644\n",
      "Batch 57 loss: 0.9548853039741516, accuracy: 0.65625\n",
      "Batch 58 loss: 0.8624638319015503, accuracy: 0.6570444703102112\n",
      "Batch 59 loss: 0.8898522257804871, accuracy: 0.6569010615348816\n",
      "Batch 60 loss: 1.0488554239273071, accuracy: 0.6554815769195557\n",
      "Batch 61 loss: 0.7641378045082092, accuracy: 0.6563760042190552\n",
      "Batch 62 loss: 0.891250491142273, accuracy: 0.6563740372657776\n",
      "Batch 63 loss: 1.0472087860107422, accuracy: 0.6563720703125\n",
      "Batch 64 loss: 0.9015296697616577, accuracy: 0.6560096144676208\n",
      "Batch 65 loss: 0.8936722278594971, accuracy: 0.6563683748245239\n",
      "Batch 66 loss: 0.9399212002754211, accuracy: 0.6560167670249939\n",
      "Batch 67 loss: 0.9378912448883057, accuracy: 0.656135082244873\n",
      "Batch 68 loss: 0.8414900302886963, accuracy: 0.656589686870575\n",
      "Batch 69 loss: 0.9534778594970703, accuracy: 0.6560267806053162\n",
      "Batch 70 loss: 0.7835390567779541, accuracy: 0.6570202708244324\n",
      "Batch 71 loss: 0.8368473052978516, accuracy: 0.6574435830116272\n",
      "Batch 72 loss: 1.0328670740127563, accuracy: 0.6566780805587769\n",
      "Batch 73 loss: 0.947715699672699, accuracy: 0.6565667390823364\n",
      "Batch 74 loss: 0.9316204190254211, accuracy: 0.6565625071525574\n",
      "Batch 75 loss: 0.7742730379104614, accuracy: 0.6572779417037964\n",
      "Batch 76 loss: 0.7930517792701721, accuracy: 0.658076286315918\n",
      "Batch 77 loss: 0.8151429295539856, accuracy: 0.6578525900840759\n",
      "Batch 78 loss: 0.7905746698379517, accuracy: 0.6584256291389465\n",
      "Batch 79 loss: 0.8908780217170715, accuracy: 0.6587890386581421\n",
      "Batch 80 loss: 0.8972913026809692, accuracy: 0.6584683656692505\n",
      "Batch 81 loss: 0.848570704460144, accuracy: 0.6583460569381714\n",
      "Batch 82 loss: 0.9156197309494019, accuracy: 0.6582266688346863\n",
      "Batch 83 loss: 0.9627723693847656, accuracy: 0.658017098903656\n",
      "Batch 84 loss: 0.9173980355262756, accuracy: 0.6578124761581421\n",
      "Batch 85 loss: 1.0038868188858032, accuracy: 0.6573401093482971\n",
      "Batch 86 loss: 0.8576692938804626, accuracy: 0.657417356967926\n",
      "Batch 87 loss: 1.0411533117294312, accuracy: 0.6566938757896423\n",
      "Batch 88 loss: 1.0198016166687012, accuracy: 0.6561622023582458\n",
      "Batch 89 loss: 0.8766370415687561, accuracy: 0.65625\n",
      "Batch 90 loss: 0.9412036538124084, accuracy: 0.6564217209815979\n",
      "Batch 91 loss: 0.7890196442604065, accuracy: 0.6572690010070801\n",
      "Batch 92 loss: 1.0068258047103882, accuracy: 0.656838059425354\n",
      "Batch 93 loss: 0.885562002658844, accuracy: 0.6565824747085571\n",
      "Batch 94 loss: 0.8953841924667358, accuracy: 0.65633225440979\n",
      "Batch 95 loss: 0.8527968525886536, accuracy: 0.6565755009651184\n",
      "Batch 96 loss: 0.8857364654541016, accuracy: 0.6567332744598389\n",
      "Batch 97 loss: 0.9420973062515259, accuracy: 0.6563296914100647\n",
      "Batch 98 loss: 0.840034008026123, accuracy: 0.6564078330993652\n",
      "Batch 99 loss: 0.9374474883079529, accuracy: 0.6563281416893005\n",
      "Batch 100 loss: 0.7624580264091492, accuracy: 0.6573328971862793\n",
      "Batch 101 loss: 0.9007052183151245, accuracy: 0.6569393277168274\n",
      "Batch 102 loss: 0.8471003770828247, accuracy: 0.6573119163513184\n",
      "Batch 103 loss: 0.9277165532112122, accuracy: 0.6574519276618958\n",
      "Batch 104 loss: 0.7984032034873962, accuracy: 0.6580356955528259\n",
      "Batch 105 loss: 0.9181911945343018, accuracy: 0.6578714847564697\n",
      "Batch 106 loss: 1.0820106267929077, accuracy: 0.6570531725883484\n",
      "Batch 107 loss: 0.9316649436950684, accuracy: 0.6572627425193787\n",
      "Batch 108 loss: 0.9606554508209229, accuracy: 0.6572534441947937\n",
      "Batch 109 loss: 0.9405620694160461, accuracy: 0.6568892002105713\n",
      "Batch 110 loss: 0.9677866697311401, accuracy: 0.6567426919937134\n",
      "Batch 111 loss: 0.9609264731407166, accuracy: 0.6565987467765808\n",
      "Batch 112 loss: 1.075948715209961, accuracy: 0.6559734344482422\n",
      "Batch 113 loss: 1.0211395025253296, accuracy: 0.6552905440330505\n",
      "Batch 114 loss: 0.9599586725234985, accuracy: 0.6550271511077881\n",
      "Batch 115 loss: 0.7740610837936401, accuracy: 0.655643880367279\n",
      "Batch 116 loss: 0.8623659610748291, accuracy: 0.6556490659713745\n",
      "Batch 117 loss: 0.8271952867507935, accuracy: 0.65625\n",
      "Batch 118 loss: 0.8706649541854858, accuracy: 0.6563813090324402\n",
      "Batch 119 loss: 0.8183907866477966, accuracy: 0.6568359136581421\n",
      "Batch 120 loss: 0.9170114994049072, accuracy: 0.657089352607727\n",
      "Batch 121 loss: 0.952006459236145, accuracy: 0.6570184230804443\n",
      "Batch 122 loss: 0.7539272904396057, accuracy: 0.6575203537940979\n",
      "Batch 123 loss: 0.8899521231651306, accuracy: 0.6575731039047241\n",
      "Batch 124 loss: 0.7706222534179688, accuracy: 0.6577500104904175\n",
      "Batch 125 loss: 0.9370282888412476, accuracy: 0.6575520634651184\n",
      "Batch 126 loss: 0.9469706416130066, accuracy: 0.657234251499176\n",
      "Batch 127 loss: 0.9262964129447937, accuracy: 0.65753173828125\n",
      "Batch 128 loss: 0.9368289709091187, accuracy: 0.6573401093482971\n",
      "Batch 129 loss: 1.0025084018707275, accuracy: 0.657031238079071\n",
      "Batch 130 loss: 0.9299501180648804, accuracy: 0.6572042107582092\n",
      "Batch 131 loss: 0.9308933615684509, accuracy: 0.6570194363594055\n",
      "Batch 132 loss: 0.858966052532196, accuracy: 0.6570136547088623\n",
      "Batch 133 loss: 0.953214168548584, accuracy: 0.6568329930305481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 134 loss: 0.9521342515945435, accuracy: 0.6564815044403076\n",
      "Batch 135 loss: 0.8635482788085938, accuracy: 0.6565946936607361\n",
      "Batch 136 loss: 0.8889039158821106, accuracy: 0.6567062139511108\n",
      "Batch 137 loss: 0.8983510136604309, accuracy: 0.6568727493286133\n",
      "Batch 138 loss: 0.9966138005256653, accuracy: 0.6570930480957031\n",
      "Batch 139 loss: 1.109720230102539, accuracy: 0.6563057899475098\n",
      "Batch 140 loss: 0.8180135488510132, accuracy: 0.6567486524581909\n",
      "Batch 141 loss: 0.8920037746429443, accuracy: 0.6567451357841492\n",
      "Batch 142 loss: 0.8861151933670044, accuracy: 0.6569055914878845\n",
      "Batch 143 loss: 0.9488894939422607, accuracy: 0.6571180820465088\n",
      "Batch 144 loss: 0.8405587673187256, accuracy: 0.6572198271751404\n",
      "Batch 145 loss: 1.0013378858566284, accuracy: 0.6572132110595703\n",
      "Batch 146 loss: 0.7672421932220459, accuracy: 0.6576849222183228\n",
      "Batch 147 loss: 0.7218146920204163, accuracy: 0.6579920053482056\n",
      "Batch 148 loss: 0.8851725459098816, accuracy: 0.6580327153205872\n",
      "Batch 149 loss: 0.8567876219749451, accuracy: 0.6581249833106995\n",
      "Batch 150 loss: 0.8294658064842224, accuracy: 0.6582160592079163\n",
      "Batch 151 loss: 0.7944167852401733, accuracy: 0.6586657166481018\n",
      "Batch 152 loss: 0.9085891842842102, accuracy: 0.6589052081108093\n",
      "Batch 153 loss: 1.0549927949905396, accuracy: 0.6586850881576538\n",
      "Batch 154 loss: 0.9979427456855774, accuracy: 0.6584677696228027\n",
      "Batch 155 loss: 0.9332037568092346, accuracy: 0.6586538553237915\n",
      "Batch 156 loss: 0.9631638526916504, accuracy: 0.6587380766868591\n",
      "Batch 157 loss: 0.9044300317764282, accuracy: 0.6585739850997925\n",
      "Batch 158 loss: 0.9104275107383728, accuracy: 0.6586576104164124\n",
      "Batch 159 loss: 0.9994885921478271, accuracy: 0.658642590045929\n",
      "Batch 160 loss: 0.9784713387489319, accuracy: 0.6587733030319214\n",
      "Batch 161 loss: 0.9204789996147156, accuracy: 0.6587576866149902\n",
      "Batch 162 loss: 0.8483330011367798, accuracy: 0.6589340567588806\n",
      "Batch 163 loss: 0.8416945934295654, accuracy: 0.6592035293579102\n",
      "Batch 164 loss: 0.9893510937690735, accuracy: 0.6589488387107849\n",
      "Batch 165 loss: 0.9501477479934692, accuracy: 0.6590737700462341\n",
      "Batch 166 loss: 0.9126126170158386, accuracy: 0.6589633226394653\n",
      "Batch 167 loss: 1.059262752532959, accuracy: 0.6586216688156128\n",
      "Batch 168 loss: 0.8382477164268494, accuracy: 0.659023642539978\n",
      "Batch 169 loss: 0.9293548464775085, accuracy: 0.659375011920929\n",
      "Batch 170 loss: 1.0804492235183716, accuracy: 0.6589912176132202\n",
      "Batch 171 loss: 0.9410863518714905, accuracy: 0.6589298844337463\n",
      "Batch 172 loss: 0.9897937178611755, accuracy: 0.65882408618927\n",
      "Batch 173 loss: 0.8730774521827698, accuracy: 0.6588990688323975\n",
      "Batch 174 loss: 0.9244921207427979, accuracy: 0.6589285731315613\n",
      "Batch 175 loss: 0.979594349861145, accuracy: 0.6589133739471436\n",
      "Batch 176 loss: 0.8866226673126221, accuracy: 0.6589865684509277\n",
      "Batch 177 loss: 0.9030100107192993, accuracy: 0.6589711904525757\n",
      "Batch 178 loss: 0.9241880178451538, accuracy: 0.6591305732727051\n",
      "Batch 179 loss: 0.9931875467300415, accuracy: 0.6589409708976746\n",
      "Batch 180 loss: 0.8942117691040039, accuracy: 0.659098744392395\n",
      "Batch 181 loss: 0.7664276361465454, accuracy: 0.6593835949897766\n",
      "Batch 182 loss: 0.8729578256607056, accuracy: 0.6593237519264221\n",
      "Batch 183 loss: 0.981902003288269, accuracy: 0.659052312374115\n",
      "Batch 184 loss: 0.9665507674217224, accuracy: 0.6592060923576355\n",
      "Batch 185 loss: 1.02777898311615, accuracy: 0.6590221524238586\n",
      "Batch 186 loss: 0.9316545128822327, accuracy: 0.6588402390480042\n",
      "Batch 187 loss: 0.8088575601577759, accuracy: 0.6590757966041565\n",
      "Batch 188 loss: 0.86194908618927, accuracy: 0.659102201461792\n",
      "Batch 189 loss: 0.8605054616928101, accuracy: 0.6591283082962036\n",
      "Batch 190 loss: 1.0424070358276367, accuracy: 0.6589087247848511\n",
      "Batch 191 loss: 0.8570449948310852, accuracy: 0.6589762568473816\n",
      "Batch 192 loss: 0.9046000838279724, accuracy: 0.6588811278343201\n",
      "Batch 193 loss: 0.8352742791175842, accuracy: 0.6590286493301392\n",
      "Batch 194 loss: 1.040795922279358, accuracy: 0.6587339639663696\n",
      "Batch 195 loss: 1.019631028175354, accuracy: 0.6588807106018066\n",
      "Batch 196 loss: 0.8782168626785278, accuracy: 0.6592639684677124\n",
      "Batch 197 loss: 0.800925612449646, accuracy: 0.6594065427780151\n",
      "Batch 198 loss: 0.827180027961731, accuracy: 0.659508466720581\n",
      "Batch 199 loss: 1.01907479763031, accuracy: 0.6594140529632568\n",
      "Batch 200 loss: 0.9656999707221985, accuracy: 0.6595149040222168\n",
      "Batch 201 loss: 0.7384685277938843, accuracy: 0.6600401997566223\n",
      "Batch 202 loss: 0.8475103378295898, accuracy: 0.6600215435028076\n",
      "Batch 203 loss: 1.0080548524856567, accuracy: 0.6598881483078003\n",
      "Batch 204 loss: 0.9981522560119629, accuracy: 0.659870445728302\n",
      "Batch 205 loss: 0.9331591725349426, accuracy: 0.6600045561790466\n",
      "Batch 206 loss: 1.0285080671310425, accuracy: 0.6601373553276062\n",
      "Batch 207 loss: 1.0749799013137817, accuracy: 0.6598933339118958\n",
      "Batch 208 loss: 0.7976677417755127, accuracy: 0.660025417804718\n",
      "Batch 209 loss: 0.8800588250160217, accuracy: 0.659970223903656\n",
      "Batch 210 loss: 0.8611151576042175, accuracy: 0.6600636839866638\n",
      "Batch 211 loss: 0.8186099529266357, accuracy: 0.6602668166160583\n",
      "Batch 212 loss: 0.9383533000946045, accuracy: 0.6600645780563354\n",
      "Batch 213 loss: 0.9459980726242065, accuracy: 0.6599007248878479\n",
      "Batch 214 loss: 0.9221789240837097, accuracy: 0.6598473787307739\n",
      "Batch 215 loss: 0.7692247033119202, accuracy: 0.6601200699806213\n",
      "Batch 216 loss: 0.8812729716300964, accuracy: 0.6603182554244995\n",
      "Batch 217 loss: 0.9457686543464661, accuracy: 0.6604071259498596\n",
      "Batch 218 loss: 0.9208019971847534, accuracy: 0.6603881120681763\n",
      "Batch 219 loss: 0.880551278591156, accuracy: 0.66015625\n",
      "Batch 220 loss: 0.968428909778595, accuracy: 0.6602092981338501\n",
      "Batch 221 loss: 0.8932194709777832, accuracy: 0.6601210832595825\n",
      "Batch 222 loss: 0.9012364149093628, accuracy: 0.6601036787033081\n",
      "Batch 223 loss: 0.9232590198516846, accuracy: 0.6602259874343872\n",
      "Batch 224 loss: 0.9954999089241028, accuracy: 0.6601414084434509\n",
      "Training epoch: 20, train accuracy: 66.01414489746094, train loss: 0.9077064535352919, valid accuracy: 63.02591323852539, valid loss: 1.0015317715447525 \n",
      "Batch 0 loss: 0.8628884553909302, accuracy: 0.703125\n",
      "Batch 1 loss: 0.8781596422195435, accuracy: 0.69140625\n",
      "Batch 2 loss: 0.9908459782600403, accuracy: 0.6614583134651184\n",
      "Batch 3 loss: 0.7780330181121826, accuracy: 0.673828125\n",
      "Batch 4 loss: 0.9428545236587524, accuracy: 0.671875\n",
      "Batch 5 loss: 0.8310428261756897, accuracy: 0.67578125\n",
      "Batch 6 loss: 0.8093097805976868, accuracy: 0.6808035969734192\n",
      "Batch 7 loss: 0.9275984168052673, accuracy: 0.67578125\n",
      "Batch 8 loss: 0.7685760855674744, accuracy: 0.6822916865348816\n",
      "Batch 9 loss: 0.9245896339416504, accuracy: 0.6796875\n",
      "Batch 10 loss: 0.7700372934341431, accuracy: 0.6818181872367859\n",
      "Batch 11 loss: 0.9113517999649048, accuracy: 0.681640625\n",
      "Batch 12 loss: 0.9673367142677307, accuracy: 0.676682710647583\n",
      "Batch 13 loss: 0.8867983222007751, accuracy: 0.67578125\n",
      "Batch 14 loss: 0.8615040183067322, accuracy: 0.6755208373069763\n",
      "Batch 15 loss: 0.8524346351623535, accuracy: 0.6787109375\n",
      "Batch 16 loss: 0.7451454401016235, accuracy: 0.6829044222831726\n",
      "Batch 17 loss: 0.9074739813804626, accuracy: 0.6805555820465088\n",
      "Batch 18 loss: 0.9550533294677734, accuracy: 0.6792762875556946\n",
      "Batch 19 loss: 0.891681432723999, accuracy: 0.6773437261581421\n",
      "Batch 20 loss: 0.7574501037597656, accuracy: 0.6789434552192688\n",
      "Batch 21 loss: 0.7726568579673767, accuracy: 0.6796875\n",
      "Batch 22 loss: 1.1329532861709595, accuracy: 0.6745923757553101\n",
      "Batch 23 loss: 0.9893271327018738, accuracy: 0.6735026240348816\n",
      "Batch 24 loss: 1.0354715585708618, accuracy: 0.671875\n",
      "Batch 25 loss: 0.9868887066841125, accuracy: 0.6694711446762085\n",
      "Batch 26 loss: 0.7838561534881592, accuracy: 0.6701388955116272\n",
      "Batch 27 loss: 0.9096446633338928, accuracy: 0.6693638563156128\n",
      "Batch 28 loss: 0.9158802628517151, accuracy: 0.6691810488700867\n",
      "Batch 29 loss: 0.9323031902313232, accuracy: 0.6684895753860474\n",
      "Batch 30 loss: 1.0404682159423828, accuracy: 0.6668346524238586\n",
      "Batch 31 loss: 0.9405319094657898, accuracy: 0.666748046875\n",
      "Batch 32 loss: 0.8553011417388916, accuracy: 0.6678503751754761\n",
      "Batch 33 loss: 0.8359603881835938, accuracy: 0.6677389740943909\n",
      "Batch 34 loss: 0.926020622253418, accuracy: 0.6674107313156128\n",
      "Batch 35 loss: 0.8979098796844482, accuracy: 0.6688368320465088\n",
      "Batch 36 loss: 0.9270235300064087, accuracy: 0.6682854890823364\n",
      "Batch 37 loss: 0.8175477385520935, accuracy: 0.6689966917037964\n",
      "Batch 38 loss: 0.83603835105896, accuracy: 0.6698718070983887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39 loss: 0.7903127074241638, accuracy: 0.669921875\n",
      "Batch 40 loss: 0.7647988796234131, accuracy: 0.6713033318519592\n",
      "Batch 41 loss: 0.7835428714752197, accuracy: 0.671688973903656\n",
      "Batch 42 loss: 0.8625010251998901, accuracy: 0.6711482405662537\n",
      "Batch 43 loss: 0.9043259620666504, accuracy: 0.6708096861839294\n",
      "Batch 44 loss: 0.9100968837738037, accuracy: 0.6706597208976746\n",
      "Batch 45 loss: 1.027834177017212, accuracy: 0.6694973111152649\n",
      "Batch 46 loss: 0.826423704624176, accuracy: 0.6700465679168701\n",
      "Batch 47 loss: 0.8154729008674622, accuracy: 0.6707356572151184\n",
      "Batch 48 loss: 0.912106990814209, accuracy: 0.6696428656578064\n",
      "Batch 49 loss: 0.9601197838783264, accuracy: 0.6696875095367432\n",
      "Batch 50 loss: 0.7752769589424133, accuracy: 0.6703431606292725\n",
      "Batch 51 loss: 0.8132108449935913, accuracy: 0.670823335647583\n",
      "Batch 52 loss: 0.8187243938446045, accuracy: 0.6715801954269409\n",
      "Batch 53 loss: 0.8976377248764038, accuracy: 0.6717303395271301\n",
      "Batch 54 loss: 0.8828675746917725, accuracy: 0.6710227131843567\n",
      "Batch 55 loss: 0.7955318689346313, accuracy: 0.6720145344734192\n",
      "Batch 56 loss: 1.0252270698547363, accuracy: 0.671326756477356\n",
      "Batch 57 loss: 0.796891987323761, accuracy: 0.6713362336158752\n",
      "Batch 58 loss: 0.9192391037940979, accuracy: 0.671477735042572\n",
      "Batch 59 loss: 0.9685593843460083, accuracy: 0.6705729365348816\n",
      "Batch 60 loss: 0.8896033763885498, accuracy: 0.6695696711540222\n",
      "Batch 61 loss: 1.1533091068267822, accuracy: 0.66796875\n",
      "Batch 62 loss: 0.9035821557044983, accuracy: 0.668154776096344\n",
      "Batch 63 loss: 1.0525391101837158, accuracy: 0.6669921875\n",
      "Batch 64 loss: 0.8631783723831177, accuracy: 0.6668269038200378\n",
      "Batch 65 loss: 1.0575215816497803, accuracy: 0.6661931872367859\n",
      "Batch 66 loss: 0.7904604077339172, accuracy: 0.666627824306488\n",
      "Batch 67 loss: 0.840018630027771, accuracy: 0.6667049527168274\n",
      "Batch 68 loss: 0.898673415184021, accuracy: 0.6662137508392334\n",
      "Batch 69 loss: 0.8389652371406555, accuracy: 0.6662946343421936\n",
      "Batch 70 loss: 0.8432043194770813, accuracy: 0.6667033433914185\n",
      "Batch 71 loss: 0.9061173796653748, accuracy: 0.6665581464767456\n",
      "Batch 72 loss: 0.8377935886383057, accuracy: 0.6668450236320496\n",
      "Batch 73 loss: 0.7924361824989319, accuracy: 0.6669130325317383\n",
      "Batch 74 loss: 0.8329687714576721, accuracy: 0.6672916412353516\n",
      "Batch 75 loss: 0.8532220125198364, accuracy: 0.6673519611358643\n",
      "Batch 76 loss: 0.8000948429107666, accuracy: 0.6681209206581116\n",
      "Batch 77 loss: 0.9493601322174072, accuracy: 0.66796875\n",
      "Batch 78 loss: 0.8751260042190552, accuracy: 0.6684137582778931\n",
      "Batch 79 loss: 0.848846435546875, accuracy: 0.66943359375\n",
      "Batch 80 loss: 0.7658656239509583, accuracy: 0.6700424551963806\n",
      "Batch 81 loss: 0.8998510837554932, accuracy: 0.6698742508888245\n",
      "Batch 82 loss: 0.9101417064666748, accuracy: 0.669710099697113\n",
      "Batch 83 loss: 0.9177365899085999, accuracy: 0.6698288917541504\n",
      "Batch 84 loss: 0.832082986831665, accuracy: 0.6703125238418579\n",
      "Batch 85 loss: 0.7916461229324341, accuracy: 0.6711482405662537\n",
      "Batch 86 loss: 0.9117190837860107, accuracy: 0.6708872318267822\n",
      "Batch 87 loss: 0.9555094242095947, accuracy: 0.6712535619735718\n",
      "Batch 88 loss: 0.9179478287696838, accuracy: 0.6712605357170105\n",
      "Batch 89 loss: 0.7582176923751831, accuracy: 0.6716145873069763\n",
      "Batch 90 loss: 0.7678505182266235, accuracy: 0.6713598966598511\n",
      "Batch 91 loss: 0.8053534626960754, accuracy: 0.671535313129425\n",
      "Batch 92 loss: 0.8853533864021301, accuracy: 0.6710349321365356\n",
      "Batch 93 loss: 0.8413898944854736, accuracy: 0.6712931990623474\n",
      "Batch 94 loss: 0.7536447644233704, accuracy: 0.6716282963752747\n",
      "Batch 95 loss: 0.8152647614479065, accuracy: 0.6717122197151184\n",
      "Batch 96 loss: 0.8421675562858582, accuracy: 0.6719555258750916\n",
      "Batch 97 loss: 0.8868309259414673, accuracy: 0.6715561151504517\n",
      "Batch 98 loss: 1.0203708410263062, accuracy: 0.6714015007019043\n",
      "Batch 99 loss: 0.7424229979515076, accuracy: 0.6714843511581421\n",
      "Batch 100 loss: 0.8859168887138367, accuracy: 0.671565592288971\n",
      "Batch 101 loss: 0.7014217972755432, accuracy: 0.6721047759056091\n",
      "Batch 102 loss: 0.9957922697067261, accuracy: 0.6714957356452942\n",
      "Batch 103 loss: 0.8676971793174744, accuracy: 0.671875\n",
      "Batch 104 loss: 0.8222391605377197, accuracy: 0.672247052192688\n",
      "Batch 105 loss: 0.9381709694862366, accuracy: 0.671875\n",
      "Batch 106 loss: 0.7537983655929565, accuracy: 0.6726781725883484\n",
      "Batch 107 loss: 1.0650895833969116, accuracy: 0.6723090410232544\n",
      "Batch 108 loss: 0.816461980342865, accuracy: 0.6726633906364441\n",
      "Batch 109 loss: 1.0169121026992798, accuracy: 0.6722301244735718\n",
      "Batch 110 loss: 1.042015790939331, accuracy: 0.6715934872627258\n",
      "Batch 111 loss: 0.8494551181793213, accuracy: 0.6715262532234192\n",
      "Batch 112 loss: 1.017944097518921, accuracy: 0.6715292930603027\n",
      "Batch 113 loss: 0.9512984752655029, accuracy: 0.6711896657943726\n",
      "Batch 114 loss: 0.6986232399940491, accuracy: 0.671535313129425\n",
      "Batch 115 loss: 0.938897967338562, accuracy: 0.671538233757019\n",
      "Batch 116 loss: 0.9149355888366699, accuracy: 0.671340823173523\n",
      "Batch 117 loss: 0.9722073078155518, accuracy: 0.6708818674087524\n",
      "Batch 118 loss: 0.8714902400970459, accuracy: 0.6704963445663452\n",
      "Batch 119 loss: 0.7929550409317017, accuracy: 0.6710286736488342\n",
      "Batch 120 loss: 1.0436815023422241, accuracy: 0.6707128286361694\n",
      "Batch 121 loss: 0.8624130487442017, accuracy: 0.6709784865379333\n",
      "Batch 122 loss: 0.8096597194671631, accuracy: 0.6714938879013062\n",
      "Batch 123 loss: 0.7660602331161499, accuracy: 0.6720010042190552\n",
      "Batch 124 loss: 0.9484989047050476, accuracy: 0.6720625162124634\n",
      "Batch 125 loss: 0.7914911508560181, accuracy: 0.6723710298538208\n",
      "Batch 126 loss: 0.9511252641677856, accuracy: 0.6719365119934082\n",
      "Batch 127 loss: 0.9190049767494202, accuracy: 0.67193603515625\n",
      "Batch 128 loss: 0.8215846419334412, accuracy: 0.6720566749572754\n",
      "Batch 129 loss: 0.9238629341125488, accuracy: 0.6719350814819336\n",
      "Batch 130 loss: 0.8361220955848694, accuracy: 0.6719346642494202\n",
      "Batch 131 loss: 0.838683009147644, accuracy: 0.671875\n",
      "Batch 132 loss: 0.7953804731369019, accuracy: 0.6719337701797485\n",
      "Batch 133 loss: 0.8385605812072754, accuracy: 0.6723414063453674\n",
      "Batch 134 loss: 0.8632712364196777, accuracy: 0.6723958253860474\n",
      "Batch 135 loss: 0.8365371227264404, accuracy: 0.6721047759056091\n",
      "Batch 136 loss: 0.8769708275794983, accuracy: 0.6719890236854553\n",
      "Batch 137 loss: 0.8602115511894226, accuracy: 0.672214686870575\n",
      "Batch 138 loss: 0.8953626751899719, accuracy: 0.6719874143600464\n",
      "Batch 139 loss: 0.8383298516273499, accuracy: 0.6719865798950195\n",
      "Batch 140 loss: 0.8511272668838501, accuracy: 0.6722628474235535\n",
      "Batch 141 loss: 0.8730242848396301, accuracy: 0.6723151206970215\n",
      "Batch 142 loss: 0.9971716403961182, accuracy: 0.671875\n",
      "Batch 143 loss: 1.0333482027053833, accuracy: 0.6714409589767456\n",
      "Batch 144 loss: 0.9060788750648499, accuracy: 0.6714439392089844\n",
      "Batch 145 loss: 1.0247005224227905, accuracy: 0.6711793541908264\n",
      "Batch 146 loss: 0.8321343064308167, accuracy: 0.671502947807312\n",
      "Batch 147 loss: 0.9526193141937256, accuracy: 0.6708720326423645\n",
      "Batch 148 loss: 1.1212761402130127, accuracy: 0.6702495813369751\n",
      "Batch 149 loss: 0.8582702875137329, accuracy: 0.6703125238418579\n",
      "Batch 150 loss: 0.8881345987319946, accuracy: 0.6701676249504089\n",
      "Batch 151 loss: 0.9029215574264526, accuracy: 0.6700246930122375\n",
      "Batch 152 loss: 0.6642566323280334, accuracy: 0.6707005500793457\n",
      "Batch 153 loss: 0.9138625264167786, accuracy: 0.6704038381576538\n",
      "Batch 154 loss: 0.9041936993598938, accuracy: 0.6703628897666931\n",
      "Batch 155 loss: 0.9044808149337769, accuracy: 0.6701222062110901\n",
      "Batch 156 loss: 0.8706419467926025, accuracy: 0.6699343323707581\n",
      "Batch 157 loss: 1.0320334434509277, accuracy: 0.6694521307945251\n",
      "Batch 158 loss: 0.9073706865310669, accuracy: 0.669369101524353\n",
      "Batch 159 loss: 0.8202730417251587, accuracy: 0.66943359375\n",
      "Batch 160 loss: 0.9124231934547424, accuracy: 0.6692061424255371\n",
      "Batch 161 loss: 0.968899130821228, accuracy: 0.6690779328346252\n",
      "Batch 162 loss: 0.8419585824012756, accuracy: 0.6692867875099182\n",
      "Batch 163 loss: 0.8751798868179321, accuracy: 0.6691120266914368\n",
      "Batch 164 loss: 0.92259281873703, accuracy: 0.6689867377281189\n",
      "Batch 165 loss: 0.7467228174209595, accuracy: 0.6693806648254395\n",
      "Batch 166 loss: 0.9365098476409912, accuracy: 0.669442355632782\n",
      "Batch 167 loss: 0.9721806049346924, accuracy: 0.6694103479385376\n",
      "Batch 168 loss: 0.9279384016990662, accuracy: 0.6691938042640686\n",
      "Batch 169 loss: 0.8770856261253357, accuracy: 0.6693474054336548\n",
      "Batch 170 loss: 0.781796395778656, accuracy: 0.6699561476707458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 171 loss: 0.935082197189331, accuracy: 0.6698764562606812\n",
      "Batch 172 loss: 0.9683468341827393, accuracy: 0.6699331402778625\n",
      "Batch 173 loss: 0.952558159828186, accuracy: 0.6697198152542114\n",
      "Batch 174 loss: 0.9373058676719666, accuracy: 0.6694642901420593\n",
      "Batch 175 loss: 1.0801035165786743, accuracy: 0.6691228747367859\n",
      "Batch 176 loss: 0.8393555283546448, accuracy: 0.6690059900283813\n",
      "Batch 177 loss: 0.872874915599823, accuracy: 0.6691538095474243\n",
      "Batch 178 loss: 0.9052532315254211, accuracy: 0.6690816879272461\n",
      "Batch 179 loss: 0.9924299120903015, accuracy: 0.668749988079071\n",
      "Batch 180 loss: 0.8516857624053955, accuracy: 0.6688967347145081\n",
      "Batch 181 loss: 0.9215230941772461, accuracy: 0.6686984896659851\n",
      "Batch 182 loss: 0.8008217215538025, accuracy: 0.6686304807662964\n",
      "Batch 183 loss: 0.790514349937439, accuracy: 0.668817937374115\n",
      "Batch 184 loss: 0.9257625341415405, accuracy: 0.6688767075538635\n",
      "Batch 185 loss: 0.8233249187469482, accuracy: 0.6692288517951965\n",
      "Batch 186 loss: 0.8885190486907959, accuracy: 0.6692847609519958\n",
      "Batch 187 loss: 1.2373300790786743, accuracy: 0.668799877166748\n",
      "Batch 188 loss: 0.7999264001846313, accuracy: 0.6690641641616821\n",
      "Batch 189 loss: 0.897955596446991, accuracy: 0.6691612005233765\n",
      "Batch 190 loss: 0.9320956468582153, accuracy: 0.6692162752151489\n",
      "Batch 191 loss: 0.7913217544555664, accuracy: 0.6692301630973816\n",
      "Batch 192 loss: 0.9991366267204285, accuracy: 0.668798565864563\n",
      "Batch 193 loss: 1.0572882890701294, accuracy: 0.6687741875648499\n",
      "Batch 194 loss: 1.0109081268310547, accuracy: 0.6685897707939148\n",
      "Batch 195 loss: 0.7833704948425293, accuracy: 0.6688855290412903\n",
      "Batch 196 loss: 0.9368851780891418, accuracy: 0.6687420606613159\n",
      "Batch 197 loss: 0.9195681810379028, accuracy: 0.6685606241226196\n",
      "Batch 198 loss: 0.8855128288269043, accuracy: 0.668577253818512\n",
      "Batch 199 loss: 0.8400974273681641, accuracy: 0.6687890887260437\n",
      "Batch 200 loss: 0.7738760709762573, accuracy: 0.6688432693481445\n",
      "Batch 201 loss: 0.9173387885093689, accuracy: 0.6688582897186279\n",
      "Batch 202 loss: 0.8248552083969116, accuracy: 0.668796181678772\n",
      "Batch 203 loss: 0.8952992558479309, accuracy: 0.6687729954719543\n",
      "Batch 204 loss: 0.863609790802002, accuracy: 0.6688262224197388\n",
      "Batch 205 loss: 0.9017675518989563, accuracy: 0.6689168810844421\n",
      "Batch 206 loss: 0.823361337184906, accuracy: 0.6690821051597595\n",
      "Batch 207 loss: 0.7606698870658875, accuracy: 0.6690955758094788\n",
      "Batch 208 loss: 0.9286169409751892, accuracy: 0.6688845753669739\n",
      "Batch 209 loss: 0.8535273671150208, accuracy: 0.668936014175415\n",
      "Batch 210 loss: 0.9156662225723267, accuracy: 0.6688388586044312\n",
      "Batch 211 loss: 0.9128543734550476, accuracy: 0.6688900589942932\n",
      "Batch 212 loss: 0.9179303050041199, accuracy: 0.6688306927680969\n",
      "Batch 213 loss: 0.7555874586105347, accuracy: 0.6690274477005005\n",
      "Batch 214 loss: 0.8635267615318298, accuracy: 0.6691133975982666\n",
      "Batch 215 loss: 0.9747769832611084, accuracy: 0.6689453125\n",
      "Batch 216 loss: 0.9644094705581665, accuracy: 0.6687787771224976\n",
      "Batch 217 loss: 0.9575661420822144, accuracy: 0.6688288450241089\n",
      "Batch 218 loss: 0.8634384274482727, accuracy: 0.6689497828483582\n",
      "Batch 219 loss: 0.9110302329063416, accuracy: 0.6688210368156433\n",
      "Batch 220 loss: 0.8530054688453674, accuracy: 0.6686934232711792\n",
      "Batch 221 loss: 0.8376597762107849, accuracy: 0.6687077879905701\n",
      "Batch 222 loss: 0.9808297157287598, accuracy: 0.6685117483139038\n",
      "Batch 223 loss: 0.886879026889801, accuracy: 0.6686314344406128\n",
      "Batch 224 loss: 0.8114321827888489, accuracy: 0.6686753034591675\n",
      "Training epoch: 21, train accuracy: 66.8675308227539, train loss: 0.8872566896014743, valid accuracy: 63.02591323852539, valid loss: 1.027629095932533 \n",
      "Batch 0 loss: 0.8919683694839478, accuracy: 0.6796875\n",
      "Batch 1 loss: 0.7904319763183594, accuracy: 0.67578125\n",
      "Batch 2 loss: 0.6872265338897705, accuracy: 0.7005208134651184\n",
      "Batch 3 loss: 0.7834973931312561, accuracy: 0.6875\n",
      "Batch 4 loss: 0.8917547464370728, accuracy: 0.682812511920929\n",
      "Batch 5 loss: 0.7656592726707458, accuracy: 0.6861979365348816\n",
      "Batch 6 loss: 0.8158577680587769, accuracy: 0.6852678656578064\n",
      "Batch 7 loss: 0.814098596572876, accuracy: 0.6875\n",
      "Batch 8 loss: 0.8351495862007141, accuracy: 0.6944444179534912\n",
      "Batch 9 loss: 0.8857348561286926, accuracy: 0.694531261920929\n",
      "Batch 10 loss: 0.7879691123962402, accuracy: 0.7002840638160706\n",
      "Batch 11 loss: 0.8000879287719727, accuracy: 0.69921875\n",
      "Batch 12 loss: 0.9229192733764648, accuracy: 0.6941105723381042\n",
      "Batch 13 loss: 0.7539451718330383, accuracy: 0.6958705186843872\n",
      "Batch 14 loss: 0.9163181781768799, accuracy: 0.6932291388511658\n",
      "Batch 15 loss: 0.9119722247123718, accuracy: 0.693359375\n",
      "Batch 16 loss: 0.8096694350242615, accuracy: 0.6925551295280457\n",
      "Batch 17 loss: 0.8117083311080933, accuracy: 0.6905381679534912\n",
      "Batch 18 loss: 0.8169914484024048, accuracy: 0.6920230388641357\n",
      "Batch 19 loss: 0.8098613619804382, accuracy: 0.693359375\n",
      "Batch 20 loss: 0.7831878066062927, accuracy: 0.6945684552192688\n",
      "Batch 21 loss: 0.9315457940101624, accuracy: 0.6910511255264282\n",
      "Batch 22 loss: 0.7651354074478149, accuracy: 0.69089674949646\n",
      "Batch 23 loss: 0.8183029890060425, accuracy: 0.69140625\n",
      "Batch 24 loss: 0.8395457863807678, accuracy: 0.6903125047683716\n",
      "Batch 25 loss: 0.8778461217880249, accuracy: 0.690504789352417\n",
      "Batch 26 loss: 0.7647371888160706, accuracy: 0.6912615895271301\n",
      "Batch 27 loss: 0.8739538192749023, accuracy: 0.6900111436843872\n",
      "Batch 28 loss: 0.6783425807952881, accuracy: 0.6931573152542114\n",
      "Batch 29 loss: 0.998260498046875, accuracy: 0.6898437738418579\n",
      "Batch 30 loss: 0.8622276186943054, accuracy: 0.6885080933570862\n",
      "Batch 31 loss: 1.0113633871078491, accuracy: 0.686279296875\n",
      "Batch 32 loss: 0.9874827861785889, accuracy: 0.6851325631141663\n",
      "Batch 33 loss: 0.9358152151107788, accuracy: 0.68359375\n",
      "Batch 34 loss: 0.9033774137496948, accuracy: 0.6810267567634583\n",
      "Batch 35 loss: 0.9348622560501099, accuracy: 0.6809895634651184\n",
      "Batch 36 loss: 1.0689092874526978, accuracy: 0.6788429021835327\n",
      "Batch 37 loss: 0.7763578295707703, accuracy: 0.6782483458518982\n",
      "Batch 38 loss: 0.8169214129447937, accuracy: 0.6786859035491943\n",
      "Batch 39 loss: 1.059233546257019, accuracy: 0.677929699420929\n",
      "Batch 40 loss: 0.8997052907943726, accuracy: 0.6774008870124817\n",
      "Batch 41 loss: 0.9521172046661377, accuracy: 0.67578125\n",
      "Batch 42 loss: 0.8872701525688171, accuracy: 0.6755087375640869\n",
      "Batch 43 loss: 0.9695903062820435, accuracy: 0.6750710010528564\n",
      "Batch 44 loss: 0.7877217531204224, accuracy: 0.675868034362793\n",
      "Batch 45 loss: 0.9593338370323181, accuracy: 0.674932062625885\n",
      "Batch 46 loss: 0.8843243718147278, accuracy: 0.674035906791687\n",
      "Batch 47 loss: 0.7422102689743042, accuracy: 0.6751301884651184\n",
      "Batch 48 loss: 0.8549599647521973, accuracy: 0.6755421161651611\n",
      "Batch 49 loss: 0.9219657778739929, accuracy: 0.6753125190734863\n",
      "Batch 50 loss: 1.004894733428955, accuracy: 0.6749387383460999\n",
      "Batch 51 loss: 0.7952389121055603, accuracy: 0.6750300526618958\n",
      "Batch 52 loss: 0.8622841238975525, accuracy: 0.6758549809455872\n",
      "Batch 53 loss: 0.7959712743759155, accuracy: 0.6767939925193787\n",
      "Batch 54 loss: 0.811815619468689, accuracy: 0.6775568127632141\n",
      "Batch 55 loss: 0.8727763891220093, accuracy: 0.6774553656578064\n",
      "Batch 56 loss: 0.9009690880775452, accuracy: 0.6770833134651184\n",
      "Batch 57 loss: 0.8563252687454224, accuracy: 0.6772629022598267\n",
      "Batch 58 loss: 0.7849541306495667, accuracy: 0.6773040294647217\n",
      "Batch 59 loss: 0.7975526452064514, accuracy: 0.6774739623069763\n",
      "Batch 60 loss: 0.7763569355010986, accuracy: 0.6778944730758667\n",
      "Batch 61 loss: 0.9594072699546814, accuracy: 0.6775453686714172\n",
      "Batch 62 loss: 0.9305154085159302, accuracy: 0.6770833134651184\n",
      "Batch 63 loss: 0.7891978025436401, accuracy: 0.6773681640625\n",
      "Batch 64 loss: 0.785219669342041, accuracy: 0.6774038672447205\n",
      "Batch 65 loss: 0.8352548480033875, accuracy: 0.6774384379386902\n",
      "Batch 66 loss: 0.8989234566688538, accuracy: 0.6779384613037109\n",
      "Batch 67 loss: 0.850801408290863, accuracy: 0.6780790686607361\n",
      "Batch 68 loss: 0.8012086153030396, accuracy: 0.6782155632972717\n",
      "Batch 69 loss: 0.9640284180641174, accuracy: 0.6776785850524902\n",
      "Batch 70 loss: 0.7761719226837158, accuracy: 0.6782570481300354\n",
      "Batch 71 loss: 0.6561477780342102, accuracy: 0.6790364384651184\n",
      "Batch 72 loss: 0.9479422569274902, accuracy: 0.6777611374855042\n",
      "Batch 73 loss: 1.041041374206543, accuracy: 0.6771537065505981\n",
      "Batch 74 loss: 0.8734878301620483, accuracy: 0.6770833134651184\n",
      "Batch 75 loss: 0.8195646405220032, accuracy: 0.6770148277282715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 76 loss: 0.8555547595024109, accuracy: 0.6765422224998474\n",
      "Batch 77 loss: 1.0246176719665527, accuracy: 0.6761819124221802\n",
      "Batch 78 loss: 0.7899244427680969, accuracy: 0.6763251423835754\n",
      "Batch 79 loss: 0.6702028512954712, accuracy: 0.6771484613418579\n",
      "Batch 80 loss: 0.8987442851066589, accuracy: 0.6768904328346252\n",
      "Batch 81 loss: 0.8003675937652588, accuracy: 0.6768292784690857\n",
      "Batch 82 loss: 0.8675654530525208, accuracy: 0.6763930916786194\n",
      "Batch 83 loss: 0.8413962125778198, accuracy: 0.6764323115348816\n",
      "Batch 84 loss: 1.0381356477737427, accuracy: 0.6758272051811218\n",
      "Batch 85 loss: 0.7988656163215637, accuracy: 0.67578125\n",
      "Batch 86 loss: 0.9102158546447754, accuracy: 0.6757363677024841\n",
      "Batch 87 loss: 0.7770814299583435, accuracy: 0.6759588122367859\n",
      "Batch 88 loss: 0.8163395524024963, accuracy: 0.676000714302063\n",
      "Batch 89 loss: 0.7857908606529236, accuracy: 0.6763020753860474\n",
      "Batch 90 loss: 0.7971621155738831, accuracy: 0.676682710647583\n",
      "Batch 91 loss: 0.9912627935409546, accuracy: 0.67629075050354\n",
      "Batch 92 loss: 0.8159515857696533, accuracy: 0.6766632795333862\n",
      "Batch 93 loss: 0.857328474521637, accuracy: 0.6767786145210266\n",
      "Batch 94 loss: 0.9498633146286011, accuracy: 0.6763157844543457\n",
      "Batch 95 loss: 0.8292758464813232, accuracy: 0.676025390625\n",
      "Batch 96 loss: 0.9011096954345703, accuracy: 0.6760631203651428\n",
      "Batch 97 loss: 0.9643651247024536, accuracy: 0.67578125\n",
      "Batch 98 loss: 0.9165096879005432, accuracy: 0.6756628751754761\n",
      "Batch 99 loss: 0.8516588807106018, accuracy: 0.6760937571525574\n",
      "Batch 100 loss: 0.9291156530380249, accuracy: 0.6756651997566223\n",
      "Batch 101 loss: 0.7733820080757141, accuracy: 0.6757046580314636\n",
      "Batch 102 loss: 1.0525151491165161, accuracy: 0.6750606894493103\n",
      "Batch 103 loss: 0.9320212602615356, accuracy: 0.6751051545143127\n",
      "Batch 104 loss: 0.8789023160934448, accuracy: 0.6745535731315613\n",
      "Batch 105 loss: 0.9318169355392456, accuracy: 0.6743071675300598\n",
      "Batch 106 loss: 0.8024106621742249, accuracy: 0.67450350522995\n",
      "Batch 107 loss: 0.8771744966506958, accuracy: 0.6740451455116272\n",
      "Batch 108 loss: 1.0776323080062866, accuracy: 0.6733801364898682\n",
      "Batch 109 loss: 1.0114266872406006, accuracy: 0.6728693246841431\n",
      "Batch 110 loss: 0.795478343963623, accuracy: 0.6728603839874268\n",
      "Batch 111 loss: 0.8565414547920227, accuracy: 0.6732003092765808\n",
      "Batch 112 loss: 0.7144225835800171, accuracy: 0.6736034154891968\n",
      "Batch 113 loss: 0.893532931804657, accuracy: 0.6735882759094238\n",
      "Batch 114 loss: 0.9701422452926636, accuracy: 0.673301637172699\n",
      "Batch 115 loss: 0.889251172542572, accuracy: 0.6732893586158752\n",
      "Batch 116 loss: 1.0066545009613037, accuracy: 0.6726762652397156\n",
      "Batch 117 loss: 0.8827019333839417, accuracy: 0.6726033091545105\n",
      "Batch 118 loss: 0.8156605362892151, accuracy: 0.6725314855575562\n",
      "Batch 119 loss: 0.8565106391906738, accuracy: 0.6724609136581421\n",
      "Batch 120 loss: 0.9677352905273438, accuracy: 0.6718104481697083\n",
      "Batch 121 loss: 0.995411217212677, accuracy: 0.6715548038482666\n",
      "Batch 122 loss: 0.8575760722160339, accuracy: 0.6716209053993225\n",
      "Batch 123 loss: 0.7978931665420532, accuracy: 0.6717489957809448\n",
      "Batch 124 loss: 1.0073539018630981, accuracy: 0.6713749766349792\n",
      "Batch 125 loss: 0.83286452293396, accuracy: 0.672061026096344\n",
      "Batch 126 loss: 0.939429759979248, accuracy: 0.6719365119934082\n",
      "Batch 127 loss: 0.9495271444320679, accuracy: 0.67156982421875\n",
      "Batch 128 loss: 0.6823874115943909, accuracy: 0.6719961166381836\n",
      "Batch 129 loss: 0.8497850894927979, accuracy: 0.6722355484962463\n",
      "Batch 130 loss: 0.9007704854011536, accuracy: 0.6721732020378113\n",
      "Batch 131 loss: 0.8917138576507568, accuracy: 0.6722301244735718\n",
      "Batch 132 loss: 0.9107146263122559, accuracy: 0.671875\n",
      "Batch 133 loss: 0.8807675838470459, accuracy: 0.671875\n",
      "Batch 134 loss: 1.082395076751709, accuracy: 0.6714120507240295\n",
      "Batch 135 loss: 0.7960020899772644, accuracy: 0.671760082244873\n",
      "Batch 136 loss: 0.9273459911346436, accuracy: 0.6715898513793945\n",
      "Batch 137 loss: 0.8810215592384338, accuracy: 0.6717617511749268\n",
      "Batch 138 loss: 0.840593159198761, accuracy: 0.6717625856399536\n",
      "Batch 139 loss: 1.0203970670700073, accuracy: 0.6712611317634583\n",
      "Batch 140 loss: 0.9239920973777771, accuracy: 0.6711546778678894\n",
      "Batch 141 loss: 0.862417995929718, accuracy: 0.6713248491287231\n",
      "Batch 142 loss: 0.9978790879249573, accuracy: 0.6711647510528564\n",
      "Batch 143 loss: 0.9120303988456726, accuracy: 0.6709526777267456\n",
      "Batch 144 loss: 0.8947815895080566, accuracy: 0.6710129380226135\n",
      "Batch 145 loss: 0.8389545679092407, accuracy: 0.6711793541908264\n",
      "Batch 146 loss: 0.8413506746292114, accuracy: 0.6711841225624084\n",
      "Batch 147 loss: 0.9402544498443604, accuracy: 0.6709775924682617\n",
      "Batch 148 loss: 1.0475157499313354, accuracy: 0.6707214713096619\n",
      "Batch 149 loss: 0.8636826872825623, accuracy: 0.670885443687439\n",
      "Batch 150 loss: 0.8758169412612915, accuracy: 0.6708402037620544\n",
      "Batch 151 loss: 0.9974196553230286, accuracy: 0.6706414222717285\n",
      "Batch 152 loss: 1.0430376529693604, accuracy: 0.6702409982681274\n",
      "Batch 153 loss: 0.918549656867981, accuracy: 0.6702516078948975\n",
      "Batch 154 loss: 0.9657751321792603, accuracy: 0.6699596643447876\n",
      "Batch 155 loss: 0.770275354385376, accuracy: 0.6700721383094788\n",
      "Batch 156 loss: 0.8738375902175903, accuracy: 0.6701831221580505\n",
      "Batch 157 loss: 0.8616952300071716, accuracy: 0.6702432632446289\n",
      "Batch 158 loss: 0.8354316353797913, accuracy: 0.6703518033027649\n",
      "Batch 159 loss: 0.9540497064590454, accuracy: 0.6703125238418579\n",
      "Batch 160 loss: 0.8866502046585083, accuracy: 0.6703707575798035\n",
      "Batch 161 loss: 0.9742931723594666, accuracy: 0.6698495149612427\n",
      "Batch 162 loss: 0.8922091126441956, accuracy: 0.6697661280632019\n",
      "Batch 163 loss: 1.0363444089889526, accuracy: 0.6695883870124817\n",
      "Batch 164 loss: 0.7719918489456177, accuracy: 0.6697916388511658\n",
      "Batch 165 loss: 0.8405369520187378, accuracy: 0.6698513031005859\n",
      "Batch 166 loss: 1.0621044635772705, accuracy: 0.6695359349250793\n",
      "Batch 167 loss: 0.880881130695343, accuracy: 0.669735848903656\n",
      "Batch 168 loss: 0.9831631779670715, accuracy: 0.6696560382843018\n",
      "Batch 169 loss: 0.7625946998596191, accuracy: 0.6697150468826294\n",
      "Batch 170 loss: 0.7373684644699097, accuracy: 0.6698190569877625\n",
      "Batch 171 loss: 0.9835219979286194, accuracy: 0.6695131063461304\n",
      "Batch 172 loss: 0.8046741485595703, accuracy: 0.6697976589202881\n",
      "Batch 173 loss: 0.9703488349914551, accuracy: 0.6694055199623108\n",
      "Batch 174 loss: 0.8804376721382141, accuracy: 0.6695089340209961\n",
      "Batch 175 loss: 0.9113041162490845, accuracy: 0.6695223450660706\n",
      "Batch 176 loss: 0.9273698925971985, accuracy: 0.6694032549858093\n",
      "Batch 177 loss: 0.8899601697921753, accuracy: 0.6693293452262878\n",
      "Batch 178 loss: 0.9570419192314148, accuracy: 0.6692562699317932\n",
      "Batch 179 loss: 0.8809949159622192, accuracy: 0.6693576574325562\n",
      "Batch 180 loss: 0.8167433142662048, accuracy: 0.6693715453147888\n",
      "Batch 181 loss: 0.8128121495246887, accuracy: 0.6695570349693298\n",
      "Batch 182 loss: 0.9347209334373474, accuracy: 0.6692708134651184\n",
      "Batch 183 loss: 0.7454285025596619, accuracy: 0.66945481300354\n",
      "Batch 184 loss: 0.8289832472801208, accuracy: 0.6696368455886841\n",
      "Batch 185 loss: 0.8064428567886353, accuracy: 0.6696908473968506\n",
      "Batch 186 loss: 0.6865418553352356, accuracy: 0.6699114441871643\n",
      "Batch 187 loss: 0.8401263356208801, accuracy: 0.6700881123542786\n",
      "Batch 188 loss: 0.8901411890983582, accuracy: 0.6700975298881531\n",
      "Batch 189 loss: 0.818047821521759, accuracy: 0.6700657606124878\n",
      "Batch 190 loss: 1.0580182075500488, accuracy: 0.6698707342147827\n",
      "Batch 191 loss: 0.8847638368606567, accuracy: 0.6701253056526184\n",
      "Batch 192 loss: 0.8620142936706543, accuracy: 0.6701748967170715\n",
      "Batch 193 loss: 0.8869421482086182, accuracy: 0.6701030731201172\n",
      "Batch 194 loss: 0.7828584313392639, accuracy: 0.670352578163147\n",
      "Batch 195 loss: 0.9955275058746338, accuracy: 0.6702008843421936\n",
      "Batch 196 loss: 0.9637653231620789, accuracy: 0.6700507402420044\n",
      "Batch 197 loss: 0.8280037641525269, accuracy: 0.6700205206871033\n",
      "Batch 198 loss: 0.9463269710540771, accuracy: 0.6699120402336121\n",
      "Batch 199 loss: 0.9077659845352173, accuracy: 0.6698437333106995\n",
      "Batch 200 loss: 0.727485716342926, accuracy: 0.670048177242279\n",
      "Batch 201 loss: 0.8575239777565002, accuracy: 0.6700185537338257\n",
      "Batch 202 loss: 0.9407316446304321, accuracy: 0.6698352694511414\n",
      "Batch 203 loss: 0.9552463293075562, accuracy: 0.669806957244873\n",
      "Batch 204 loss: 0.9162058234214783, accuracy: 0.6698551774024963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 205 loss: 0.8506073355674744, accuracy: 0.6699408292770386\n",
      "Batch 206 loss: 0.7833945155143738, accuracy: 0.6703276038169861\n",
      "Batch 207 loss: 0.8967690467834473, accuracy: 0.670335054397583\n",
      "Batch 208 loss: 0.7161223292350769, accuracy: 0.6705666780471802\n",
      "Batch 209 loss: 0.84112149477005, accuracy: 0.6704984903335571\n",
      "Batch 210 loss: 0.8351694345474243, accuracy: 0.6707271933555603\n",
      "Batch 211 loss: 0.831487774848938, accuracy: 0.6709905862808228\n",
      "Batch 212 loss: 0.7755981087684631, accuracy: 0.6711414456367493\n",
      "Batch 213 loss: 1.0678646564483643, accuracy: 0.6709988117218018\n",
      "Batch 214 loss: 0.8279522657394409, accuracy: 0.6710392236709595\n",
      "Batch 215 loss: 0.8274316787719727, accuracy: 0.6710792779922485\n",
      "Batch 216 loss: 0.9704890251159668, accuracy: 0.6709389686584473\n",
      "Batch 217 loss: 0.9214871525764465, accuracy: 0.6706206798553467\n",
      "Batch 218 loss: 0.9648658037185669, accuracy: 0.6704837083816528\n",
      "Batch 219 loss: 0.9602306485176086, accuracy: 0.6702059507369995\n",
      "Batch 220 loss: 0.9194573760032654, accuracy: 0.6701074838638306\n",
      "Batch 221 loss: 0.7225707769393921, accuracy: 0.6703265905380249\n",
      "Batch 222 loss: 0.7856678366661072, accuracy: 0.6705437302589417\n",
      "Batch 223 loss: 0.8116727471351624, accuracy: 0.6706194281578064\n",
      "Batch 224 loss: 0.9648948907852173, accuracy: 0.6705911159515381\n",
      "Training epoch: 22, train accuracy: 67.05911254882812, train loss: 0.876294719643063, valid accuracy: 62.71942138671875, valid loss: 1.0636292058846046 \n",
      "Batch 0 loss: 0.8924061059951782, accuracy: 0.59375\n",
      "Batch 1 loss: 0.8412488698959351, accuracy: 0.609375\n",
      "Batch 2 loss: 0.863545298576355, accuracy: 0.6354166865348816\n",
      "Batch 3 loss: 0.8607607483863831, accuracy: 0.63671875\n",
      "Batch 4 loss: 0.7078319787979126, accuracy: 0.659375011920929\n",
      "Batch 5 loss: 0.8905484676361084, accuracy: 0.6588541865348816\n",
      "Batch 6 loss: 0.7770212888717651, accuracy: 0.6618303656578064\n",
      "Batch 7 loss: 0.6557304859161377, accuracy: 0.6708984375\n",
      "Batch 8 loss: 0.7387706637382507, accuracy: 0.6762152910232544\n",
      "Batch 9 loss: 1.0281776189804077, accuracy: 0.671093761920929\n",
      "Batch 10 loss: 0.8199232220649719, accuracy: 0.6761363744735718\n",
      "Batch 11 loss: 0.7058296203613281, accuracy: 0.6829426884651184\n",
      "Batch 12 loss: 1.157842755317688, accuracy: 0.6796875\n",
      "Batch 13 loss: 0.8171322345733643, accuracy: 0.6819196343421936\n",
      "Batch 14 loss: 0.9201570153236389, accuracy: 0.6786458492279053\n",
      "Batch 15 loss: 0.8589906096458435, accuracy: 0.6787109375\n",
      "Batch 16 loss: 0.9460027813911438, accuracy: 0.6773896813392639\n",
      "Batch 17 loss: 0.76347815990448, accuracy: 0.6792534589767456\n",
      "Batch 18 loss: 0.9337579607963562, accuracy: 0.6780427694320679\n",
      "Batch 19 loss: 0.8671371936798096, accuracy: 0.676953136920929\n",
      "Batch 20 loss: 0.7531347274780273, accuracy: 0.6781994104385376\n",
      "Batch 21 loss: 0.8672013878822327, accuracy: 0.6772016882896423\n",
      "Batch 22 loss: 0.8723829388618469, accuracy: 0.6759510636329651\n",
      "Batch 23 loss: 0.7586798071861267, accuracy: 0.6780598759651184\n",
      "Batch 24 loss: 0.9011918306350708, accuracy: 0.6756250262260437\n",
      "Batch 25 loss: 0.7777867317199707, accuracy: 0.6778846383094788\n",
      "Batch 26 loss: 0.969065248966217, accuracy: 0.6753472089767456\n",
      "Batch 27 loss: 0.8752241134643555, accuracy: 0.6760602593421936\n",
      "Batch 28 loss: 0.9119625091552734, accuracy: 0.6764547228813171\n",
      "Batch 29 loss: 0.8313490748405457, accuracy: 0.6776041388511658\n",
      "Batch 30 loss: 0.901684582233429, accuracy: 0.6766632795333862\n",
      "Batch 31 loss: 0.9358304142951965, accuracy: 0.67578125\n",
      "Batch 32 loss: 0.7421020865440369, accuracy: 0.6780303120613098\n",
      "Batch 33 loss: 0.7711669206619263, accuracy: 0.6810661554336548\n",
      "Batch 34 loss: 0.8796230554580688, accuracy: 0.6801339387893677\n",
      "Batch 35 loss: 0.8983446359634399, accuracy: 0.6796875\n",
      "Batch 36 loss: 0.8571205139160156, accuracy: 0.6803209185600281\n",
      "Batch 37 loss: 0.8028032183647156, accuracy: 0.6803042888641357\n",
      "Batch 38 loss: 0.9670288562774658, accuracy: 0.6782852411270142\n",
      "Batch 39 loss: 0.9100926518440247, accuracy: 0.6781250238418579\n",
      "Batch 40 loss: 0.8402091860771179, accuracy: 0.6779725551605225\n",
      "Batch 41 loss: 0.7823793888092041, accuracy: 0.6781994104385376\n",
      "Batch 42 loss: 0.8555867671966553, accuracy: 0.6778706312179565\n",
      "Batch 43 loss: 0.8048736453056335, accuracy: 0.6787996888160706\n",
      "Batch 44 loss: 0.8409586548805237, accuracy: 0.6784722208976746\n",
      "Batch 45 loss: 0.7392327785491943, accuracy: 0.6798573136329651\n",
      "Batch 46 loss: 0.8245839476585388, accuracy: 0.6798537373542786\n",
      "Batch 47 loss: 0.7073946595191956, accuracy: 0.681640625\n",
      "Batch 48 loss: 0.9122351408004761, accuracy: 0.6814413070678711\n",
      "Batch 49 loss: 0.8147146105766296, accuracy: 0.6817187666893005\n",
      "Batch 50 loss: 0.7193146347999573, accuracy: 0.6825980544090271\n",
      "Batch 51 loss: 0.9157745242118835, accuracy: 0.6822415590286255\n",
      "Batch 52 loss: 0.8984436988830566, accuracy: 0.682193398475647\n",
      "Batch 53 loss: 0.8093001842498779, accuracy: 0.6818576455116272\n",
      "Batch 54 loss: 0.8320931196212769, accuracy: 0.6813920736312866\n",
      "Batch 55 loss: 0.9758028388023376, accuracy: 0.6813616156578064\n",
      "Batch 56 loss: 0.8638030886650085, accuracy: 0.6814693212509155\n",
      "Batch 57 loss: 0.8867003917694092, accuracy: 0.6815732717514038\n",
      "Batch 58 loss: 0.9711882472038269, accuracy: 0.6806144118309021\n",
      "Batch 59 loss: 0.856467604637146, accuracy: 0.680468738079071\n",
      "Batch 60 loss: 0.8006656765937805, accuracy: 0.6809682250022888\n",
      "Batch 61 loss: 0.9466453790664673, accuracy: 0.6818296313285828\n",
      "Batch 62 loss: 1.020992636680603, accuracy: 0.680927574634552\n",
      "Batch 63 loss: 0.8805139660835266, accuracy: 0.6802978515625\n",
      "Batch 64 loss: 0.855734646320343, accuracy: 0.6802884340286255\n",
      "Batch 65 loss: 0.8031149506568909, accuracy: 0.6803977489471436\n",
      "Batch 66 loss: 0.8155050277709961, accuracy: 0.6800373196601868\n",
      "Batch 67 loss: 0.8042470812797546, accuracy: 0.6799172759056091\n",
      "Batch 68 loss: 0.8033186793327332, accuracy: 0.6801403760910034\n",
      "Batch 69 loss: 0.9302316904067993, accuracy: 0.6796875\n",
      "Batch 70 loss: 0.91340172290802, accuracy: 0.6786971688270569\n",
      "Batch 71 loss: 0.7363624572753906, accuracy: 0.6795790195465088\n",
      "Batch 72 loss: 0.943556547164917, accuracy: 0.6789383292198181\n",
      "Batch 73 loss: 0.7245618104934692, accuracy: 0.6797930598258972\n",
      "Batch 74 loss: 0.9382085204124451, accuracy: 0.67989581823349\n",
      "Batch 75 loss: 0.7231311202049255, accuracy: 0.6806126832962036\n",
      "Batch 76 loss: 0.9732558131217957, accuracy: 0.6799919009208679\n",
      "Batch 77 loss: 0.8545441627502441, accuracy: 0.6800881624221802\n",
      "Batch 78 loss: 0.7939891815185547, accuracy: 0.6809731125831604\n",
      "Batch 79 loss: 0.965302050113678, accuracy: 0.6805664300918579\n",
      "Batch 80 loss: 0.8082839250564575, accuracy: 0.6806520223617554\n",
      "Batch 81 loss: 0.8799301385879517, accuracy: 0.6803544163703918\n",
      "Batch 82 loss: 0.7598865032196045, accuracy: 0.6812876462936401\n",
      "Batch 83 loss: 0.8168847560882568, accuracy: 0.6813616156578064\n",
      "Batch 84 loss: 0.9430609345436096, accuracy: 0.6805146932601929\n",
      "Batch 85 loss: 0.7646297216415405, accuracy: 0.6809592843055725\n",
      "Batch 86 loss: 0.8373637795448303, accuracy: 0.6816630959510803\n",
      "Batch 87 loss: 0.815099835395813, accuracy: 0.6820845007896423\n",
      "Batch 88 loss: 0.7841188907623291, accuracy: 0.6824086904525757\n",
      "Batch 89 loss: 0.7878458499908447, accuracy: 0.6823784708976746\n",
      "Batch 90 loss: 0.8358277678489685, accuracy: 0.681833803653717\n",
      "Batch 91 loss: 0.9004423022270203, accuracy: 0.682235062122345\n",
      "Batch 92 loss: 0.780846893787384, accuracy: 0.6827117204666138\n",
      "Batch 93 loss: 1.0220370292663574, accuracy: 0.682263970375061\n",
      "Batch 94 loss: 0.9159396886825562, accuracy: 0.6821545958518982\n",
      "Batch 95 loss: 0.9038456082344055, accuracy: 0.6817219853401184\n",
      "Batch 96 loss: 0.9560526609420776, accuracy: 0.6812983155250549\n",
      "Batch 97 loss: 0.9827434420585632, accuracy: 0.6810427308082581\n",
      "Batch 98 loss: 0.8164609670639038, accuracy: 0.6812657713890076\n",
      "Batch 99 loss: 0.7142689824104309, accuracy: 0.6815624833106995\n",
      "Batch 100 loss: 0.826799213886261, accuracy: 0.6813892126083374\n",
      "Batch 101 loss: 0.7973042726516724, accuracy: 0.6819853186607361\n",
      "Batch 102 loss: 0.9678616523742676, accuracy: 0.6812044978141785\n",
      "Batch 103 loss: 0.9451428651809692, accuracy: 0.6804386973381042\n",
      "Batch 104 loss: 0.7745737433433533, accuracy: 0.6810267567634583\n",
      "Batch 105 loss: 0.8395203351974487, accuracy: 0.6813826560974121\n",
      "Batch 106 loss: 0.8952605724334717, accuracy: 0.6811477541923523\n",
      "Batch 107 loss: 0.8319770097732544, accuracy: 0.6815682649612427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 108 loss: 0.9091250896453857, accuracy: 0.6811926364898682\n",
      "Batch 109 loss: 0.8116386532783508, accuracy: 0.6815341114997864\n",
      "Batch 110 loss: 0.8049507141113281, accuracy: 0.681869387626648\n",
      "Batch 111 loss: 0.8947294354438782, accuracy: 0.6815708875656128\n",
      "Batch 112 loss: 0.9912532567977905, accuracy: 0.6812776327133179\n",
      "Batch 113 loss: 0.8804628849029541, accuracy: 0.6811266541481018\n",
      "Batch 114 loss: 0.856866717338562, accuracy: 0.6809103488922119\n",
      "Batch 115 loss: 0.8452500104904175, accuracy: 0.6809671521186829\n",
      "Batch 116 loss: 0.784394383430481, accuracy: 0.6812232732772827\n",
      "Batch 117 loss: 0.8582554459571838, accuracy: 0.6816075444221497\n",
      "Batch 118 loss: 0.8090681433677673, accuracy: 0.681525707244873\n",
      "Batch 119 loss: 0.812132716178894, accuracy: 0.6815103888511658\n",
      "Batch 120 loss: 0.815828263759613, accuracy: 0.6818181872367859\n",
      "Batch 121 loss: 0.9631383419036865, accuracy: 0.6817367076873779\n",
      "Batch 122 loss: 1.0175163745880127, accuracy: 0.6814659833908081\n",
      "Batch 123 loss: 0.8680853247642517, accuracy: 0.6813256144523621\n",
      "Batch 124 loss: 0.9609991312026978, accuracy: 0.6806874871253967\n",
      "Batch 125 loss: 0.9185689687728882, accuracy: 0.6803695559501648\n",
      "Batch 126 loss: 0.8457971215248108, accuracy: 0.6803026795387268\n",
      "Batch 127 loss: 0.9736902713775635, accuracy: 0.67974853515625\n",
      "Batch 128 loss: 0.8492839336395264, accuracy: 0.6799297332763672\n",
      "Batch 129 loss: 0.817927360534668, accuracy: 0.6801081895828247\n",
      "Batch 130 loss: 0.7431548237800598, accuracy: 0.6802242398262024\n",
      "Batch 131 loss: 0.9469529986381531, accuracy: 0.6799834370613098\n",
      "Batch 132 loss: 0.9103876352310181, accuracy: 0.6797462701797485\n",
      "Batch 133 loss: 0.8019406795501709, accuracy: 0.679862380027771\n",
      "Batch 134 loss: 0.851047158241272, accuracy: 0.6800346970558167\n",
      "Batch 135 loss: 0.9232843518257141, accuracy: 0.6801470518112183\n",
      "Batch 136 loss: 1.0167510509490967, accuracy: 0.6799726486206055\n",
      "Batch 137 loss: 1.0404537916183472, accuracy: 0.6795742511749268\n",
      "Batch 138 loss: 0.8313050866127014, accuracy: 0.6796312928199768\n",
      "Batch 139 loss: 0.9384931325912476, accuracy: 0.6792968511581421\n",
      "Batch 140 loss: 0.8699132800102234, accuracy: 0.6790226101875305\n",
      "Batch 141 loss: 0.9131197333335876, accuracy: 0.6786971688270569\n",
      "Batch 142 loss: 0.7653514742851257, accuracy: 0.6789772510528564\n",
      "Batch 143 loss: 0.706936240196228, accuracy: 0.6795790195465088\n",
      "Batch 144 loss: 0.9247395396232605, accuracy: 0.6793642044067383\n",
      "Batch 145 loss: 1.1556235551834106, accuracy: 0.6790453791618347\n",
      "Batch 146 loss: 0.986290454864502, accuracy: 0.6784651279449463\n",
      "Batch 147 loss: 0.7637316584587097, accuracy: 0.6784734129905701\n",
      "Batch 148 loss: 0.8248863816261292, accuracy: 0.6785339713096619\n",
      "Batch 149 loss: 0.852406919002533, accuracy: 0.6784895658493042\n",
      "Batch 150 loss: 0.7748870253562927, accuracy: 0.6783940196037292\n",
      "Batch 151 loss: 0.850030779838562, accuracy: 0.6784025430679321\n",
      "Batch 152 loss: 0.9399752616882324, accuracy: 0.6784109473228455\n",
      "Batch 153 loss: 0.7949538826942444, accuracy: 0.6786729097366333\n",
      "Batch 154 loss: 0.7673076391220093, accuracy: 0.6787298321723938\n",
      "Batch 155 loss: 1.0201060771942139, accuracy: 0.6781350374221802\n",
      "Batch 156 loss: 0.8741778135299683, accuracy: 0.6782941818237305\n",
      "Batch 157 loss: 1.0763518810272217, accuracy: 0.6778085231781006\n",
      "Batch 158 loss: 0.7800601124763489, accuracy: 0.6778694987297058\n",
      "Batch 159 loss: 0.899567723274231, accuracy: 0.677783191204071\n",
      "Batch 160 loss: 0.8596585988998413, accuracy: 0.6774553656578064\n",
      "Batch 161 loss: 0.7663657665252686, accuracy: 0.6775655746459961\n",
      "Batch 162 loss: 0.9922195672988892, accuracy: 0.6773389577865601\n",
      "Batch 163 loss: 0.9223887920379639, accuracy: 0.6771150827407837\n",
      "Batch 164 loss: 0.9513198733329773, accuracy: 0.6770359873771667\n",
      "Batch 165 loss: 0.8425580859184265, accuracy: 0.6774284839630127\n",
      "Batch 166 loss: 0.8612923622131348, accuracy: 0.677582323551178\n",
      "Batch 167 loss: 0.7596212029457092, accuracy: 0.6780133843421936\n",
      "Batch 168 loss: 0.9254444241523743, accuracy: 0.6779770851135254\n",
      "Batch 169 loss: 0.9246911406517029, accuracy: 0.6774815917015076\n",
      "Batch 170 loss: 0.9525757431983948, accuracy: 0.6771746873855591\n",
      "Batch 171 loss: 0.9443652629852295, accuracy: 0.6770984530448914\n",
      "Batch 172 loss: 0.9157971143722534, accuracy: 0.677068293094635\n",
      "Batch 173 loss: 0.756886899471283, accuracy: 0.6772180199623108\n",
      "Batch 174 loss: 0.8626434803009033, accuracy: 0.6774553656578064\n",
      "Batch 175 loss: 0.9802375435829163, accuracy: 0.6771573424339294\n",
      "Batch 176 loss: 0.7941576242446899, accuracy: 0.677127480506897\n",
      "Batch 177 loss: 0.9455158114433289, accuracy: 0.6767029762268066\n",
      "Batch 178 loss: 0.9405708312988281, accuracy: 0.6765013933181763\n",
      "Batch 179 loss: 0.7889468669891357, accuracy: 0.6766927242279053\n",
      "Batch 180 loss: 0.8985681533813477, accuracy: 0.6765797734260559\n",
      "Batch 181 loss: 0.9422715306282043, accuracy: 0.6762105226516724\n",
      "Batch 182 loss: 0.8752486109733582, accuracy: 0.6761868000030518\n",
      "Batch 183 loss: 0.956871509552002, accuracy: 0.67603600025177\n",
      "Batch 184 loss: 0.7461003065109253, accuracy: 0.6763513684272766\n",
      "Batch 185 loss: 1.0184155702590942, accuracy: 0.676075279712677\n",
      "Batch 186 loss: 0.8239524364471436, accuracy: 0.6763870120048523\n",
      "Batch 187 loss: 0.8737344145774841, accuracy: 0.6762799024581909\n",
      "Batch 188 loss: 0.8795111179351807, accuracy: 0.6761326193809509\n",
      "Batch 189 loss: 0.9809258580207825, accuracy: 0.6763569116592407\n",
      "Batch 190 loss: 1.0481096506118774, accuracy: 0.6761289238929749\n",
      "Batch 191 loss: 0.9172636270523071, accuracy: 0.6759846806526184\n",
      "Batch 192 loss: 0.7615343332290649, accuracy: 0.6762872338294983\n",
      "Batch 193 loss: 0.8241824507713318, accuracy: 0.676425576210022\n",
      "Batch 194 loss: 0.7222816348075867, accuracy: 0.6768830418586731\n",
      "Batch 195 loss: 0.9066512584686279, accuracy: 0.6766182780265808\n",
      "Batch 196 loss: 0.8334089517593384, accuracy: 0.6764355897903442\n",
      "Batch 197 loss: 0.9860300421714783, accuracy: 0.6760574579238892\n",
      "Batch 198 loss: 0.7515602707862854, accuracy: 0.6762327551841736\n",
      "Batch 199 loss: 0.7908109426498413, accuracy: 0.6764453053474426\n",
      "Batch 200 loss: 0.8297304511070251, accuracy: 0.6766557693481445\n",
      "Batch 201 loss: 0.9246978759765625, accuracy: 0.6765934228897095\n",
      "Batch 202 loss: 0.9398233890533447, accuracy: 0.6765317320823669\n",
      "Batch 203 loss: 0.8610626459121704, accuracy: 0.6764323115348816\n",
      "Batch 204 loss: 0.776919960975647, accuracy: 0.6767149567604065\n",
      "Batch 205 loss: 0.9773277044296265, accuracy: 0.6766535043716431\n",
      "Batch 206 loss: 1.0502172708511353, accuracy: 0.6762152910232544\n",
      "Batch 207 loss: 0.8673778176307678, accuracy: 0.6763070821762085\n",
      "Batch 208 loss: 0.9820442795753479, accuracy: 0.6762858629226685\n",
      "Batch 209 loss: 0.7114313244819641, accuracy: 0.6764509081840515\n",
      "Batch 210 loss: 0.8586934804916382, accuracy: 0.6765402555465698\n",
      "Batch 211 loss: 0.8199410438537598, accuracy: 0.6764814257621765\n",
      "Batch 212 loss: 0.9274113774299622, accuracy: 0.676459789276123\n",
      "Batch 213 loss: 0.9027273654937744, accuracy: 0.6763653755187988\n",
      "Batch 214 loss: 0.9314108490943909, accuracy: 0.6762717962265015\n",
      "Batch 215 loss: 1.1070407629013062, accuracy: 0.6758174300193787\n",
      "Batch 216 loss: 0.7980299592018127, accuracy: 0.6759432554244995\n",
      "Batch 217 loss: 0.7559452652931213, accuracy: 0.6759604215621948\n",
      "Batch 218 loss: 0.6927071809768677, accuracy: 0.6762271523475647\n",
      "Batch 219 loss: 0.7778995633125305, accuracy: 0.676562488079071\n",
      "Batch 220 loss: 0.8843103051185608, accuracy: 0.6763998866081238\n",
      "Batch 221 loss: 0.8997231721878052, accuracy: 0.6764146685600281\n",
      "Batch 222 loss: 0.9529339671134949, accuracy: 0.6761841177940369\n",
      "Batch 223 loss: 0.9752898216247559, accuracy: 0.6762346625328064\n",
      "Batch 224 loss: 0.8346479535102844, accuracy: 0.6763036251068115\n",
      "Training epoch: 23, train accuracy: 67.63036346435547, train loss: 0.868042680422465, valid accuracy: 63.13736343383789, valid loss: 1.0891119385587757 \n",
      "Batch 0 loss: 0.8142972588539124, accuracy: 0.7890625\n",
      "Batch 1 loss: 0.8665258884429932, accuracy: 0.734375\n",
      "Batch 2 loss: 0.7558929324150085, accuracy: 0.7265625\n",
      "Batch 3 loss: 0.9081146717071533, accuracy: 0.712890625\n",
      "Batch 4 loss: 0.8594030737876892, accuracy: 0.7015625238418579\n",
      "Batch 5 loss: 1.026461124420166, accuracy: 0.6953125\n",
      "Batch 6 loss: 0.8748126029968262, accuracy: 0.6964285969734192\n",
      "Batch 7 loss: 0.9073505997657776, accuracy: 0.69140625\n",
      "Batch 8 loss: 0.8623890280723572, accuracy: 0.6935763955116272\n",
      "Batch 9 loss: 0.8077395558357239, accuracy: 0.6953125\n",
      "Batch 10 loss: 0.9819821119308472, accuracy: 0.6903409361839294\n",
      "Batch 11 loss: 0.6861753463745117, accuracy: 0.6940104365348816\n",
      "Batch 12 loss: 0.8222196102142334, accuracy: 0.6959134340286255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13 loss: 0.8499102592468262, accuracy: 0.6936383843421936\n",
      "Batch 14 loss: 0.8735224008560181, accuracy: 0.690625011920929\n",
      "Batch 15 loss: 0.9223537445068359, accuracy: 0.68994140625\n",
      "Batch 16 loss: 0.86139976978302, accuracy: 0.6865808963775635\n",
      "Batch 17 loss: 0.8252717852592468, accuracy: 0.6866319179534912\n",
      "Batch 18 loss: 0.8777443766593933, accuracy: 0.6842105388641357\n",
      "Batch 19 loss: 0.8764065504074097, accuracy: 0.6832031011581421\n",
      "Batch 20 loss: 0.7621117830276489, accuracy: 0.6867559552192688\n",
      "Batch 21 loss: 0.8803002238273621, accuracy: 0.6885653138160706\n",
      "Batch 22 loss: 0.7177353501319885, accuracy: 0.6912364363670349\n",
      "Batch 23 loss: 0.8575019836425781, accuracy: 0.6907551884651184\n",
      "Batch 24 loss: 1.0053476095199585, accuracy: 0.6878125071525574\n",
      "Batch 25 loss: 0.8091174364089966, accuracy: 0.6875\n",
      "Batch 26 loss: 0.816629946231842, accuracy: 0.6883680820465088\n",
      "Batch 27 loss: 0.7982891201972961, accuracy: 0.6883370280265808\n",
      "Batch 28 loss: 0.8983803987503052, accuracy: 0.6877694129943848\n",
      "Batch 29 loss: 0.9710231423377991, accuracy: 0.6859375238418579\n",
      "Batch 30 loss: 0.8769549131393433, accuracy: 0.6839717626571655\n",
      "Batch 31 loss: 0.7714823484420776, accuracy: 0.683349609375\n",
      "Batch 32 loss: 0.8750720620155334, accuracy: 0.6811079382896423\n",
      "Batch 33 loss: 1.0555706024169922, accuracy: 0.6796875\n",
      "Batch 34 loss: 0.6972358226776123, accuracy: 0.6819196343421936\n",
      "Batch 35 loss: 0.7981001734733582, accuracy: 0.6833767294883728\n",
      "Batch 36 loss: 0.7229611873626709, accuracy: 0.6851773858070374\n",
      "Batch 37 loss: 0.7829126119613647, accuracy: 0.6846216917037964\n",
      "Batch 38 loss: 0.8326658010482788, accuracy: 0.6852964758872986\n",
      "Batch 39 loss: 0.8505480885505676, accuracy: 0.6851562261581421\n",
      "Batch 40 loss: 0.8991575837135315, accuracy: 0.6850228905677795\n",
      "Batch 41 loss: 0.8975970149040222, accuracy: 0.6860119104385376\n",
      "Batch 42 loss: 0.9166150093078613, accuracy: 0.6853197813034058\n",
      "Batch 43 loss: 0.7979975938796997, accuracy: 0.6853693127632141\n",
      "Batch 44 loss: 0.8008912205696106, accuracy: 0.6847222447395325\n",
      "Batch 45 loss: 0.8273822665214539, accuracy: 0.6847826242446899\n",
      "Batch 46 loss: 0.7178488969802856, accuracy: 0.6858377456665039\n",
      "Batch 47 loss: 0.8957531452178955, accuracy: 0.6853840947151184\n",
      "Batch 48 loss: 0.7596574425697327, accuracy: 0.6855867505073547\n",
      "Batch 49 loss: 0.921832263469696, accuracy: 0.6848437786102295\n",
      "Batch 50 loss: 0.9255393743515015, accuracy: 0.6845894455909729\n",
      "Batch 51 loss: 0.8430496454238892, accuracy: 0.684645414352417\n",
      "Batch 52 loss: 0.8615739345550537, accuracy: 0.6836674809455872\n",
      "Batch 53 loss: 0.8086770176887512, accuracy: 0.6843171119689941\n",
      "Batch 54 loss: 0.9747162461280823, accuracy: 0.6835227012634277\n",
      "Batch 55 loss: 0.7862900495529175, accuracy: 0.6833147406578064\n",
      "Batch 56 loss: 1.0547435283660889, accuracy: 0.6817434430122375\n",
      "Batch 57 loss: 0.8125149607658386, accuracy: 0.6825161576271057\n",
      "Batch 58 loss: 0.739463210105896, accuracy: 0.6837923526763916\n",
      "Batch 59 loss: 0.8281475901603699, accuracy: 0.6838541626930237\n",
      "Batch 60 loss: 0.8501870036125183, accuracy: 0.6831454634666443\n",
      "Batch 61 loss: 0.8861207365989685, accuracy: 0.6832157373428345\n",
      "Batch 62 loss: 0.9265097379684448, accuracy: 0.6829116940498352\n",
      "Batch 63 loss: 0.9095888733863831, accuracy: 0.6826171875\n",
      "Batch 64 loss: 0.8178804516792297, accuracy: 0.682812511920929\n",
      "Batch 65 loss: 0.727720320224762, accuracy: 0.6838304996490479\n",
      "Batch 66 loss: 0.7119727730751038, accuracy: 0.68458491563797\n",
      "Batch 67 loss: 0.8652329444885254, accuracy: 0.6847426295280457\n",
      "Batch 68 loss: 0.8412918448448181, accuracy: 0.6847826242446899\n",
      "Batch 69 loss: 0.701927125453949, accuracy: 0.6859375238418579\n",
      "Batch 70 loss: 0.8603573441505432, accuracy: 0.6858494877815247\n",
      "Batch 71 loss: 1.077075719833374, accuracy: 0.6845703125\n",
      "Batch 72 loss: 0.9757705926895142, accuracy: 0.6837542653083801\n",
      "Batch 73 loss: 0.9352885484695435, accuracy: 0.6832770109176636\n",
      "Batch 74 loss: 0.819821298122406, accuracy: 0.6831250190734863\n",
      "Batch 75 loss: 0.8468749523162842, accuracy: 0.6831825375556946\n",
      "Batch 76 loss: 0.7859578132629395, accuracy: 0.6833400726318359\n",
      "Batch 77 loss: 0.8249040246009827, accuracy: 0.6834936141967773\n",
      "Batch 78 loss: 0.858224093914032, accuracy: 0.682852029800415\n",
      "Batch 79 loss: 0.7648274898529053, accuracy: 0.68359375\n",
      "Batch 80 loss: 0.8516796231269836, accuracy: 0.6839313507080078\n",
      "Batch 81 loss: 0.7712032794952393, accuracy: 0.6845464706420898\n",
      "Batch 82 loss: 0.9621340036392212, accuracy: 0.6834525465965271\n",
      "Batch 83 loss: 0.7788323163986206, accuracy: 0.6833147406578064\n",
      "Batch 84 loss: 0.7528254389762878, accuracy: 0.6833639740943909\n",
      "Batch 85 loss: 0.8275228142738342, accuracy: 0.6828669905662537\n",
      "Batch 86 loss: 0.83570796251297, accuracy: 0.6826508641242981\n",
      "Batch 87 loss: 0.7629865407943726, accuracy: 0.6831498742103577\n",
      "Batch 88 loss: 0.7711628675460815, accuracy: 0.6836376190185547\n",
      "Batch 89 loss: 0.751422107219696, accuracy: 0.684374988079071\n",
      "Batch 90 loss: 0.9364036321640015, accuracy: 0.6837225556373596\n",
      "Batch 91 loss: 0.876710057258606, accuracy: 0.6837635636329651\n",
      "Batch 92 loss: 0.7412090301513672, accuracy: 0.6842237710952759\n",
      "Batch 93 loss: 0.7933514714241028, accuracy: 0.6842586398124695\n",
      "Batch 94 loss: 0.7473925352096558, accuracy: 0.6845394968986511\n",
      "Batch 95 loss: 0.7712563276290894, accuracy: 0.6846516728401184\n",
      "Batch 96 loss: 0.8913475275039673, accuracy: 0.6844394207000732\n",
      "Batch 97 loss: 0.8261611461639404, accuracy: 0.683992326259613\n",
      "Batch 98 loss: 0.7863527536392212, accuracy: 0.6835542917251587\n",
      "Batch 99 loss: 0.989281177520752, accuracy: 0.6832031011581421\n",
      "Batch 100 loss: 0.9660366177558899, accuracy: 0.682394802570343\n",
      "Batch 101 loss: 1.0165667533874512, accuracy: 0.6819087266921997\n",
      "Batch 102 loss: 0.7584643363952637, accuracy: 0.6821905374526978\n",
      "Batch 103 loss: 0.8328920006752014, accuracy: 0.682692289352417\n",
      "Batch 104 loss: 0.8925454020500183, accuracy: 0.6828868985176086\n",
      "Batch 105 loss: 0.9301785230636597, accuracy: 0.6827093362808228\n",
      "Batch 106 loss: 0.8324282765388489, accuracy: 0.6829001307487488\n",
      "Batch 107 loss: 0.9502366185188293, accuracy: 0.6824363470077515\n",
      "Batch 108 loss: 0.7145576477050781, accuracy: 0.6831278800964355\n",
      "Batch 109 loss: 0.7809659838676453, accuracy: 0.6832386255264282\n",
      "Batch 110 loss: 0.9424223899841309, accuracy: 0.6827139854431152\n",
      "Batch 111 loss: 0.8799077272415161, accuracy: 0.6825474500656128\n",
      "Batch 112 loss: 0.8994653820991516, accuracy: 0.6823146939277649\n",
      "Batch 113 loss: 0.7942020297050476, accuracy: 0.6824972629547119\n",
      "Batch 114 loss: 0.9103113412857056, accuracy: 0.6817255616188049\n",
      "Batch 115 loss: 0.9554200768470764, accuracy: 0.681034505367279\n",
      "Batch 116 loss: 0.9962124228477478, accuracy: 0.6808226704597473\n",
      "Batch 117 loss: 0.8469918370246887, accuracy: 0.6809454560279846\n",
      "Batch 118 loss: 0.8864035606384277, accuracy: 0.6812631487846375\n",
      "Batch 119 loss: 0.9810957908630371, accuracy: 0.6807291507720947\n",
      "Batch 120 loss: 0.9034121036529541, accuracy: 0.6798166036605835\n",
      "Batch 121 loss: 0.7736093997955322, accuracy: 0.6798155903816223\n",
      "Batch 122 loss: 1.0021135807037354, accuracy: 0.6794334053993225\n",
      "Batch 123 loss: 0.8509736657142639, accuracy: 0.6794984936714172\n",
      "Batch 124 loss: 0.9605594277381897, accuracy: 0.6791250109672546\n",
      "Batch 125 loss: 0.8248667120933533, accuracy: 0.6794394850730896\n",
      "Batch 126 loss: 0.7968576550483704, accuracy: 0.6796875\n",
      "Batch 127 loss: 0.7705985307693481, accuracy: 0.67987060546875\n",
      "Batch 128 loss: 0.9929007291793823, accuracy: 0.679384708404541\n",
      "Batch 129 loss: 0.7269650101661682, accuracy: 0.6799278855323792\n",
      "Batch 130 loss: 0.9019022583961487, accuracy: 0.6796278357505798\n",
      "Batch 131 loss: 0.8207728266716003, accuracy: 0.6801018118858337\n",
      "Batch 132 loss: 0.9054191708564758, accuracy: 0.6798637509346008\n",
      "Batch 133 loss: 0.8343830704689026, accuracy: 0.6799790263175964\n",
      "Batch 134 loss: 0.7960458397865295, accuracy: 0.6800925731658936\n",
      "Batch 135 loss: 0.8795994520187378, accuracy: 0.6798598170280457\n",
      "Batch 136 loss: 0.8903348445892334, accuracy: 0.6799726486206055\n",
      "Batch 137 loss: 0.9302571415901184, accuracy: 0.6800838112831116\n",
      "Batch 138 loss: 0.8982560038566589, accuracy: 0.6800809502601624\n",
      "Batch 139 loss: 0.820667564868927, accuracy: 0.6802455186843872\n",
      "Batch 140 loss: 0.8786939382553101, accuracy: 0.6801307797431946\n",
      "Batch 141 loss: 0.9122623205184937, accuracy: 0.6801276206970215\n",
      "Batch 142 loss: 0.924454927444458, accuracy: 0.6795782446861267\n",
      "Batch 143 loss: 1.0117775201797485, accuracy: 0.6790364384651184\n",
      "Batch 144 loss: 0.7505543231964111, accuracy: 0.6794180870056152\n",
      "Batch 145 loss: 0.7780290842056274, accuracy: 0.6795269846916199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 146 loss: 0.9048279523849487, accuracy: 0.6791560649871826\n",
      "Batch 147 loss: 0.7244588136672974, accuracy: 0.6797930598258972\n",
      "Batch 148 loss: 0.9222695231437683, accuracy: 0.6797923445701599\n",
      "Batch 149 loss: 0.8934842944145203, accuracy: 0.6797395944595337\n",
      "Batch 150 loss: 0.7292840480804443, accuracy: 0.6798944473266602\n",
      "Batch 151 loss: 0.8929799199104309, accuracy: 0.6798416972160339\n",
      "Batch 152 loss: 0.9898858070373535, accuracy: 0.679330050945282\n",
      "Batch 153 loss: 0.8787400722503662, accuracy: 0.6790787577629089\n",
      "Batch 154 loss: 0.8205506801605225, accuracy: 0.6789818406105042\n",
      "Batch 155 loss: 0.790228545665741, accuracy: 0.6790865659713745\n",
      "Batch 156 loss: 0.6878132224082947, accuracy: 0.6797372698783875\n",
      "Batch 157 loss: 0.8424144387245178, accuracy: 0.6799347400665283\n",
      "Batch 158 loss: 0.7192805409431458, accuracy: 0.6803753972053528\n",
      "Batch 159 loss: 0.8898866772651672, accuracy: 0.68017578125\n",
      "Batch 160 loss: 0.8126894235610962, accuracy: 0.6802697777748108\n",
      "Batch 161 loss: 0.8892938494682312, accuracy: 0.6803144216537476\n",
      "Batch 162 loss: 0.8337978720664978, accuracy: 0.6803106069564819\n",
      "Batch 163 loss: 1.0251986980438232, accuracy: 0.6798780560493469\n",
      "Batch 164 loss: 0.771567165851593, accuracy: 0.6799242496490479\n",
      "Batch 165 loss: 0.8884938955307007, accuracy: 0.6796404123306274\n",
      "Batch 166 loss: 0.9479904174804688, accuracy: 0.6792664527893066\n",
      "Batch 167 loss: 0.9058791995048523, accuracy: 0.6791759729385376\n",
      "Batch 168 loss: 0.6945099234580994, accuracy: 0.6794101595878601\n",
      "Batch 169 loss: 0.7638130187988281, accuracy: 0.6799172759056091\n",
      "Batch 170 loss: 0.9341366291046143, accuracy: 0.6796875\n",
      "Batch 171 loss: 0.9955508708953857, accuracy: 0.6795512437820435\n",
      "Batch 172 loss: 0.919908881187439, accuracy: 0.6791907548904419\n",
      "Batch 173 loss: 0.8979842066764832, accuracy: 0.6790589094161987\n",
      "Batch 174 loss: 0.9154362082481384, accuracy: 0.678928554058075\n",
      "Batch 175 loss: 0.9166099429130554, accuracy: 0.6787553429603577\n",
      "Batch 176 loss: 0.7417614459991455, accuracy: 0.6789371371269226\n",
      "Batch 177 loss: 0.9561744332313538, accuracy: 0.6787219047546387\n",
      "Batch 178 loss: 0.8409371972084045, accuracy: 0.6789455413818359\n",
      "Batch 179 loss: 0.7638822197914124, accuracy: 0.6790798902511597\n",
      "Batch 180 loss: 0.8510111570358276, accuracy: 0.6788673996925354\n",
      "Batch 181 loss: 0.8887529969215393, accuracy: 0.6787431240081787\n",
      "Batch 182 loss: 0.811453640460968, accuracy: 0.6788763403892517\n",
      "Batch 183 loss: 0.8603660464286804, accuracy: 0.6788807511329651\n",
      "Batch 184 loss: 0.7064987421035767, accuracy: 0.6792230010032654\n",
      "Batch 185 loss: 0.886372447013855, accuracy: 0.679393470287323\n",
      "Batch 186 loss: 0.7753052711486816, accuracy: 0.6796457171440125\n",
      "Batch 187 loss: 0.9077625870704651, accuracy: 0.6795628070831299\n",
      "Batch 188 loss: 0.8682175874710083, accuracy: 0.6795634627342224\n",
      "Batch 189 loss: 0.866310715675354, accuracy: 0.6793174147605896\n",
      "Batch 190 loss: 0.7359805107116699, accuracy: 0.6795238852500916\n",
      "Batch 191 loss: 0.8817697167396545, accuracy: 0.6797688603401184\n",
      "Batch 192 loss: 0.9625279307365417, accuracy: 0.679768443107605\n",
      "Batch 193 loss: 0.7118054628372192, accuracy: 0.6801304817199707\n",
      "Batch 194 loss: 0.8410757184028625, accuracy: 0.6802083253860474\n",
      "Batch 195 loss: 0.7480086088180542, accuracy: 0.6804448366165161\n",
      "Batch 196 loss: 0.8108292818069458, accuracy: 0.6805202960968018\n",
      "Batch 197 loss: 0.8660455942153931, accuracy: 0.6803977489471436\n",
      "Batch 198 loss: 0.9112233519554138, accuracy: 0.6802763938903809\n",
      "Batch 199 loss: 0.8205309510231018, accuracy: 0.6802343726158142\n",
      "Batch 200 loss: 0.8891574144363403, accuracy: 0.6800373196601868\n",
      "Batch 201 loss: 0.7265560626983643, accuracy: 0.6801902651786804\n",
      "Batch 202 loss: 0.8785214424133301, accuracy: 0.6802262663841248\n",
      "Batch 203 loss: 0.8319762349128723, accuracy: 0.6801470518112183\n",
      "Batch 204 loss: 0.8154130578041077, accuracy: 0.6801066994667053\n",
      "Batch 205 loss: 0.8392477035522461, accuracy: 0.6799908876419067\n",
      "Batch 206 loss: 0.9621811509132385, accuracy: 0.6799139380455017\n",
      "Batch 207 loss: 0.7728654742240906, accuracy: 0.68017578125\n",
      "Batch 208 loss: 0.8058229684829712, accuracy: 0.6802482008934021\n",
      "Batch 209 loss: 0.8327363729476929, accuracy: 0.6802083253860474\n",
      "Batch 210 loss: 0.8479433059692383, accuracy: 0.6800947785377502\n",
      "Batch 211 loss: 0.9157395362854004, accuracy: 0.6798349022865295\n",
      "Batch 212 loss: 0.7679521441459656, accuracy: 0.6797975301742554\n",
      "Batch 213 loss: 0.8027516007423401, accuracy: 0.6799430251121521\n",
      "Batch 214 loss: 0.8841562271118164, accuracy: 0.6797601580619812\n",
      "Batch 215 loss: 0.9180743098258972, accuracy: 0.6796151399612427\n",
      "Batch 216 loss: 0.8724827766418457, accuracy: 0.679579496383667\n",
      "Batch 217 loss: 0.8238931894302368, accuracy: 0.6794366240501404\n",
      "Batch 218 loss: 0.7587004899978638, accuracy: 0.6795448064804077\n",
      "Batch 219 loss: 0.7854139804840088, accuracy: 0.6795454621315002\n",
      "Batch 220 loss: 0.9302354454994202, accuracy: 0.6795814633369446\n",
      "Batch 221 loss: 0.9177145957946777, accuracy: 0.6796171069145203\n",
      "Batch 222 loss: 0.8934727907180786, accuracy: 0.6792320609092712\n",
      "Batch 223 loss: 0.8229116797447205, accuracy: 0.6791294813156128\n",
      "Batch 224 loss: 0.6322414875030518, accuracy: 0.6792991757392883\n",
      "Training epoch: 24, train accuracy: 67.92991638183594, train loss: 0.8514095075925191, valid accuracy: 63.6667594909668, valid loss: 1.0181817700122964 \n",
      "Batch 0 loss: 0.7918202877044678, accuracy: 0.734375\n",
      "Batch 1 loss: 0.6502986550331116, accuracy: 0.765625\n",
      "Batch 2 loss: 0.7734639644622803, accuracy: 0.7526041865348816\n",
      "Batch 3 loss: 0.8458929657936096, accuracy: 0.728515625\n",
      "Batch 4 loss: 0.9695741534233093, accuracy: 0.7046874761581421\n",
      "Batch 5 loss: 0.9041915535926819, accuracy: 0.7018229365348816\n",
      "Batch 6 loss: 0.8279789686203003, accuracy: 0.6975446343421936\n",
      "Batch 7 loss: 0.7704465389251709, accuracy: 0.6982421875\n",
      "Batch 8 loss: 0.8736492395401001, accuracy: 0.6987847089767456\n",
      "Batch 9 loss: 0.774220883846283, accuracy: 0.69921875\n",
      "Batch 10 loss: 0.7833343148231506, accuracy: 0.7017045617103577\n",
      "Batch 11 loss: 0.8497518301010132, accuracy: 0.6985676884651184\n",
      "Batch 12 loss: 0.7952117919921875, accuracy: 0.6971153616905212\n",
      "Batch 13 loss: 0.903367817401886, accuracy: 0.6902901530265808\n",
      "Batch 14 loss: 0.8305683732032776, accuracy: 0.6901041865348816\n",
      "Batch 15 loss: 0.9073103070259094, accuracy: 0.689453125\n",
      "Batch 16 loss: 0.7891348600387573, accuracy: 0.6888786554336548\n",
      "Batch 17 loss: 0.8686497807502747, accuracy: 0.6879340410232544\n",
      "Batch 18 loss: 0.7779966592788696, accuracy: 0.6899670958518982\n",
      "Batch 19 loss: 0.830079972743988, accuracy: 0.692578136920929\n",
      "Batch 20 loss: 0.7402757406234741, accuracy: 0.6941964030265808\n",
      "Batch 21 loss: 0.9057889580726624, accuracy: 0.6921164989471436\n",
      "Batch 22 loss: 0.9563981890678406, accuracy: 0.69191575050354\n",
      "Batch 23 loss: 0.7302675843238831, accuracy: 0.6943359375\n",
      "Batch 24 loss: 0.8755707740783691, accuracy: 0.6924999952316284\n",
      "Batch 25 loss: 0.8064987063407898, accuracy: 0.6926081776618958\n",
      "Batch 26 loss: 0.7801363468170166, accuracy: 0.6909722089767456\n",
      "Batch 27 loss: 0.7441191673278809, accuracy: 0.6930803656578064\n",
      "Batch 28 loss: 0.8509163856506348, accuracy: 0.6928879022598267\n",
      "Batch 29 loss: 0.7379744648933411, accuracy: 0.694531261920929\n",
      "Batch 30 loss: 0.8558821082115173, accuracy: 0.6932963728904724\n",
      "Batch 31 loss: 0.8778863549232483, accuracy: 0.6923828125\n",
      "Batch 32 loss: 0.9372926950454712, accuracy: 0.6910511255264282\n",
      "Batch 33 loss: 0.7407399415969849, accuracy: 0.6916360259056091\n",
      "Batch 34 loss: 0.9330599308013916, accuracy: 0.6917410492897034\n",
      "Batch 35 loss: 0.8730684518814087, accuracy: 0.6924912929534912\n",
      "Batch 36 loss: 0.9155927896499634, accuracy: 0.6925675868988037\n",
      "Batch 37 loss: 0.6639645099639893, accuracy: 0.6942845582962036\n",
      "Batch 38 loss: 0.9128009676933289, accuracy: 0.6933093070983887\n",
      "Batch 39 loss: 0.7065293192863464, accuracy: 0.693164050579071\n",
      "Batch 40 loss: 0.618932843208313, accuracy: 0.6943597793579102\n",
      "Batch 41 loss: 0.9253923296928406, accuracy: 0.6932663917541504\n",
      "Batch 42 loss: 0.7696571350097656, accuracy: 0.6940407156944275\n",
      "Batch 43 loss: 0.9290307760238647, accuracy: 0.6922940611839294\n",
      "Batch 44 loss: 0.8390081524848938, accuracy: 0.6923611164093018\n",
      "Batch 45 loss: 0.8656375408172607, accuracy: 0.6920856237411499\n",
      "Batch 46 loss: 0.9436035752296448, accuracy: 0.6901595592498779\n",
      "Batch 47 loss: 0.8803019523620605, accuracy: 0.6892903447151184\n",
      "Batch 48 loss: 0.7600001692771912, accuracy: 0.6903699040412903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 49 loss: 0.9220128655433655, accuracy: 0.6909375190734863\n",
      "Batch 50 loss: 0.7466384172439575, accuracy: 0.6907169222831726\n",
      "Batch 51 loss: 0.8908454775810242, accuracy: 0.690504789352417\n",
      "Batch 52 loss: 0.9209474325180054, accuracy: 0.690005898475647\n",
      "Batch 53 loss: 0.7666875123977661, accuracy: 0.69140625\n",
      "Batch 54 loss: 0.9834582805633545, accuracy: 0.6892045736312866\n",
      "Batch 55 loss: 0.7480204105377197, accuracy: 0.689453125\n",
      "Batch 56 loss: 0.7268796563148499, accuracy: 0.6905153393745422\n",
      "Batch 57 loss: 0.7622842192649841, accuracy: 0.6908674836158752\n",
      "Batch 58 loss: 0.8462786674499512, accuracy: 0.6909428238868713\n",
      "Batch 59 loss: 0.7073082327842712, accuracy: 0.692187488079071\n",
      "Batch 60 loss: 0.807295560836792, accuracy: 0.6921106576919556\n",
      "Batch 61 loss: 0.9288915395736694, accuracy: 0.6911542415618896\n",
      "Batch 62 loss: 0.6643283367156982, accuracy: 0.6920883059501648\n",
      "Batch 63 loss: 0.775890052318573, accuracy: 0.6917724609375\n",
      "Batch 64 loss: 0.9371525049209595, accuracy: 0.6908653974533081\n",
      "Batch 65 loss: 0.8271918296813965, accuracy: 0.6917613744735718\n",
      "Batch 66 loss: 0.8700782060623169, accuracy: 0.6914645433425903\n",
      "Batch 67 loss: 0.8793222904205322, accuracy: 0.6917509436607361\n",
      "Batch 68 loss: 0.9322418570518494, accuracy: 0.6911231875419617\n",
      "Batch 69 loss: 0.9293130040168762, accuracy: 0.6907365918159485\n",
      "Batch 70 loss: 0.8081201314926147, accuracy: 0.6909111142158508\n",
      "Batch 71 loss: 0.7547997236251831, accuracy: 0.6918402910232544\n",
      "Batch 72 loss: 0.7497830986976624, accuracy: 0.692315936088562\n",
      "Batch 73 loss: 0.8094564080238342, accuracy: 0.6927787065505981\n",
      "Batch 74 loss: 0.740565836429596, accuracy: 0.6936458349227905\n",
      "Batch 75 loss: 0.6522574424743652, accuracy: 0.6942845582962036\n",
      "Batch 76 loss: 0.6880410313606262, accuracy: 0.6948052048683167\n",
      "Batch 77 loss: 0.7150050401687622, accuracy: 0.6957131624221802\n",
      "Batch 78 loss: 0.7140005826950073, accuracy: 0.6961036324501038\n",
      "Batch 79 loss: 0.7958777546882629, accuracy: 0.6962890625\n",
      "Batch 80 loss: 0.9229656457901001, accuracy: 0.6956983208656311\n",
      "Batch 81 loss: 0.7779167890548706, accuracy: 0.695598304271698\n",
      "Batch 82 loss: 1.0106276273727417, accuracy: 0.6946536302566528\n",
      "Batch 83 loss: 0.9065931439399719, accuracy: 0.6941034197807312\n",
      "Batch 84 loss: 0.8175671100616455, accuracy: 0.6942095756530762\n",
      "Batch 85 loss: 0.8918923735618591, accuracy: 0.6939498782157898\n",
      "Batch 86 loss: 0.8439671397209167, accuracy: 0.6945043206214905\n",
      "Batch 87 loss: 0.932741105556488, accuracy: 0.6940696239471436\n",
      "Batch 88 loss: 0.8123733401298523, accuracy: 0.6937324404716492\n",
      "Batch 89 loss: 0.9214210510253906, accuracy: 0.6934027671813965\n",
      "Batch 90 loss: 0.8659709095954895, accuracy: 0.6929086446762085\n",
      "Batch 91 loss: 0.7420989274978638, accuracy: 0.6931895613670349\n",
      "Batch 92 loss: 0.7715911865234375, accuracy: 0.6935483813285828\n",
      "Batch 93 loss: 0.8147444725036621, accuracy: 0.693567156791687\n",
      "Batch 94 loss: 0.853489339351654, accuracy: 0.6933388113975525\n",
      "Batch 95 loss: 0.734835684299469, accuracy: 0.6939290165901184\n",
      "Batch 96 loss: 0.9469294548034668, accuracy: 0.6934600472450256\n",
      "Batch 97 loss: 0.7743244171142578, accuracy: 0.6932398080825806\n",
      "Batch 98 loss: 0.7933678030967712, accuracy: 0.6933396458625793\n",
      "Batch 99 loss: 0.7595057487487793, accuracy: 0.693359375\n",
      "Batch 100 loss: 0.9045613408088684, accuracy: 0.6933013796806335\n",
      "Batch 101 loss: 0.8010423183441162, accuracy: 0.6937040686607361\n",
      "Batch 102 loss: 0.8909969925880432, accuracy: 0.6934921145439148\n",
      "Batch 103 loss: 0.9041520953178406, accuracy: 0.6929837465286255\n",
      "Batch 104 loss: 0.7039588093757629, accuracy: 0.6937500238418579\n",
      "Batch 105 loss: 1.012263536453247, accuracy: 0.692511796951294\n",
      "Batch 106 loss: 0.8930140137672424, accuracy: 0.6922459006309509\n",
      "Batch 107 loss: 0.8519628047943115, accuracy: 0.6922019720077515\n",
      "Batch 108 loss: 0.8454388380050659, accuracy: 0.6919438242912292\n",
      "Batch 109 loss: 0.9461362361907959, accuracy: 0.6909801363945007\n",
      "Batch 110 loss: 0.753918468952179, accuracy: 0.6908079981803894\n",
      "Batch 111 loss: 0.7980649471282959, accuracy: 0.6908482313156128\n",
      "Batch 112 loss: 0.9366362690925598, accuracy: 0.6904729008674622\n",
      "Batch 113 loss: 0.7734251022338867, accuracy: 0.691132128238678\n",
      "Batch 114 loss: 0.8285022974014282, accuracy: 0.691440224647522\n",
      "Batch 115 loss: 0.7312318086624146, accuracy: 0.6918777227401733\n",
      "Batch 116 loss: 1.0595617294311523, accuracy: 0.691038966178894\n",
      "Batch 117 loss: 0.8685850501060486, accuracy: 0.6906117796897888\n",
      "Batch 118 loss: 0.7955232262611389, accuracy: 0.6908482313156128\n",
      "Batch 119 loss: 0.780464231967926, accuracy: 0.6910156011581421\n",
      "Batch 120 loss: 0.8491277098655701, accuracy: 0.6904700398445129\n",
      "Batch 121 loss: 0.7953947186470032, accuracy: 0.6901895403862\n",
      "Batch 122 loss: 0.9028815031051636, accuracy: 0.6898500919342041\n",
      "Batch 123 loss: 0.9324629306793213, accuracy: 0.6896421313285828\n",
      "Batch 124 loss: 0.8171043395996094, accuracy: 0.6894999742507935\n",
      "Batch 125 loss: 0.8190464377403259, accuracy: 0.6899181604385376\n",
      "Batch 126 loss: 0.7699713110923767, accuracy: 0.6899606585502625\n",
      "Batch 127 loss: 0.8545028567314148, accuracy: 0.68988037109375\n",
      "Batch 128 loss: 0.9933627843856812, accuracy: 0.689437985420227\n",
      "Batch 129 loss: 0.7738432884216309, accuracy: 0.6897836327552795\n",
      "Batch 130 loss: 0.7757300734519958, accuracy: 0.6900644302368164\n",
      "Batch 131 loss: 0.9298555850982666, accuracy: 0.6899266242980957\n",
      "Batch 132 loss: 0.8339262008666992, accuracy: 0.690143346786499\n",
      "Batch 133 loss: 0.7666207551956177, accuracy: 0.690240204334259\n",
      "Batch 134 loss: 0.8509714007377625, accuracy: 0.6905092597007751\n",
      "Batch 135 loss: 0.8568476438522339, accuracy: 0.6907743811607361\n",
      "Batch 136 loss: 0.7849574089050293, accuracy: 0.6908645033836365\n",
      "Batch 137 loss: 0.8635137677192688, accuracy: 0.6906136870384216\n",
      "Batch 138 loss: 0.8611020445823669, accuracy: 0.6905350685119629\n",
      "Batch 139 loss: 0.8245636224746704, accuracy: 0.6906808018684387\n",
      "Batch 140 loss: 0.7190505862236023, accuracy: 0.6911569237709045\n",
      "Batch 141 loss: 0.7465013861656189, accuracy: 0.6914612650871277\n",
      "Batch 142 loss: 0.9495590925216675, accuracy: 0.6912150382995605\n",
      "Batch 143 loss: 0.7753393054008484, accuracy: 0.6915147304534912\n",
      "Batch 144 loss: 0.8171854019165039, accuracy: 0.6915948390960693\n",
      "Batch 145 loss: 1.0182271003723145, accuracy: 0.6913527250289917\n",
      "Batch 146 loss: 0.8249956369400024, accuracy: 0.6911671161651611\n",
      "Batch 147 loss: 0.9326550960540771, accuracy: 0.6908783912658691\n",
      "Batch 148 loss: 0.8757334351539612, accuracy: 0.6904886960983276\n",
      "Batch 149 loss: 0.6634652018547058, accuracy: 0.6910416483879089\n",
      "Batch 150 loss: 0.882297694683075, accuracy: 0.6911216974258423\n",
      "Batch 151 loss: 0.7868384122848511, accuracy: 0.6910978555679321\n",
      "Batch 152 loss: 0.8466048240661621, accuracy: 0.6910743713378906\n",
      "Batch 153 loss: 0.9431642293930054, accuracy: 0.6909496784210205\n",
      "Batch 154 loss: 0.8337532877922058, accuracy: 0.6908265948295593\n",
      "Batch 155 loss: 0.7821457982063293, accuracy: 0.6910557150840759\n",
      "Batch 156 loss: 1.0236198902130127, accuracy: 0.6909832954406738\n",
      "Batch 157 loss: 0.8520976305007935, accuracy: 0.6910106539726257\n",
      "Batch 158 loss: 0.9616819024085999, accuracy: 0.6904481053352356\n",
      "Batch 159 loss: 0.7067511677742004, accuracy: 0.690722644329071\n",
      "Batch 160 loss: 0.7584421038627625, accuracy: 0.6909452676773071\n",
      "Batch 161 loss: 0.8736216425895691, accuracy: 0.6907793283462524\n",
      "Batch 162 loss: 0.8134720325469971, accuracy: 0.6909030079841614\n",
      "Batch 163 loss: 0.9668437838554382, accuracy: 0.6905487775802612\n",
      "Batch 164 loss: 0.9128472805023193, accuracy: 0.6903409361839294\n",
      "Batch 165 loss: 0.923944890499115, accuracy: 0.690182626247406\n",
      "Batch 166 loss: 0.8271785378456116, accuracy: 0.6902133226394653\n",
      "Batch 167 loss: 0.8890379667282104, accuracy: 0.690057635307312\n",
      "Batch 168 loss: 1.080701470375061, accuracy: 0.6896727085113525\n",
      "Batch 169 loss: 0.9058225750923157, accuracy: 0.6894761323928833\n",
      "Batch 170 loss: 0.9205076694488525, accuracy: 0.6896016001701355\n",
      "Batch 171 loss: 0.8058475255966187, accuracy: 0.689453125\n",
      "Batch 172 loss: 0.7435808777809143, accuracy: 0.6893966794013977\n",
      "Batch 173 loss: 0.8763325810432434, accuracy: 0.6891613006591797\n",
      "Batch 174 loss: 0.8851331472396851, accuracy: 0.689464271068573\n",
      "Batch 175 loss: 0.8503037095069885, accuracy: 0.6895862817764282\n",
      "Batch 176 loss: 0.8463693261146545, accuracy: 0.6894862055778503\n",
      "Batch 177 loss: 0.9239605069160461, accuracy: 0.6895189881324768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 178 loss: 0.8265066146850586, accuracy: 0.6894204020500183\n",
      "Batch 179 loss: 0.7465721368789673, accuracy: 0.689366340637207\n",
      "Batch 180 loss: 0.824903130531311, accuracy: 0.6889675259590149\n",
      "Batch 181 loss: 0.7294468283653259, accuracy: 0.6891741156578064\n",
      "Batch 182 loss: 0.9719762206077576, accuracy: 0.6887807250022888\n",
      "Batch 183 loss: 0.9752455353736877, accuracy: 0.688816249370575\n",
      "Batch 184 loss: 0.6806539297103882, accuracy: 0.6889780163764954\n",
      "Batch 185 loss: 0.8136846423149109, accuracy: 0.6889700889587402\n",
      "Batch 186 loss: 0.9754829406738281, accuracy: 0.6887115836143494\n",
      "Batch 187 loss: 0.7854763865470886, accuracy: 0.688621997833252\n",
      "Batch 188 loss: 0.8203401565551758, accuracy: 0.6886160969734192\n",
      "Batch 189 loss: 0.9249181151390076, accuracy: 0.6884868144989014\n",
      "Batch 190 loss: 0.7740556597709656, accuracy: 0.6885225772857666\n",
      "Batch 191 loss: 0.7635630965232849, accuracy: 0.6885986328125\n",
      "Batch 192 loss: 0.7909603714942932, accuracy: 0.6887953281402588\n",
      "Batch 193 loss: 0.8496342301368713, accuracy: 0.6886276006698608\n",
      "Batch 194 loss: 0.8162241578102112, accuracy: 0.6887019276618958\n",
      "Batch 195 loss: 0.9204370975494385, accuracy: 0.6885363459587097\n",
      "Batch 196 loss: 0.9202747941017151, accuracy: 0.6883327960968018\n",
      "Batch 197 loss: 0.9052327871322632, accuracy: 0.6882497072219849\n",
      "Batch 198 loss: 0.7746586799621582, accuracy: 0.6883244514465332\n",
      "Batch 199 loss: 0.8660646677017212, accuracy: 0.6881640553474426\n",
      "Batch 200 loss: 0.8573681712150574, accuracy: 0.6877720952033997\n",
      "Batch 201 loss: 0.7416085600852966, accuracy: 0.6879254579544067\n",
      "Batch 202 loss: 0.826545774936676, accuracy: 0.6878848671913147\n",
      "Batch 203 loss: 1.0385161638259888, accuracy: 0.6875765919685364\n",
      "Batch 204 loss: 0.8418566584587097, accuracy: 0.6876905560493469\n",
      "Batch 205 loss: 0.788223385810852, accuracy: 0.6878412961959839\n",
      "Batch 206 loss: 0.8877151012420654, accuracy: 0.6877641677856445\n",
      "Batch 207 loss: 0.8113616108894348, accuracy: 0.6877253651618958\n",
      "Batch 208 loss: 0.8799444437026978, accuracy: 0.6874626278877258\n",
      "Batch 209 loss: 0.8477097153663635, accuracy: 0.6874256134033203\n",
      "Batch 210 loss: 0.7248123288154602, accuracy: 0.687574028968811\n",
      "Batch 211 loss: 0.804368793964386, accuracy: 0.6872420310974121\n",
      "Batch 212 loss: 0.9083285927772522, accuracy: 0.6871699094772339\n",
      "Batch 213 loss: 0.7976433634757996, accuracy: 0.687171459197998\n",
      "Batch 214 loss: 0.8806585669517517, accuracy: 0.6869186162948608\n",
      "Batch 215 loss: 0.7368969321250916, accuracy: 0.6871021389961243\n",
      "Batch 216 loss: 0.8182246088981628, accuracy: 0.6871399879455566\n",
      "Batch 217 loss: 0.8279786705970764, accuracy: 0.6870341300964355\n",
      "Batch 218 loss: 0.7907938957214355, accuracy: 0.6869292259216309\n",
      "Batch 219 loss: 0.8077730536460876, accuracy: 0.6870383620262146\n",
      "Batch 220 loss: 0.827654242515564, accuracy: 0.6870051026344299\n",
      "Batch 221 loss: 0.766014575958252, accuracy: 0.6872184872627258\n",
      "Batch 222 loss: 0.7970714569091797, accuracy: 0.6870796084403992\n",
      "Batch 223 loss: 0.7693364024162292, accuracy: 0.6870814561843872\n",
      "Batch 224 loss: 0.6633085012435913, accuracy: 0.6871016025543213\n",
      "Training epoch: 25, train accuracy: 68.71015930175781, train loss: 0.8358579307132297, valid accuracy: 62.58010482788086, valid loss: 1.0707328340102886 \n",
      "Batch 0 loss: 0.7382119297981262, accuracy: 0.71875\n",
      "Batch 1 loss: 0.7575671076774597, accuracy: 0.73046875\n",
      "Batch 2 loss: 0.7875610589981079, accuracy: 0.7239583134651184\n",
      "Batch 3 loss: 0.7626192569732666, accuracy: 0.7265625\n",
      "Batch 4 loss: 0.8193693161010742, accuracy: 0.721875011920929\n",
      "Batch 5 loss: 0.848456621170044, accuracy: 0.7057291865348816\n",
      "Batch 6 loss: 0.766701877117157, accuracy: 0.6986607313156128\n",
      "Batch 7 loss: 0.8946143388748169, accuracy: 0.6923828125\n",
      "Batch 8 loss: 0.8170559406280518, accuracy: 0.6866319179534912\n",
      "Batch 9 loss: 0.8581225275993347, accuracy: 0.6812499761581421\n",
      "Batch 10 loss: 0.8783103823661804, accuracy: 0.6796875\n",
      "Batch 11 loss: 0.8773106336593628, accuracy: 0.6783854365348816\n",
      "Batch 12 loss: 0.8174616694450378, accuracy: 0.6754807829856873\n",
      "Batch 13 loss: 0.7631819248199463, accuracy: 0.6774553656578064\n",
      "Batch 14 loss: 0.9459837079048157, accuracy: 0.6729166507720947\n",
      "Batch 15 loss: 0.7447117567062378, accuracy: 0.6767578125\n",
      "Batch 16 loss: 0.9062390327453613, accuracy: 0.6773896813392639\n",
      "Batch 17 loss: 0.6709067821502686, accuracy: 0.6818576455116272\n",
      "Batch 18 loss: 0.7138858437538147, accuracy: 0.6837993264198303\n",
      "Batch 19 loss: 0.8025540709495544, accuracy: 0.682812511920929\n",
      "Batch 20 loss: 0.8369083404541016, accuracy: 0.6841517686843872\n",
      "Batch 21 loss: 0.7582660913467407, accuracy: 0.6850141882896423\n",
      "Batch 22 loss: 0.7870821356773376, accuracy: 0.6875\n",
      "Batch 23 loss: 0.845982551574707, accuracy: 0.6875\n",
      "Batch 24 loss: 0.9006760120391846, accuracy: 0.6871874928474426\n",
      "Batch 25 loss: 0.9622538685798645, accuracy: 0.6875\n",
      "Batch 26 loss: 0.9109459519386292, accuracy: 0.6866319179534912\n",
      "Batch 27 loss: 0.7013196349143982, accuracy: 0.6886160969734192\n",
      "Batch 28 loss: 0.8128853440284729, accuracy: 0.686152994632721\n",
      "Batch 29 loss: 0.8240144848823547, accuracy: 0.6864583492279053\n",
      "Batch 30 loss: 0.7142255306243896, accuracy: 0.6875\n",
      "Batch 31 loss: 0.8272090554237366, accuracy: 0.6875\n",
      "Batch 32 loss: 0.8272960782051086, accuracy: 0.6875\n",
      "Batch 33 loss: 0.7512335181236267, accuracy: 0.6870404481887817\n",
      "Batch 34 loss: 0.8080235719680786, accuracy: 0.6863839030265808\n",
      "Batch 35 loss: 0.7462570071220398, accuracy: 0.6870659589767456\n",
      "Batch 36 loss: 0.7234809994697571, accuracy: 0.6891891956329346\n",
      "Batch 37 loss: 0.8835422992706299, accuracy: 0.6881167888641357\n",
      "Batch 38 loss: 0.818831205368042, accuracy: 0.6883012652397156\n",
      "Batch 39 loss: 0.9093757271766663, accuracy: 0.687304675579071\n",
      "Batch 40 loss: 0.870211660861969, accuracy: 0.6852133870124817\n",
      "Batch 41 loss: 0.6484370231628418, accuracy: 0.6875\n",
      "Batch 42 loss: 0.7448418140411377, accuracy: 0.6896802186965942\n",
      "Batch 43 loss: 0.8605080842971802, accuracy: 0.6887428760528564\n",
      "Batch 44 loss: 0.7867950201034546, accuracy: 0.6888889074325562\n",
      "Batch 45 loss: 0.7838679552078247, accuracy: 0.68919837474823\n",
      "Batch 46 loss: 0.5656546950340271, accuracy: 0.690990686416626\n",
      "Batch 47 loss: 0.6761943697929382, accuracy: 0.6923828125\n",
      "Batch 48 loss: 1.0299631357192993, accuracy: 0.6919642686843872\n",
      "Batch 49 loss: 0.8564242720603943, accuracy: 0.6915624737739563\n",
      "Batch 50 loss: 0.7531893849372864, accuracy: 0.6916360259056091\n",
      "Batch 51 loss: 0.7969153523445129, accuracy: 0.6926081776618958\n",
      "Batch 52 loss: 0.8781386613845825, accuracy: 0.6923643946647644\n",
      "Batch 53 loss: 0.8579658269882202, accuracy: 0.6906828880310059\n",
      "Batch 54 loss: 0.7147810459136963, accuracy: 0.6920454502105713\n",
      "Batch 55 loss: 0.8311885595321655, accuracy: 0.6911272406578064\n",
      "Batch 56 loss: 0.7763200998306274, accuracy: 0.6914747953414917\n",
      "Batch 57 loss: 0.9821479916572571, accuracy: 0.690059244632721\n",
      "Batch 58 loss: 0.9739643335342407, accuracy: 0.6893538236618042\n",
      "Batch 59 loss: 0.9187936782836914, accuracy: 0.6884114742279053\n",
      "Batch 60 loss: 0.6873924136161804, accuracy: 0.6889088153839111\n",
      "Batch 61 loss: 0.8650792241096497, accuracy: 0.6885080933570862\n",
      "Batch 62 loss: 0.9276018738746643, accuracy: 0.688740074634552\n",
      "Batch 63 loss: 0.922183096408844, accuracy: 0.6884765625\n",
      "Batch 64 loss: 0.8883001804351807, accuracy: 0.6875\n",
      "Batch 65 loss: 0.8031793236732483, accuracy: 0.6873816251754761\n",
      "Batch 66 loss: 0.6012397408485413, accuracy: 0.688666045665741\n",
      "Batch 67 loss: 0.9717608690261841, accuracy: 0.6887637972831726\n",
      "Batch 68 loss: 0.8693269491195679, accuracy: 0.6882925629615784\n",
      "Batch 69 loss: 1.0101672410964966, accuracy: 0.6873884201049805\n",
      "Batch 70 loss: 0.8540631532669067, accuracy: 0.6877200603485107\n",
      "Batch 71 loss: 0.890156626701355, accuracy: 0.6870659589767456\n",
      "Batch 72 loss: 0.8073310852050781, accuracy: 0.6873929500579834\n",
      "Batch 73 loss: 0.712721586227417, accuracy: 0.6880278587341309\n",
      "Batch 74 loss: 0.9080888628959656, accuracy: 0.6871874928474426\n",
      "Batch 75 loss: 0.7818241715431213, accuracy: 0.6870887875556946\n",
      "Batch 76 loss: 0.8019706010818481, accuracy: 0.6875\n",
      "Batch 77 loss: 0.8720197677612305, accuracy: 0.6872996687889099\n",
      "Batch 78 loss: 0.9174141883850098, accuracy: 0.6870055198669434\n",
      "Batch 79 loss: 0.8798759579658508, accuracy: 0.6871093511581421\n",
      "Batch 80 loss: 0.7655530571937561, accuracy: 0.6879822611808777\n",
      "Batch 81 loss: 0.9424049258232117, accuracy: 0.6869283318519592\n",
      "Batch 82 loss: 0.767849862575531, accuracy: 0.6872175931930542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 83 loss: 0.9470292925834656, accuracy: 0.6867559552192688\n",
      "Batch 84 loss: 0.9061732292175293, accuracy: 0.6864889860153198\n",
      "Batch 85 loss: 0.7621749043464661, accuracy: 0.6870458126068115\n",
      "Batch 86 loss: 0.9042482972145081, accuracy: 0.6865122318267822\n",
      "Batch 87 loss: 0.8962504863739014, accuracy: 0.6856356263160706\n",
      "Batch 88 loss: 1.0301634073257446, accuracy: 0.6851299405097961\n",
      "Batch 89 loss: 0.8349376916885376, accuracy: 0.6852430701255798\n",
      "Batch 90 loss: 0.7984903454780579, accuracy: 0.6857829689979553\n",
      "Batch 91 loss: 0.9136538505554199, accuracy: 0.6854619383811951\n",
      "Batch 92 loss: 0.8640486598014832, accuracy: 0.6850638389587402\n",
      "Batch 93 loss: 0.779477059841156, accuracy: 0.6850897669792175\n",
      "Batch 94 loss: 0.8995866775512695, accuracy: 0.6850329041481018\n",
      "Batch 95 loss: 0.7192636728286743, accuracy: 0.6854655146598816\n",
      "Batch 96 loss: 0.826394259929657, accuracy: 0.6856475472450256\n",
      "Batch 97 loss: 0.7521321773529053, accuracy: 0.6859056353569031\n",
      "Batch 98 loss: 0.9760531187057495, accuracy: 0.684974730014801\n",
      "Batch 99 loss: 0.8157987594604492, accuracy: 0.6850000023841858\n",
      "Batch 100 loss: 0.7680827975273132, accuracy: 0.6854888796806335\n",
      "Batch 101 loss: 0.9248138666152954, accuracy: 0.6852787733078003\n",
      "Batch 102 loss: 0.8804081082344055, accuracy: 0.6853762269020081\n",
      "Batch 103 loss: 0.9070419073104858, accuracy: 0.6853966116905212\n",
      "Batch 104 loss: 0.9526397585868835, accuracy: 0.6851934790611267\n",
      "Batch 105 loss: 0.7539055347442627, accuracy: 0.6855837106704712\n",
      "Batch 106 loss: 0.7957027554512024, accuracy: 0.6857476830482483\n",
      "Batch 107 loss: 0.697706937789917, accuracy: 0.6861255764961243\n",
      "Batch 108 loss: 0.8093640208244324, accuracy: 0.6864965558052063\n",
      "Batch 109 loss: 0.9062930345535278, accuracy: 0.6862215995788574\n",
      "Batch 110 loss: 0.7866924405097961, accuracy: 0.6864442825317383\n",
      "Batch 111 loss: 0.7490920424461365, accuracy: 0.6869419813156128\n",
      "Batch 112 loss: 0.8163272142410278, accuracy: 0.6867395043373108\n",
      "Batch 113 loss: 0.8180365562438965, accuracy: 0.6868146657943726\n",
      "Batch 114 loss: 0.910430908203125, accuracy: 0.686345100402832\n",
      "Batch 115 loss: 0.8990970849990845, accuracy: 0.6862203478813171\n",
      "Batch 116 loss: 0.838417649269104, accuracy: 0.6861645579338074\n",
      "Batch 117 loss: 0.8811241984367371, accuracy: 0.6863744854927063\n",
      "Batch 118 loss: 0.7872607111930847, accuracy: 0.6865808963775635\n",
      "Batch 119 loss: 0.7201555371284485, accuracy: 0.687304675579071\n",
      "Batch 120 loss: 0.9653822183609009, accuracy: 0.6869189143180847\n",
      "Batch 121 loss: 0.8284083008766174, accuracy: 0.6871798038482666\n",
      "Batch 122 loss: 0.7579774260520935, accuracy: 0.6878175735473633\n",
      "Batch 123 loss: 0.9721900224685669, accuracy: 0.6871849894523621\n",
      "Batch 124 loss: 0.80250483751297, accuracy: 0.6868749856948853\n",
      "Batch 125 loss: 0.9477601647377014, accuracy: 0.6861358880996704\n",
      "Batch 126 loss: 0.6974018216133118, accuracy: 0.6866387724876404\n",
      "Batch 127 loss: 0.7839614748954773, accuracy: 0.68695068359375\n",
      "Batch 128 loss: 0.8121078610420227, accuracy: 0.6870155334472656\n",
      "Batch 129 loss: 0.8461183309555054, accuracy: 0.6872596144676208\n",
      "Batch 130 loss: 0.8697963953018188, accuracy: 0.6871421933174133\n",
      "Batch 131 loss: 0.7699630856513977, accuracy: 0.6873224377632141\n",
      "Batch 132 loss: 0.7419829964637756, accuracy: 0.6875587701797485\n",
      "Batch 133 loss: 0.7961477637290955, accuracy: 0.6880246996879578\n",
      "Batch 134 loss: 0.796058177947998, accuracy: 0.6884258985519409\n",
      "Batch 135 loss: 0.7251022458076477, accuracy: 0.6888786554336548\n",
      "Batch 136 loss: 0.8397001624107361, accuracy: 0.6892107725143433\n",
      "Batch 137 loss: 1.0002152919769287, accuracy: 0.6885756254196167\n",
      "Batch 138 loss: 0.8190822005271912, accuracy: 0.6888489127159119\n",
      "Batch 139 loss: 0.6973514556884766, accuracy: 0.6890066862106323\n",
      "Batch 140 loss: 0.7774934768676758, accuracy: 0.689383864402771\n",
      "Batch 141 loss: 0.8147104382514954, accuracy: 0.689700722694397\n",
      "Batch 142 loss: 0.8954658508300781, accuracy: 0.6894121766090393\n",
      "Batch 143 loss: 0.8816512227058411, accuracy: 0.6891818642616272\n",
      "Batch 144 loss: 0.8793858885765076, accuracy: 0.6890085935592651\n",
      "Batch 145 loss: 0.7915250062942505, accuracy: 0.688998281955719\n",
      "Batch 146 loss: 0.740817129611969, accuracy: 0.6889880895614624\n",
      "Batch 147 loss: 0.7053024768829346, accuracy: 0.6891363859176636\n",
      "Batch 148 loss: 0.7890205979347229, accuracy: 0.6893875598907471\n",
      "Batch 149 loss: 0.849028468132019, accuracy: 0.6894270777702332\n",
      "Batch 150 loss: 0.8216472864151001, accuracy: 0.6894660592079163\n",
      "Batch 151 loss: 0.9733196496963501, accuracy: 0.6891447305679321\n",
      "Batch 152 loss: 0.7741933465003967, accuracy: 0.6892361044883728\n",
      "Batch 153 loss: 0.8436758518218994, accuracy: 0.6892248392105103\n",
      "Batch 154 loss: 0.7678431272506714, accuracy: 0.6895665526390076\n",
      "Batch 155 loss: 0.9750884771347046, accuracy: 0.6892527937889099\n",
      "Batch 156 loss: 0.890051007270813, accuracy: 0.6890923380851746\n",
      "Batch 157 loss: 0.8959175944328308, accuracy: 0.6890822649002075\n",
      "Batch 158 loss: 0.891362726688385, accuracy: 0.6889249086380005\n",
      "Batch 159 loss: 0.7377824783325195, accuracy: 0.68896484375\n",
      "Batch 160 loss: 0.8486766815185547, accuracy: 0.6890528202056885\n",
      "Batch 161 loss: 0.6737337708473206, accuracy: 0.6893325448036194\n",
      "Batch 162 loss: 0.7820901870727539, accuracy: 0.6893213391304016\n",
      "Batch 163 loss: 0.8309116959571838, accuracy: 0.6893578767776489\n",
      "Batch 164 loss: 0.9246904253959656, accuracy: 0.6888731122016907\n",
      "Batch 165 loss: 0.8541243672370911, accuracy: 0.6889589428901672\n",
      "Batch 166 loss: 0.6480669975280762, accuracy: 0.6893244981765747\n",
      "Batch 167 loss: 0.7164897322654724, accuracy: 0.6895461082458496\n",
      "Batch 168 loss: 0.885613203048706, accuracy: 0.6896264553070068\n",
      "Batch 169 loss: 0.9836518168449402, accuracy: 0.689338207244873\n",
      "Batch 170 loss: 0.8409786224365234, accuracy: 0.6892361044883728\n",
      "Batch 171 loss: 0.9316450357437134, accuracy: 0.6889989376068115\n",
      "Batch 172 loss: 0.8476604223251343, accuracy: 0.6889451146125793\n",
      "Batch 173 loss: 0.6557755470275879, accuracy: 0.6892959475517273\n",
      "Batch 174 loss: 0.8591586351394653, accuracy: 0.6891071200370789\n",
      "Batch 175 loss: 0.8404064774513245, accuracy: 0.6887872815132141\n",
      "Batch 176 loss: 0.8000571131706238, accuracy: 0.6888241767883301\n",
      "Batch 177 loss: 0.7499837875366211, accuracy: 0.6890800595283508\n",
      "Batch 178 loss: 0.9359180331230164, accuracy: 0.6889402866363525\n",
      "Batch 179 loss: 0.9210560917854309, accuracy: 0.6887152791023254\n",
      "Batch 180 loss: 0.6947242617607117, accuracy: 0.6886653900146484\n",
      "Batch 181 loss: 0.814170241355896, accuracy: 0.6888736486434937\n",
      "Batch 182 loss: 0.8689167499542236, accuracy: 0.688866138458252\n",
      "Batch 183 loss: 0.875154435634613, accuracy: 0.68873131275177\n",
      "Batch 184 loss: 0.7506119012832642, accuracy: 0.6888513565063477\n",
      "Batch 185 loss: 0.852802038192749, accuracy: 0.6888440847396851\n",
      "Batch 186 loss: 0.8852971196174622, accuracy: 0.6887533664703369\n",
      "Batch 187 loss: 0.8507929444313049, accuracy: 0.6885804533958435\n",
      "Batch 188 loss: 0.8957164287567139, accuracy: 0.6882854104042053\n",
      "Batch 189 loss: 0.8821754455566406, accuracy: 0.6881167888641357\n",
      "Batch 190 loss: 0.94158536195755, accuracy: 0.6878681182861328\n",
      "Batch 191 loss: 0.8471443057060242, accuracy: 0.68798828125\n",
      "Batch 192 loss: 0.7934128642082214, accuracy: 0.6879857778549194\n",
      "Batch 193 loss: 0.9235261678695679, accuracy: 0.6877416372299194\n",
      "Batch 194 loss: 0.7522249817848206, accuracy: 0.6879807710647583\n",
      "Batch 195 loss: 0.9229555130004883, accuracy: 0.6879783272743225\n",
      "Batch 196 loss: 0.8603898882865906, accuracy: 0.687856912612915\n",
      "Batch 197 loss: 0.824870228767395, accuracy: 0.6878945827484131\n",
      "Batch 198 loss: 0.9530328512191772, accuracy: 0.687539279460907\n",
      "Batch 199 loss: 0.9033649563789368, accuracy: 0.6875\n",
      "Batch 200 loss: 0.9836832880973816, accuracy: 0.6873834133148193\n",
      "Batch 201 loss: 0.7709226608276367, accuracy: 0.6875773668289185\n",
      "Batch 202 loss: 1.0400819778442383, accuracy: 0.6871921420097351\n",
      "Batch 203 loss: 0.8075105547904968, accuracy: 0.6871170401573181\n",
      "Batch 204 loss: 0.8272580504417419, accuracy: 0.6871188879013062\n",
      "Batch 205 loss: 0.8658507466316223, accuracy: 0.6871587038040161\n",
      "Batch 206 loss: 0.9721409678459167, accuracy: 0.6867828965187073\n",
      "Batch 207 loss: 1.0016697645187378, accuracy: 0.6865234375\n",
      "Batch 208 loss: 0.831093966960907, accuracy: 0.6867523789405823\n",
      "Batch 209 loss: 0.8846027255058289, accuracy: 0.6867931485176086\n",
      "Batch 210 loss: 0.8047674894332886, accuracy: 0.687018632888794\n",
      "Batch 211 loss: 0.7469823956489563, accuracy: 0.6870946288108826\n",
      "Batch 212 loss: 0.8059051036834717, accuracy: 0.6870231628417969\n",
      "Batch 213 loss: 0.839130163192749, accuracy: 0.6873174905776978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 214 loss: 0.6720430850982666, accuracy: 0.6876816749572754\n",
      "Batch 215 loss: 0.7710784673690796, accuracy: 0.6878616809844971\n",
      "Batch 216 loss: 0.9084730744361877, accuracy: 0.6878600120544434\n",
      "Batch 217 loss: 0.894929826259613, accuracy: 0.6878942251205444\n",
      "Batch 218 loss: 0.9013763666152954, accuracy: 0.6876783967018127\n",
      "Batch 219 loss: 0.8842279314994812, accuracy: 0.6875355243682861\n",
      "Batch 220 loss: 0.7950129508972168, accuracy: 0.6875\n",
      "Batch 221 loss: 0.8060577511787415, accuracy: 0.6874296069145203\n",
      "Batch 222 loss: 0.896738588809967, accuracy: 0.6872897744178772\n",
      "Batch 223 loss: 0.8725901246070862, accuracy: 0.6872907280921936\n",
      "Batch 224 loss: 0.8186336159706116, accuracy: 0.6872409582138062\n",
      "Training epoch: 26, train accuracy: 68.72409057617188, train loss: 0.8341943340831333, valid accuracy: 62.607967376708984, valid loss: 1.0735173410382763 \n",
      "Batch 0 loss: 0.7169212698936462, accuracy: 0.765625\n",
      "Batch 1 loss: 0.8624157309532166, accuracy: 0.71484375\n",
      "Batch 2 loss: 0.6615143418312073, accuracy: 0.7265625\n",
      "Batch 3 loss: 0.8639822006225586, accuracy: 0.72265625\n",
      "Batch 4 loss: 0.7557312250137329, accuracy: 0.7171875238418579\n",
      "Batch 5 loss: 0.87433260679245, accuracy: 0.69921875\n",
      "Batch 6 loss: 0.7850136160850525, accuracy: 0.6975446343421936\n",
      "Batch 7 loss: 0.6714212894439697, accuracy: 0.705078125\n",
      "Batch 8 loss: 0.8369523882865906, accuracy: 0.7022569179534912\n",
      "Batch 9 loss: 0.8145607709884644, accuracy: 0.703125\n",
      "Batch 10 loss: 0.7668461203575134, accuracy: 0.7038352489471436\n",
      "Batch 11 loss: 0.9011198282241821, accuracy: 0.7005208134651184\n",
      "Batch 12 loss: 0.921268105506897, accuracy: 0.6953125\n",
      "Batch 13 loss: 0.8845194578170776, accuracy: 0.6930803656578064\n",
      "Batch 14 loss: 0.9180079698562622, accuracy: 0.6916666626930237\n",
      "Batch 15 loss: 0.7688242197036743, accuracy: 0.6904296875\n",
      "Batch 16 loss: 0.8015825152397156, accuracy: 0.6884191036224365\n",
      "Batch 17 loss: 0.6878923773765564, accuracy: 0.6905381679534912\n",
      "Batch 18 loss: 0.7097111344337463, accuracy: 0.6916118264198303\n",
      "Batch 19 loss: 0.7786955833435059, accuracy: 0.690234363079071\n",
      "Batch 20 loss: 0.7337363958358765, accuracy: 0.691220223903656\n",
      "Batch 21 loss: 0.7469185590744019, accuracy: 0.6921164989471436\n",
      "Batch 22 loss: 0.8326493501663208, accuracy: 0.69191575050354\n",
      "Batch 23 loss: 0.7188897728919983, accuracy: 0.6930338740348816\n",
      "Batch 24 loss: 0.775431752204895, accuracy: 0.6928125023841858\n",
      "Batch 25 loss: 0.8717444539070129, accuracy: 0.6932091116905212\n",
      "Batch 26 loss: 0.7207797765731812, accuracy: 0.6956018805503845\n",
      "Batch 27 loss: 0.8654826879501343, accuracy: 0.693359375\n",
      "Batch 28 loss: 0.7878465056419373, accuracy: 0.6945043206214905\n",
      "Batch 29 loss: 0.7042140960693359, accuracy: 0.6963541507720947\n",
      "Batch 30 loss: 0.7120369076728821, accuracy: 0.6973286271095276\n",
      "Batch 31 loss: 0.7291942834854126, accuracy: 0.697021484375\n",
      "Batch 32 loss: 0.6933354139328003, accuracy: 0.6979166865348816\n",
      "Batch 33 loss: 0.9369823336601257, accuracy: 0.6941636204719543\n",
      "Batch 34 loss: 0.6353052258491516, accuracy: 0.6962053775787354\n",
      "Batch 35 loss: 0.6954577565193176, accuracy: 0.6994357705116272\n",
      "Batch 36 loss: 0.8392232060432434, accuracy: 0.6993243098258972\n",
      "Batch 37 loss: 0.8810954093933105, accuracy: 0.6986019611358643\n",
      "Batch 38 loss: 0.8660883903503418, accuracy: 0.6981169581413269\n",
      "Batch 39 loss: 0.6662280559539795, accuracy: 0.700976550579071\n",
      "Batch 40 loss: 0.8378837704658508, accuracy: 0.7006478905677795\n",
      "Batch 41 loss: 0.8709205389022827, accuracy: 0.7001488208770752\n",
      "Batch 42 loss: 0.8594731688499451, accuracy: 0.6993095874786377\n",
      "Batch 43 loss: 0.7540115118026733, accuracy: 0.7004616260528564\n",
      "Batch 44 loss: 0.7692939639091492, accuracy: 0.7017360925674438\n",
      "Batch 45 loss: 0.8854806423187256, accuracy: 0.70142662525177\n",
      "Batch 46 loss: 0.7407809495925903, accuracy: 0.702293872833252\n",
      "Batch 47 loss: 0.7497138977050781, accuracy: 0.7019856572151184\n",
      "Batch 48 loss: 0.8133761882781982, accuracy: 0.702168345451355\n",
      "Batch 49 loss: 0.8307587504386902, accuracy: 0.7017187476158142\n",
      "Batch 50 loss: 0.7247108817100525, accuracy: 0.703125\n",
      "Batch 51 loss: 0.7420415282249451, accuracy: 0.7025240659713745\n",
      "Batch 52 loss: 0.9068586230278015, accuracy: 0.7019457817077637\n",
      "Batch 53 loss: 0.9527273774147034, accuracy: 0.7005208134651184\n",
      "Batch 54 loss: 0.7904070615768433, accuracy: 0.6997159123420715\n",
      "Batch 55 loss: 0.8752861618995667, accuracy: 0.7000557780265808\n",
      "Batch 56 loss: 0.895427942276001, accuracy: 0.6998355388641357\n",
      "Batch 57 loss: 0.776767909526825, accuracy: 0.7002963423728943\n",
      "Batch 58 loss: 0.7620875239372253, accuracy: 0.6999470591545105\n",
      "Batch 59 loss: 0.8968994617462158, accuracy: 0.6990885138511658\n",
      "Batch 60 loss: 0.9502886533737183, accuracy: 0.6976178288459778\n",
      "Batch 61 loss: 0.8193985223770142, accuracy: 0.6973286271095276\n",
      "Batch 62 loss: 0.7365188002586365, accuracy: 0.6974206566810608\n",
      "Batch 63 loss: 0.8166826367378235, accuracy: 0.697509765625\n",
      "Batch 64 loss: 0.8267267942428589, accuracy: 0.6971153616905212\n",
      "Batch 65 loss: 0.893621027469635, accuracy: 0.6963778138160706\n",
      "Batch 66 loss: 0.8749248385429382, accuracy: 0.6955457329750061\n",
      "Batch 67 loss: 0.8460641503334045, accuracy: 0.6950827240943909\n",
      "Batch 68 loss: 0.8367265462875366, accuracy: 0.6955389380455017\n",
      "Batch 69 loss: 0.8428168296813965, accuracy: 0.6950892806053162\n",
      "Batch 70 loss: 0.7085126042366028, accuracy: 0.6956425905227661\n",
      "Batch 71 loss: 0.6909928917884827, accuracy: 0.6961805820465088\n",
      "Batch 72 loss: 0.8418418169021606, accuracy: 0.6965967416763306\n",
      "Batch 73 loss: 0.8195309042930603, accuracy: 0.6968961358070374\n",
      "Batch 74 loss: 0.7095102667808533, accuracy: 0.6977083086967468\n",
      "Batch 75 loss: 0.8460442423820496, accuracy: 0.6976768374443054\n",
      "Batch 76 loss: 0.779884934425354, accuracy: 0.6977475881576538\n",
      "Batch 77 loss: 0.8261340260505676, accuracy: 0.6978164911270142\n",
      "Batch 78 loss: 0.831900417804718, accuracy: 0.6975870132446289\n",
      "Batch 79 loss: 0.9337959289550781, accuracy: 0.69775390625\n",
      "Batch 80 loss: 0.7053030133247375, accuracy: 0.6983989477157593\n",
      "Batch 81 loss: 0.6216699481010437, accuracy: 0.6990281939506531\n",
      "Batch 82 loss: 0.886166512966156, accuracy: 0.6990775465965271\n",
      "Batch 83 loss: 0.943882942199707, accuracy: 0.6982886791229248\n",
      "Batch 84 loss: 0.7962741255760193, accuracy: 0.6983456015586853\n",
      "Batch 85 loss: 0.9279435276985168, accuracy: 0.6983103156089783\n",
      "Batch 86 loss: 0.7878691554069519, accuracy: 0.6983656883239746\n",
      "Batch 87 loss: 0.7932391166687012, accuracy: 0.6977983117103577\n",
      "Batch 88 loss: 0.7542362213134766, accuracy: 0.6976825594902039\n",
      "Batch 89 loss: 0.9819556474685669, accuracy: 0.6969618201255798\n",
      "Batch 90 loss: 0.8435046076774597, accuracy: 0.6972012519836426\n",
      "Batch 91 loss: 0.9318877458572388, accuracy: 0.6965013742446899\n",
      "Batch 92 loss: 0.7633824348449707, accuracy: 0.6966565847396851\n",
      "Batch 93 loss: 0.9955982565879822, accuracy: 0.6954787373542786\n",
      "Batch 94 loss: 0.875885009765625, accuracy: 0.69539475440979\n",
      "Batch 95 loss: 0.9117313623428345, accuracy: 0.6949055790901184\n",
      "Batch 96 loss: 0.9817238450050354, accuracy: 0.694668173789978\n",
      "Batch 97 loss: 0.8021371364593506, accuracy: 0.6945152878761292\n",
      "Batch 98 loss: 0.7886717915534973, accuracy: 0.6946022510528564\n",
      "Batch 99 loss: 0.9692977070808411, accuracy: 0.6942187547683716\n",
      "Batch 100 loss: 0.8575085997581482, accuracy: 0.6933013796806335\n",
      "Batch 101 loss: 0.853859007358551, accuracy: 0.6930146813392639\n",
      "Batch 102 loss: 0.7721447944641113, accuracy: 0.6933404207229614\n",
      "Batch 103 loss: 0.8305299878120422, accuracy: 0.693359375\n",
      "Batch 104 loss: 0.8472777009010315, accuracy: 0.6934523582458496\n",
      "Batch 105 loss: 0.7783833146095276, accuracy: 0.6934699416160583\n",
      "Batch 106 loss: 0.8778990507125854, accuracy: 0.6935601830482483\n",
      "Batch 107 loss: 0.8334335088729858, accuracy: 0.6934317350387573\n",
      "Batch 108 loss: 0.9400136470794678, accuracy: 0.6928039193153381\n",
      "Batch 109 loss: 0.8624645471572876, accuracy: 0.6924715638160706\n",
      "Batch 110 loss: 0.8043142557144165, accuracy: 0.6925675868988037\n",
      "Batch 111 loss: 0.7132977843284607, accuracy: 0.69287109375\n",
      "Batch 112 loss: 0.8230937719345093, accuracy: 0.693238377571106\n",
      "Batch 113 loss: 0.8923084735870361, accuracy: 0.6930509805679321\n",
      "Batch 114 loss: 0.8181779384613037, accuracy: 0.6932065486907959\n",
      "Batch 115 loss: 0.8909496665000916, accuracy: 0.6929552555084229\n",
      "Batch 116 loss: 0.8614949584007263, accuracy: 0.69264155626297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 117 loss: 0.8229976296424866, accuracy: 0.6923331618309021\n",
      "Batch 118 loss: 0.8074018955230713, accuracy: 0.6922268867492676\n",
      "Batch 119 loss: 0.996896505355835, accuracy: 0.6917317509651184\n",
      "Batch 120 loss: 0.8583008050918579, accuracy: 0.6917613744735718\n",
      "Batch 121 loss: 0.7803982496261597, accuracy: 0.6920466423034668\n",
      "Batch 122 loss: 0.7942362427711487, accuracy: 0.6923907399177551\n",
      "Batch 123 loss: 0.7585716843605042, accuracy: 0.6926663517951965\n",
      "Batch 124 loss: 0.7522547245025635, accuracy: 0.6926875114440918\n",
      "Batch 125 loss: 0.6940253973007202, accuracy: 0.6935144066810608\n",
      "Batch 126 loss: 0.7562382221221924, accuracy: 0.6937130689620972\n",
      "Batch 127 loss: 0.7443352341651917, accuracy: 0.69384765625\n",
      "Batch 128 loss: 0.6700629591941833, accuracy: 0.6943435072898865\n",
      "Batch 129 loss: 0.8003985285758972, accuracy: 0.6946514248847961\n",
      "Batch 130 loss: 0.6746485233306885, accuracy: 0.6950739622116089\n",
      "Batch 131 loss: 0.7275198698043823, accuracy: 0.6956676244735718\n",
      "Batch 132 loss: 0.8040208220481873, accuracy: 0.6957237124443054\n",
      "Batch 133 loss: 0.748194694519043, accuracy: 0.6958371996879578\n",
      "Batch 134 loss: 0.8795418739318848, accuracy: 0.6954861283302307\n",
      "Batch 135 loss: 0.7906994819641113, accuracy: 0.695197582244873\n",
      "Batch 136 loss: 0.8409871459007263, accuracy: 0.6951414346694946\n",
      "Batch 137 loss: 0.869800865650177, accuracy: 0.6947463750839233\n",
      "Batch 138 loss: 0.749796450138092, accuracy: 0.6945256590843201\n",
      "Batch 139 loss: 0.8292911052703857, accuracy: 0.6945870518684387\n",
      "Batch 140 loss: 0.8381658792495728, accuracy: 0.6947030425071716\n",
      "Batch 141 loss: 0.7214539051055908, accuracy: 0.6948723793029785\n",
      "Batch 142 loss: 0.7756791114807129, accuracy: 0.6948208212852478\n",
      "Batch 143 loss: 0.9232543706893921, accuracy: 0.6942816972732544\n",
      "Batch 144 loss: 0.711020290851593, accuracy: 0.6945582032203674\n",
      "Batch 145 loss: 0.7237568497657776, accuracy: 0.6950449347496033\n",
      "Batch 146 loss: 0.8822740912437439, accuracy: 0.6946216225624084\n",
      "Batch 147 loss: 0.8593056797981262, accuracy: 0.6943623423576355\n",
      "Batch 148 loss: 0.8268373608589172, accuracy: 0.6946308612823486\n",
      "Batch 149 loss: 0.7325055599212646, accuracy: 0.6952604055404663\n",
      "Batch 150 loss: 0.8165798187255859, accuracy: 0.695260763168335\n",
      "Batch 151 loss: 0.8802216053009033, accuracy: 0.6952611207962036\n",
      "Batch 152 loss: 0.8170745968818665, accuracy: 0.6956188678741455\n",
      "Batch 153 loss: 0.8043875098228455, accuracy: 0.6957690715789795\n",
      "Batch 154 loss: 0.7291615009307861, accuracy: 0.6957157254219055\n",
      "Batch 155 loss: 0.963937520980835, accuracy: 0.6954126358032227\n",
      "Batch 156 loss: 0.7454860210418701, accuracy: 0.695511519908905\n",
      "Batch 157 loss: 0.8121610283851624, accuracy: 0.695460855960846\n",
      "Batch 158 loss: 0.829913318157196, accuracy: 0.6952633857727051\n",
      "Batch 159 loss: 0.8303124308586121, accuracy: 0.695361316204071\n",
      "Batch 160 loss: 0.8838814496994019, accuracy: 0.6952154636383057\n",
      "Batch 161 loss: 0.9409601092338562, accuracy: 0.6948302388191223\n",
      "Batch 162 loss: 0.6954519748687744, accuracy: 0.6950249075889587\n",
      "Batch 163 loss: 0.878982424736023, accuracy: 0.6950743198394775\n",
      "Batch 164 loss: 0.9070483446121216, accuracy: 0.6947442889213562\n",
      "Batch 165 loss: 0.8828833103179932, accuracy: 0.6947006583213806\n",
      "Batch 166 loss: 0.7058677673339844, accuracy: 0.694797933101654\n",
      "Batch 167 loss: 0.7946292161941528, accuracy: 0.6946614384651184\n",
      "Batch 168 loss: 0.8540455102920532, accuracy: 0.6946653127670288\n",
      "Batch 169 loss: 0.7040399312973022, accuracy: 0.6948529481887817\n",
      "Batch 170 loss: 0.8129309415817261, accuracy: 0.6946728825569153\n",
      "Batch 171 loss: 0.8370427489280701, accuracy: 0.6944040656089783\n",
      "Batch 172 loss: 0.8793826103210449, accuracy: 0.6943190097808838\n",
      "Batch 173 loss: 0.8273636102676392, accuracy: 0.6941900253295898\n",
      "Batch 174 loss: 0.7933121919631958, accuracy: 0.6942410469055176\n",
      "Batch 175 loss: 0.7685340046882629, accuracy: 0.6942471861839294\n",
      "Batch 176 loss: 0.9507376551628113, accuracy: 0.6940765976905823\n",
      "Batch 177 loss: 0.7490720152854919, accuracy: 0.6941713690757751\n",
      "Batch 178 loss: 0.7876473069190979, accuracy: 0.6940904259681702\n",
      "Batch 179 loss: 0.859565019607544, accuracy: 0.694140613079071\n",
      "Batch 180 loss: 0.8558385968208313, accuracy: 0.6940175890922546\n",
      "Batch 181 loss: 0.868980348110199, accuracy: 0.6938530206680298\n",
      "Batch 182 loss: 0.8195432424545288, accuracy: 0.693946361541748\n",
      "Batch 183 loss: 0.9583781957626343, accuracy: 0.693359375\n",
      "Batch 184 loss: 1.0078105926513672, accuracy: 0.6929054260253906\n",
      "Batch 185 loss: 0.8213359117507935, accuracy: 0.6927083134651184\n",
      "Batch 186 loss: 0.8417980074882507, accuracy: 0.6922627091407776\n",
      "Batch 187 loss: 0.7647722363471985, accuracy: 0.6925697922706604\n",
      "Batch 188 loss: 0.7902376055717468, accuracy: 0.6925843358039856\n",
      "Batch 189 loss: 0.8058083653450012, accuracy: 0.6926808953285217\n",
      "Batch 190 loss: 0.8420592546463013, accuracy: 0.692612886428833\n",
      "Batch 191 loss: 0.744679868221283, accuracy: 0.6930338740348816\n",
      "Batch 192 loss: 0.7889212965965271, accuracy: 0.6931266188621521\n",
      "Batch 193 loss: 0.7998487949371338, accuracy: 0.6931781768798828\n",
      "Batch 194 loss: 0.9603878259658813, accuracy: 0.6927083134651184\n",
      "Batch 195 loss: 1.0106910467147827, accuracy: 0.692362904548645\n",
      "Batch 196 loss: 0.7191882133483887, accuracy: 0.6927744150161743\n",
      "Batch 197 loss: 0.9034134745597839, accuracy: 0.6929450631141663\n",
      "Batch 198 loss: 0.8782029747962952, accuracy: 0.692721426486969\n",
      "Batch 199 loss: 0.8136628866195679, accuracy: 0.6928125023841858\n",
      "Batch 200 loss: 0.9008371233940125, accuracy: 0.6926694512367249\n",
      "Batch 201 loss: 0.8215883374214172, accuracy: 0.6926438808441162\n",
      "Batch 202 loss: 0.6854826211929321, accuracy: 0.6929264068603516\n",
      "Batch 203 loss: 0.9725673794746399, accuracy: 0.6925934553146362\n",
      "Batch 204 loss: 0.583842933177948, accuracy: 0.6929115653038025\n",
      "Batch 205 loss: 0.8656529188156128, accuracy: 0.6929232478141785\n",
      "Batch 206 loss: 0.7641543745994568, accuracy: 0.6931235194206238\n",
      "Batch 207 loss: 1.0015547275543213, accuracy: 0.692946195602417\n",
      "Batch 208 loss: 0.7705855965614319, accuracy: 0.6930323243141174\n",
      "Batch 209 loss: 0.8362281322479248, accuracy: 0.6927827596664429\n",
      "Batch 210 loss: 0.8110546469688416, accuracy: 0.6928317546844482\n",
      "Batch 211 loss: 0.8043700456619263, accuracy: 0.6929908394813538\n",
      "Batch 212 loss: 0.6542719006538391, accuracy: 0.6933318376541138\n",
      "Batch 213 loss: 0.856415331363678, accuracy: 0.6932681202888489\n",
      "Batch 214 loss: 0.7358720302581787, accuracy: 0.6936410069465637\n",
      "Batch 215 loss: 0.9213417768478394, accuracy: 0.6934678554534912\n",
      "Batch 216 loss: 0.8656736612319946, accuracy: 0.6932603716850281\n",
      "Batch 217 loss: 0.7573161125183105, accuracy: 0.6933414340019226\n",
      "Batch 218 loss: 0.6766835451126099, accuracy: 0.6937071681022644\n",
      "Batch 219 loss: 0.7992592453956604, accuracy: 0.6938210129737854\n",
      "Batch 220 loss: 0.7807742953300476, accuracy: 0.6940045356750488\n",
      "Batch 221 loss: 0.7636137008666992, accuracy: 0.6942567825317383\n",
      "Batch 222 loss: 0.8589612245559692, accuracy: 0.6939111351966858\n",
      "Batch 223 loss: 0.8944568634033203, accuracy: 0.6938127875328064\n",
      "Batch 224 loss: 0.8078109622001648, accuracy: 0.6937894225120544\n",
      "Training epoch: 27, train accuracy: 69.37893676757812, train loss: 0.8162386782964071, valid accuracy: 63.72248458862305, valid loss: 1.0823768356750751 \n",
      "Batch 0 loss: 0.8472833633422852, accuracy: 0.7109375\n",
      "Batch 1 loss: 0.8364229202270508, accuracy: 0.703125\n",
      "Batch 2 loss: 0.8392108082771301, accuracy: 0.7135416865348816\n",
      "Batch 3 loss: 0.6871874928474426, accuracy: 0.71484375\n",
      "Batch 4 loss: 0.797299861907959, accuracy: 0.707812488079071\n",
      "Batch 5 loss: 0.8212131261825562, accuracy: 0.7057291865348816\n",
      "Batch 6 loss: 0.8243298530578613, accuracy: 0.7008928656578064\n",
      "Batch 7 loss: 0.9347919821739197, accuracy: 0.6962890625\n",
      "Batch 8 loss: 0.7707234025001526, accuracy: 0.7005208134651184\n",
      "Batch 9 loss: 0.7078811526298523, accuracy: 0.703125\n",
      "Batch 10 loss: 0.8424000144004822, accuracy: 0.7002840638160706\n",
      "Batch 11 loss: 0.7646159529685974, accuracy: 0.7024739384651184\n",
      "Batch 12 loss: 0.8678246140480042, accuracy: 0.6977163553237915\n",
      "Batch 13 loss: 0.955345630645752, accuracy: 0.6947544813156128\n",
      "Batch 14 loss: 0.7774960398674011, accuracy: 0.6973958611488342\n",
      "Batch 15 loss: 0.7462299466133118, accuracy: 0.6962890625\n",
      "Batch 16 loss: 0.6809290051460266, accuracy: 0.6976103186607361\n",
      "Batch 17 loss: 0.8121223449707031, accuracy: 0.69921875\n",
      "Batch 18 loss: 1.010993242263794, accuracy: 0.6957237124443054\n",
      "Batch 19 loss: 0.7062132358551025, accuracy: 0.696484386920929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 loss: 0.7804798483848572, accuracy: 0.6968005895614624\n",
      "Batch 21 loss: 0.9285531640052795, accuracy: 0.6956676244735718\n",
      "Batch 22 loss: 0.9181702136993408, accuracy: 0.693953812122345\n",
      "Batch 23 loss: 0.7720794081687927, accuracy: 0.6927083134651184\n",
      "Batch 24 loss: 0.8643778562545776, accuracy: 0.6918749809265137\n",
      "Batch 25 loss: 0.8340224027633667, accuracy: 0.6935096383094788\n",
      "Batch 26 loss: 0.8498660326004028, accuracy: 0.6932870149612427\n",
      "Batch 27 loss: 0.8711117506027222, accuracy: 0.6908482313156128\n",
      "Batch 28 loss: 0.9150410294532776, accuracy: 0.6915409564971924\n",
      "Batch 29 loss: 0.6549630761146545, accuracy: 0.6940104365348816\n",
      "Batch 30 loss: 0.9959915280342102, accuracy: 0.6912802457809448\n",
      "Batch 31 loss: 0.8466044068336487, accuracy: 0.69189453125\n",
      "Batch 32 loss: 0.6314934492111206, accuracy: 0.6946022510528564\n",
      "Batch 33 loss: 0.8443774580955505, accuracy: 0.6941636204719543\n",
      "Batch 34 loss: 0.7808651924133301, accuracy: 0.6935268044471741\n",
      "Batch 35 loss: 0.6701814532279968, accuracy: 0.6950954794883728\n",
      "Batch 36 loss: 0.9806649684906006, accuracy: 0.6946790814399719\n",
      "Batch 37 loss: 0.7528924942016602, accuracy: 0.6953125\n",
      "Batch 38 loss: 0.7219221591949463, accuracy: 0.6965144276618958\n",
      "Batch 39 loss: 0.7442330121994019, accuracy: 0.696484386920929\n",
      "Batch 40 loss: 0.7698363065719604, accuracy: 0.6958841681480408\n",
      "Batch 41 loss: 0.7644469738006592, accuracy: 0.6966145634651184\n",
      "Batch 42 loss: 0.7111993432044983, accuracy: 0.6973110437393188\n",
      "Batch 43 loss: 0.8509794473648071, accuracy: 0.6956676244735718\n",
      "Batch 44 loss: 0.9185653924942017, accuracy: 0.6953125\n",
      "Batch 45 loss: 0.8459797501564026, accuracy: 0.6954823136329651\n",
      "Batch 46 loss: 0.6950539350509644, accuracy: 0.696143627166748\n",
      "Batch 47 loss: 0.8261575102806091, accuracy: 0.6954752802848816\n",
      "Batch 48 loss: 0.7539343237876892, accuracy: 0.6957908272743225\n",
      "Batch 49 loss: 0.714608907699585, accuracy: 0.696093738079071\n",
      "Batch 50 loss: 0.8261005282402039, accuracy: 0.6968443393707275\n",
      "Batch 51 loss: 0.8690811991691589, accuracy: 0.6966646909713745\n",
      "Batch 52 loss: 0.6819459795951843, accuracy: 0.6969339847564697\n",
      "Batch 53 loss: 0.7314283847808838, accuracy: 0.6977719664573669\n",
      "Batch 54 loss: 0.7096287608146667, accuracy: 0.6985795497894287\n",
      "Batch 55 loss: 0.8088266253471375, accuracy: 0.6982421875\n",
      "Batch 56 loss: 0.7944077253341675, accuracy: 0.6983278393745422\n",
      "Batch 57 loss: 0.8232930898666382, accuracy: 0.6977370977401733\n",
      "Batch 58 loss: 0.9229084849357605, accuracy: 0.6970338821411133\n",
      "Batch 59 loss: 0.8594602346420288, accuracy: 0.696093738079071\n",
      "Batch 60 loss: 0.6723831295967102, accuracy: 0.6971055269241333\n",
      "Batch 61 loss: 0.7909324169158936, accuracy: 0.6969506144523621\n",
      "Batch 62 loss: 0.7023608088493347, accuracy: 0.6975446343421936\n",
      "Batch 63 loss: 0.8050962686538696, accuracy: 0.6976318359375\n",
      "Batch 64 loss: 0.766168475151062, accuracy: 0.6972355842590332\n",
      "Batch 65 loss: 0.8444047570228577, accuracy: 0.6968513131141663\n",
      "Batch 66 loss: 0.6512474417686462, accuracy: 0.6979944109916687\n",
      "Batch 67 loss: 0.6903448700904846, accuracy: 0.6985294222831726\n",
      "Batch 68 loss: 0.7091249227523804, accuracy: 0.6989356875419617\n",
      "Batch 69 loss: 0.8808926343917847, accuracy: 0.6987723112106323\n",
      "Batch 70 loss: 0.7825402617454529, accuracy: 0.6989436745643616\n",
      "Batch 71 loss: 0.7157106995582581, accuracy: 0.6996527910232544\n",
      "Batch 72 loss: 0.8199167847633362, accuracy: 0.6993792653083801\n",
      "Batch 73 loss: 0.790702760219574, accuracy: 0.6996410489082336\n",
      "Batch 74 loss: 0.8336330652236938, accuracy: 0.6988541483879089\n",
      "Batch 75 loss: 0.8894252777099609, accuracy: 0.6987047791481018\n",
      "Batch 76 loss: 0.7786321043968201, accuracy: 0.6987621784210205\n",
      "Batch 77 loss: 0.6864439845085144, accuracy: 0.6991186141967773\n",
      "Batch 78 loss: 0.828197717666626, accuracy: 0.6994659900665283\n",
      "Batch 79 loss: 0.8009626865386963, accuracy: 0.6993163824081421\n",
      "Batch 80 loss: 0.63564532995224, accuracy: 0.7003279328346252\n",
      "Batch 81 loss: 0.7770471572875977, accuracy: 0.7000762224197388\n",
      "Batch 82 loss: 0.8564120531082153, accuracy: 0.6999247074127197\n",
      "Batch 83 loss: 0.6873999238014221, accuracy: 0.7003348469734192\n",
      "Batch 84 loss: 0.905665397644043, accuracy: 0.6996323466300964\n",
      "Batch 85 loss: 0.7917260527610779, accuracy: 0.6998546719551086\n",
      "Batch 86 loss: 0.9316269755363464, accuracy: 0.6987248659133911\n",
      "Batch 87 loss: 0.8241561055183411, accuracy: 0.6983309388160706\n",
      "Batch 88 loss: 0.837540864944458, accuracy: 0.6979459524154663\n",
      "Batch 89 loss: 0.967262864112854, accuracy: 0.6974826455116272\n",
      "Batch 90 loss: 0.7959506511688232, accuracy: 0.6978021860122681\n",
      "Batch 91 loss: 0.8047667145729065, accuracy: 0.697605311870575\n",
      "Batch 92 loss: 0.7861287593841553, accuracy: 0.6976646780967712\n",
      "Batch 93 loss: 0.80023193359375, accuracy: 0.6977227330207825\n",
      "Batch 94 loss: 0.7393919229507446, accuracy: 0.6981085538864136\n",
      "Batch 95 loss: 0.7986617088317871, accuracy: 0.6979166865348816\n",
      "Batch 96 loss: 0.7958089113235474, accuracy: 0.6975676417350769\n",
      "Batch 97 loss: 0.8432630300521851, accuracy: 0.6975446343421936\n",
      "Batch 98 loss: 0.7825323343276978, accuracy: 0.6975221037864685\n",
      "Batch 99 loss: 0.715577244758606, accuracy: 0.6979687213897705\n",
      "Batch 100 loss: 0.7905657887458801, accuracy: 0.6981744766235352\n",
      "Batch 101 loss: 0.7824585437774658, accuracy: 0.6981464624404907\n",
      "Batch 102 loss: 0.7826826572418213, accuracy: 0.6986498832702637\n",
      "Batch 103 loss: 0.8605151772499084, accuracy: 0.6982421875\n",
      "Batch 104 loss: 0.6275551319122314, accuracy: 0.6989583373069763\n",
      "Batch 105 loss: 0.7371453046798706, accuracy: 0.69921875\n",
      "Batch 106 loss: 0.8075954914093018, accuracy: 0.6991822719573975\n",
      "Batch 107 loss: 0.8690377473831177, accuracy: 0.6989293694496155\n",
      "Batch 108 loss: 0.7771154046058655, accuracy: 0.6991112232208252\n",
      "Batch 109 loss: 0.7422904372215271, accuracy: 0.6995738744735718\n",
      "Batch 110 loss: 0.8037378787994385, accuracy: 0.6997466087341309\n",
      "Batch 111 loss: 0.7314019799232483, accuracy: 0.7001255750656128\n",
      "Batch 112 loss: 0.7517942190170288, accuracy: 0.7002212405204773\n",
      "Batch 113 loss: 0.7856795191764832, accuracy: 0.7001096606254578\n",
      "Batch 114 loss: 0.8796148896217346, accuracy: 0.7002037763595581\n",
      "Batch 115 loss: 0.7957984209060669, accuracy: 0.7003636956214905\n",
      "Batch 116 loss: 0.8744521737098694, accuracy: 0.700120210647583\n",
      "Batch 117 loss: 0.7302935719490051, accuracy: 0.7008077502250671\n",
      "Batch 118 loss: 0.7498756647109985, accuracy: 0.7008928656578064\n",
      "Batch 119 loss: 0.8977736234664917, accuracy: 0.7007812261581421\n",
      "Batch 120 loss: 0.6963425278663635, accuracy: 0.7014462947845459\n",
      "Batch 121 loss: 0.9038724899291992, accuracy: 0.7012679576873779\n",
      "Batch 122 loss: 0.9792882800102234, accuracy: 0.7005208134651184\n",
      "Batch 123 loss: 0.7428216338157654, accuracy: 0.7008568644523621\n",
      "Batch 124 loss: 0.6578255295753479, accuracy: 0.7013750076293945\n",
      "Batch 125 loss: 0.836320161819458, accuracy: 0.7013888955116272\n",
      "Batch 126 loss: 0.8181349635124207, accuracy: 0.7013410329818726\n",
      "Batch 127 loss: 0.8101698160171509, accuracy: 0.70098876953125\n",
      "Batch 128 loss: 0.9712698459625244, accuracy: 0.7005813717842102\n",
      "Batch 129 loss: 0.8753398060798645, accuracy: 0.7004807591438293\n",
      "Batch 130 loss: 0.6671313047409058, accuracy: 0.701097309589386\n",
      "Batch 131 loss: 0.7958850860595703, accuracy: 0.701231062412262\n",
      "Batch 132 loss: 0.8364615440368652, accuracy: 0.7013627886772156\n",
      "Batch 133 loss: 0.7896876931190491, accuracy: 0.7015508413314819\n",
      "Batch 134 loss: 0.9344367980957031, accuracy: 0.7010416388511658\n",
      "Batch 135 loss: 0.8655429482460022, accuracy: 0.7009420990943909\n",
      "Batch 136 loss: 0.6784342527389526, accuracy: 0.7011290788650513\n",
      "Batch 137 loss: 0.787097156047821, accuracy: 0.7010869383811951\n",
      "Batch 138 loss: 0.7865052819252014, accuracy: 0.7010453939437866\n",
      "Batch 139 loss: 0.9198775887489319, accuracy: 0.7010044455528259\n",
      "Batch 140 loss: 0.872370719909668, accuracy: 0.7011303305625916\n",
      "Batch 141 loss: 0.6356192231178284, accuracy: 0.701419472694397\n",
      "Batch 142 loss: 0.7349491119384766, accuracy: 0.701813817024231\n",
      "Batch 143 loss: 0.8088090419769287, accuracy: 0.7015516757965088\n",
      "Batch 144 loss: 0.8782113790512085, accuracy: 0.7011314630508423\n",
      "Batch 145 loss: 0.8075242042541504, accuracy: 0.7012521624565125\n",
      "Batch 146 loss: 0.9385530948638916, accuracy: 0.7008396983146667\n",
      "Batch 147 loss: 0.8898865580558777, accuracy: 0.7002217173576355\n",
      "Batch 148 loss: 0.8768088817596436, accuracy: 0.6998217105865479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 149 loss: 0.7621973752975464, accuracy: 0.6997395753860474\n",
      "Batch 150 loss: 0.7699465155601501, accuracy: 0.6999689340591431\n",
      "Batch 151 loss: 0.7814897298812866, accuracy: 0.7000411152839661\n",
      "Batch 152 loss: 0.8396873474121094, accuracy: 0.6999591588973999\n",
      "Batch 153 loss: 0.7522495985031128, accuracy: 0.7000811696052551\n",
      "Batch 154 loss: 0.8592160940170288, accuracy: 0.7001512050628662\n",
      "Batch 155 loss: 0.8516885042190552, accuracy: 0.7003205418586731\n",
      "Batch 156 loss: 0.6496458649635315, accuracy: 0.7007862329483032\n",
      "Batch 157 loss: 0.8613513112068176, accuracy: 0.7008504867553711\n",
      "Batch 158 loss: 0.9394861459732056, accuracy: 0.7004716992378235\n",
      "Batch 159 loss: 0.8455889225006104, accuracy: 0.700488269329071\n",
      "Batch 160 loss: 0.8599205017089844, accuracy: 0.7002134919166565\n",
      "Batch 161 loss: 0.7382058501243591, accuracy: 0.7001832723617554\n",
      "Batch 162 loss: 0.7806456685066223, accuracy: 0.7002971768379211\n",
      "Batch 163 loss: 0.8884539604187012, accuracy: 0.7001238465309143\n",
      "Batch 164 loss: 0.7867391705513, accuracy: 0.7002840638160706\n",
      "Batch 165 loss: 0.8190540671348572, accuracy: 0.700160026550293\n",
      "Batch 166 loss: 0.7736120223999023, accuracy: 0.70003741979599\n",
      "Batch 167 loss: 0.8779341578483582, accuracy: 0.700009286403656\n",
      "Batch 168 loss: 0.8392550349235535, accuracy: 0.6996579170227051\n",
      "Batch 169 loss: 0.9114500880241394, accuracy: 0.6998161673545837\n",
      "Batch 170 loss: 0.6701453328132629, accuracy: 0.7000182867050171\n",
      "Batch 171 loss: 0.7130836248397827, accuracy: 0.7002634406089783\n",
      "Batch 172 loss: 0.7705950736999512, accuracy: 0.7001445293426514\n",
      "Batch 173 loss: 0.8171486258506775, accuracy: 0.7001616358757019\n",
      "Batch 174 loss: 0.7781537175178528, accuracy: 0.7001339197158813\n",
      "Batch 175 loss: 0.8417654633522034, accuracy: 0.7000621557235718\n",
      "Batch 176 loss: 0.7439361810684204, accuracy: 0.7003442645072937\n",
      "Batch 177 loss: 0.781329870223999, accuracy: 0.7004915475845337\n",
      "Batch 178 loss: 0.8077172040939331, accuracy: 0.700419008731842\n",
      "Batch 179 loss: 0.7121663093566895, accuracy: 0.7006076574325562\n",
      "Batch 180 loss: 0.9049363136291504, accuracy: 0.7001898884773254\n",
      "Batch 181 loss: 0.7934106588363647, accuracy: 0.7001631259918213\n",
      "Batch 182 loss: 0.8341724872589111, accuracy: 0.7001792788505554\n",
      "Batch 183 loss: 0.6807899475097656, accuracy: 0.70066237449646\n",
      "Batch 184 loss: 0.7751958966255188, accuracy: 0.7008023858070374\n",
      "Batch 185 loss: 0.7853819727897644, accuracy: 0.7007728219032288\n",
      "Batch 186 loss: 0.946894645690918, accuracy: 0.7004511952400208\n",
      "Batch 187 loss: 0.7787723541259766, accuracy: 0.7004654407501221\n",
      "Batch 188 loss: 0.908029317855835, accuracy: 0.7004795074462891\n",
      "Batch 189 loss: 0.7283912897109985, accuracy: 0.7008223533630371\n",
      "Batch 190 loss: 0.8085750341415405, accuracy: 0.7009162306785583\n",
      "Batch 191 loss: 0.8797582983970642, accuracy: 0.7008463740348816\n",
      "Batch 192 loss: 0.7619848251342773, accuracy: 0.7009795904159546\n",
      "Batch 193 loss: 0.7283198237419128, accuracy: 0.7011517286300659\n",
      "Batch 194 loss: 0.9849545359611511, accuracy: 0.7010015845298767\n",
      "Batch 195 loss: 0.8887423276901245, accuracy: 0.7008529901504517\n",
      "Batch 196 loss: 0.9608473777770996, accuracy: 0.7005472779273987\n",
      "Batch 197 loss: 0.9281769394874573, accuracy: 0.7001657485961914\n",
      "Batch 198 loss: 0.8741994500160217, accuracy: 0.7000235319137573\n",
      "Batch 199 loss: 0.8247206807136536, accuracy: 0.7000390887260437\n",
      "Batch 200 loss: 0.8534030914306641, accuracy: 0.6999378204345703\n",
      "Batch 201 loss: 0.8577798008918762, accuracy: 0.6998375654220581\n",
      "Batch 202 loss: 0.903394877910614, accuracy: 0.699853777885437\n",
      "Batch 203 loss: 0.7064992785453796, accuracy: 0.6999846696853638\n",
      "Batch 204 loss: 0.8518623113632202, accuracy: 0.6998094320297241\n",
      "Batch 205 loss: 0.8836538791656494, accuracy: 0.6994842290878296\n",
      "Batch 206 loss: 0.7682948708534241, accuracy: 0.6996905207633972\n",
      "Batch 207 loss: 0.9452716112136841, accuracy: 0.6993690133094788\n",
      "Batch 208 loss: 0.8338078260421753, accuracy: 0.699349582195282\n",
      "Batch 209 loss: 0.7134157419204712, accuracy: 0.699590802192688\n",
      "Batch 210 loss: 0.8679783344268799, accuracy: 0.6993853449821472\n",
      "Batch 211 loss: 0.9630080461502075, accuracy: 0.6990345120429993\n",
      "Batch 212 loss: 0.809356153011322, accuracy: 0.6989069581031799\n",
      "Batch 213 loss: 0.8582022786140442, accuracy: 0.6991457343101501\n",
      "Batch 214 loss: 0.9548563957214355, accuracy: 0.6986918449401855\n",
      "Batch 215 loss: 0.6905739903450012, accuracy: 0.6985676884651184\n",
      "Batch 216 loss: 0.7778738737106323, accuracy: 0.6986607313156128\n",
      "Batch 217 loss: 0.878036618232727, accuracy: 0.6981794834136963\n",
      "Batch 218 loss: 0.9963970184326172, accuracy: 0.6978809833526611\n",
      "Batch 219 loss: 0.6473276615142822, accuracy: 0.6980823874473572\n",
      "Batch 220 loss: 0.6587068438529968, accuracy: 0.6984940767288208\n",
      "Batch 221 loss: 0.811431884765625, accuracy: 0.6986204981803894\n",
      "Batch 222 loss: 0.7980541586875916, accuracy: 0.6988158822059631\n",
      "Batch 223 loss: 1.0190898180007935, accuracy: 0.6984165906906128\n",
      "Batch 224 loss: 0.8464364409446716, accuracy: 0.6983175873756409\n",
      "Training epoch: 28, train accuracy: 69.83175659179688, train loss: 0.8091091185145908, valid accuracy: 63.19308853149414, valid loss: 1.049175720790337 \n",
      "Batch 0 loss: 0.8900732398033142, accuracy: 0.6796875\n",
      "Batch 1 loss: 0.6761506795883179, accuracy: 0.71484375\n",
      "Batch 2 loss: 0.7747856378555298, accuracy: 0.7135416865348816\n",
      "Batch 3 loss: 0.850303053855896, accuracy: 0.6953125\n",
      "Batch 4 loss: 0.7111333608627319, accuracy: 0.699999988079071\n",
      "Batch 5 loss: 0.777439534664154, accuracy: 0.69921875\n",
      "Batch 6 loss: 0.7472227215766907, accuracy: 0.7053571343421936\n",
      "Batch 7 loss: 0.6919659376144409, accuracy: 0.7109375\n",
      "Batch 8 loss: 0.9956339001655579, accuracy: 0.703125\n",
      "Batch 9 loss: 0.6776637434959412, accuracy: 0.703906238079071\n",
      "Batch 10 loss: 0.752804696559906, accuracy: 0.703125\n",
      "Batch 11 loss: 0.7043243646621704, accuracy: 0.7057291865348816\n",
      "Batch 12 loss: 0.6327264308929443, accuracy: 0.7091346383094788\n",
      "Batch 13 loss: 0.6551618576049805, accuracy: 0.7154017686843872\n",
      "Batch 14 loss: 0.7480505704879761, accuracy: 0.715624988079071\n",
      "Batch 15 loss: 0.8033442497253418, accuracy: 0.71484375\n",
      "Batch 16 loss: 0.7806722521781921, accuracy: 0.7146139740943909\n",
      "Batch 17 loss: 0.6217401027679443, accuracy: 0.7170138955116272\n",
      "Batch 18 loss: 0.6533481478691101, accuracy: 0.7208059430122375\n",
      "Batch 19 loss: 0.6816440224647522, accuracy: 0.723437488079071\n",
      "Batch 20 loss: 0.7528943419456482, accuracy: 0.722470223903656\n",
      "Batch 21 loss: 0.7792048454284668, accuracy: 0.7212358117103577\n",
      "Batch 22 loss: 0.5792592167854309, accuracy: 0.7265625\n",
      "Batch 23 loss: 0.8113284707069397, accuracy: 0.7265625\n",
      "Batch 24 loss: 0.7506970763206482, accuracy: 0.7265625\n",
      "Batch 25 loss: 0.7880199551582336, accuracy: 0.7247596383094788\n",
      "Batch 26 loss: 0.7507109045982361, accuracy: 0.7251157164573669\n",
      "Batch 27 loss: 0.7303518056869507, accuracy: 0.7262834906578064\n",
      "Batch 28 loss: 0.7939446568489075, accuracy: 0.7268319129943848\n",
      "Batch 29 loss: 0.7755817174911499, accuracy: 0.7278645634651184\n",
      "Batch 30 loss: 0.7683408856391907, accuracy: 0.727318525314331\n",
      "Batch 31 loss: 0.8563656806945801, accuracy: 0.7265625\n",
      "Batch 32 loss: 0.8946237564086914, accuracy: 0.7258522510528564\n",
      "Batch 33 loss: 0.6585403680801392, accuracy: 0.7267922759056091\n",
      "Batch 34 loss: 0.7158549427986145, accuracy: 0.7258928418159485\n",
      "Batch 35 loss: 0.5978118181228638, accuracy: 0.7267795205116272\n",
      "Batch 36 loss: 0.8068256974220276, accuracy: 0.7257179021835327\n",
      "Batch 37 loss: 0.9921551942825317, accuracy: 0.7230674624443054\n",
      "Batch 38 loss: 0.7234340906143188, accuracy: 0.7215544581413269\n",
      "Batch 39 loss: 0.9278721809387207, accuracy: 0.720507800579071\n",
      "Batch 40 loss: 0.8871826529502869, accuracy: 0.7198932766914368\n",
      "Batch 41 loss: 0.7635326385498047, accuracy: 0.7200520634651184\n",
      "Batch 42 loss: 0.8303143978118896, accuracy: 0.7200217843055725\n",
      "Batch 43 loss: 0.7641801834106445, accuracy: 0.7205255627632141\n",
      "Batch 44 loss: 0.5839738845825195, accuracy: 0.7223958373069763\n",
      "Batch 45 loss: 0.7087186574935913, accuracy: 0.7233356237411499\n",
      "Batch 46 loss: 0.8330788016319275, accuracy: 0.723071813583374\n",
      "Batch 47 loss: 0.8105252385139465, accuracy: 0.7224934697151184\n",
      "Batch 48 loss: 0.7839600443840027, accuracy: 0.722257673740387\n",
      "Batch 49 loss: 0.8094998598098755, accuracy: 0.721875011920929\n",
      "Batch 50 loss: 0.7722512483596802, accuracy: 0.7222732901573181\n",
      "Batch 51 loss: 0.9235831499099731, accuracy: 0.7202523946762085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 52 loss: 0.792487621307373, accuracy: 0.7191922068595886\n",
      "Batch 53 loss: 0.7039955854415894, accuracy: 0.7199074029922485\n",
      "Batch 54 loss: 0.733603835105896, accuracy: 0.7196022868156433\n",
      "Batch 55 loss: 0.8798061013221741, accuracy: 0.7193080186843872\n",
      "Batch 56 loss: 0.8529458045959473, accuracy: 0.7191612124443054\n",
      "Batch 57 loss: 0.6958286762237549, accuracy: 0.7195581793785095\n",
      "Batch 58 loss: 0.7223877310752869, accuracy: 0.7196769118309021\n",
      "Batch 59 loss: 0.7165458798408508, accuracy: 0.719921886920929\n",
      "Batch 60 loss: 0.7012224197387695, accuracy: 0.7206711173057556\n",
      "Batch 61 loss: 0.7561824917793274, accuracy: 0.7203881144523621\n",
      "Batch 62 loss: 0.7494817972183228, accuracy: 0.7198660969734192\n",
      "Batch 63 loss: 0.8446620106697083, accuracy: 0.719482421875\n",
      "Batch 64 loss: 0.6859462857246399, accuracy: 0.720432698726654\n",
      "Batch 65 loss: 0.8735675811767578, accuracy: 0.7198153138160706\n",
      "Batch 66 loss: 0.9031491279602051, accuracy: 0.7193329930305481\n",
      "Batch 67 loss: 0.7642451524734497, accuracy: 0.718864917755127\n",
      "Batch 68 loss: 0.9029310941696167, accuracy: 0.7182971239089966\n",
      "Batch 69 loss: 0.8565819263458252, accuracy: 0.7176339030265808\n",
      "Batch 70 loss: 0.8286530375480652, accuracy: 0.7173195481300354\n",
      "Batch 71 loss: 0.7527492046356201, accuracy: 0.7174479365348816\n",
      "Batch 72 loss: 0.6817785501480103, accuracy: 0.7174657583236694\n",
      "Batch 73 loss: 0.8001130819320679, accuracy: 0.7170608043670654\n",
      "Batch 74 loss: 0.7598361968994141, accuracy: 0.7166666388511658\n",
      "Batch 75 loss: 0.7689423561096191, accuracy: 0.7171052694320679\n",
      "Batch 76 loss: 0.8282798528671265, accuracy: 0.7167207598686218\n",
      "Batch 77 loss: 0.6802695393562317, accuracy: 0.7173477411270142\n",
      "Batch 78 loss: 0.7385206818580627, accuracy: 0.717563271522522\n",
      "Batch 79 loss: 0.9178134202957153, accuracy: 0.7166992425918579\n",
      "Batch 80 loss: 0.8381356000900269, accuracy: 0.7158564925193787\n",
      "Batch 81 loss: 0.7322869896888733, accuracy: 0.7162728905677795\n",
      "Batch 82 loss: 0.8116258382797241, accuracy: 0.7163968086242676\n",
      "Batch 83 loss: 0.8567538857460022, accuracy: 0.7157738208770752\n",
      "Batch 84 loss: 0.9450441598892212, accuracy: 0.7151654362678528\n",
      "Batch 85 loss: 0.6416833996772766, accuracy: 0.7158430218696594\n",
      "Batch 86 loss: 0.6923041343688965, accuracy: 0.7162356376647949\n",
      "Batch 87 loss: 0.6816559433937073, accuracy: 0.7166193127632141\n",
      "Batch 88 loss: 0.8920031189918518, accuracy: 0.7160288095474243\n",
      "Batch 89 loss: 0.8672565221786499, accuracy: 0.7163194417953491\n",
      "Batch 90 loss: 0.8204924464225769, accuracy: 0.7161744236946106\n",
      "Batch 91 loss: 0.8078265190124512, accuracy: 0.7158627510070801\n",
      "Batch 92 loss: 0.6767818927764893, accuracy: 0.7160618305206299\n",
      "Batch 93 loss: 0.6382398009300232, accuracy: 0.716589093208313\n",
      "Batch 94 loss: 0.7969333529472351, accuracy: 0.7161183953285217\n",
      "Batch 95 loss: 0.7671632766723633, accuracy: 0.7162272334098816\n",
      "Batch 96 loss: 0.8122810125350952, accuracy: 0.7160115838050842\n",
      "Batch 97 loss: 0.8962537050247192, accuracy: 0.7157206535339355\n",
      "Batch 98 loss: 0.837762176990509, accuracy: 0.7152777910232544\n",
      "Batch 99 loss: 0.7356183528900146, accuracy: 0.715624988079071\n",
      "Batch 100 loss: 0.700860857963562, accuracy: 0.7162747383117676\n",
      "Batch 101 loss: 0.9249626398086548, accuracy: 0.7159160375595093\n",
      "Batch 102 loss: 0.7445756793022156, accuracy: 0.7163228392601013\n",
      "Batch 103 loss: 0.7943083047866821, accuracy: 0.7164212465286255\n",
      "Batch 104 loss: 0.7635483741760254, accuracy: 0.7163690328598022\n",
      "Batch 105 loss: 1.0790276527404785, accuracy: 0.7156544923782349\n",
      "Batch 106 loss: 0.698910653591156, accuracy: 0.7160484790802002\n",
      "Batch 107 loss: 0.7174408435821533, accuracy: 0.7161458134651184\n",
      "Batch 108 loss: 0.719524621963501, accuracy: 0.7165997624397278\n",
      "Batch 109 loss: 0.9249164462089539, accuracy: 0.7162641882896423\n",
      "Batch 110 loss: 0.6682411432266235, accuracy: 0.7164977192878723\n",
      "Batch 111 loss: 0.8385121822357178, accuracy: 0.71630859375\n",
      "Batch 112 loss: 0.8857656121253967, accuracy: 0.7158462405204773\n",
      "Batch 113 loss: 0.9940856099128723, accuracy: 0.7150493264198303\n",
      "Batch 114 loss: 0.9185965061187744, accuracy: 0.7144700884819031\n",
      "Batch 115 loss: 0.717552900314331, accuracy: 0.71484375\n",
      "Batch 116 loss: 0.7967221140861511, accuracy: 0.7149438858032227\n",
      "Batch 117 loss: 0.6952625513076782, accuracy: 0.7154396176338196\n",
      "Batch 118 loss: 0.6635177135467529, accuracy: 0.7159926295280457\n",
      "Batch 119 loss: 0.7936491370201111, accuracy: 0.7159505486488342\n",
      "Batch 120 loss: 0.7481951713562012, accuracy: 0.7160382270812988\n",
      "Batch 121 loss: 0.6540471315383911, accuracy: 0.7161885499954224\n",
      "Batch 122 loss: 0.7038334608078003, accuracy: 0.716209352016449\n",
      "Batch 123 loss: 0.7679462432861328, accuracy: 0.7161668539047241\n",
      "Batch 124 loss: 0.8004066944122314, accuracy: 0.7158750295639038\n",
      "Batch 125 loss: 0.7148171663284302, accuracy: 0.715897798538208\n",
      "Batch 126 loss: 0.7859593033790588, accuracy: 0.7157357335090637\n",
      "Batch 127 loss: 0.7145866751670837, accuracy: 0.71563720703125\n",
      "Batch 128 loss: 0.6588585376739502, accuracy: 0.7154796719551086\n",
      "Batch 129 loss: 0.6769357919692993, accuracy: 0.7158653736114502\n",
      "Batch 130 loss: 0.9030885696411133, accuracy: 0.7155892252922058\n",
      "Batch 131 loss: 0.8314129114151001, accuracy: 0.7153764367103577\n",
      "Batch 132 loss: 0.7814763784408569, accuracy: 0.7155192494392395\n",
      "Batch 133 loss: 0.7545511722564697, accuracy: 0.7156016826629639\n",
      "Batch 134 loss: 0.6630173921585083, accuracy: 0.7160301208496094\n",
      "Batch 135 loss: 0.765405535697937, accuracy: 0.7160500884056091\n",
      "Batch 136 loss: 0.8976699113845825, accuracy: 0.7155565619468689\n",
      "Batch 137 loss: 0.7568176984786987, accuracy: 0.71535325050354\n",
      "Batch 138 loss: 0.8612886071205139, accuracy: 0.7150404453277588\n",
      "Batch 139 loss: 0.663465678691864, accuracy: 0.7152343988418579\n",
      "Batch 140 loss: 0.7231379747390747, accuracy: 0.7152038812637329\n",
      "Batch 141 loss: 0.8178685903549194, accuracy: 0.7150087952613831\n",
      "Batch 142 loss: 0.7019756436347961, accuracy: 0.7153627872467041\n",
      "Batch 143 loss: 0.9270869493484497, accuracy: 0.7151692509651184\n",
      "Batch 144 loss: 1.0194098949432373, accuracy: 0.7146012783050537\n",
      "Batch 145 loss: 0.9347661137580872, accuracy: 0.714255154132843\n",
      "Batch 146 loss: 0.7515999674797058, accuracy: 0.7141262888908386\n",
      "Batch 147 loss: 0.8432867527008057, accuracy: 0.7138407826423645\n",
      "Batch 148 loss: 0.6687330603599548, accuracy: 0.7140834927558899\n",
      "Batch 149 loss: 0.7306109666824341, accuracy: 0.7141146063804626\n",
      "Batch 150 loss: 0.7908759713172913, accuracy: 0.7139383554458618\n",
      "Batch 151 loss: 0.7481502294540405, accuracy: 0.7139185667037964\n",
      "Batch 152 loss: 0.8104774951934814, accuracy: 0.7136437892913818\n",
      "Batch 153 loss: 0.5619388818740845, accuracy: 0.7142857313156128\n",
      "Batch 154 loss: 0.8378317356109619, accuracy: 0.7142640948295593\n",
      "Batch 155 loss: 0.8369269967079163, accuracy: 0.7140424847602844\n",
      "Batch 156 loss: 0.7666295170783997, accuracy: 0.714022696018219\n",
      "Batch 157 loss: 0.7916171550750732, accuracy: 0.7140031456947327\n",
      "Batch 158 loss: 0.8351418972015381, accuracy: 0.7140821814537048\n",
      "Batch 159 loss: 0.7479315996170044, accuracy: 0.7142578363418579\n",
      "Batch 160 loss: 0.7341650128364563, accuracy: 0.7141886353492737\n",
      "Batch 161 loss: 0.7460769414901733, accuracy: 0.7140239477157593\n",
      "Batch 162 loss: 0.78180330991745, accuracy: 0.7140049934387207\n",
      "Batch 163 loss: 0.7849380970001221, accuracy: 0.7137957215309143\n",
      "Batch 164 loss: 0.9491724967956543, accuracy: 0.7134943008422852\n",
      "Batch 165 loss: 0.8277926445007324, accuracy: 0.7132906913757324\n",
      "Batch 166 loss: 0.7869175672531128, accuracy: 0.713229775428772\n",
      "Batch 167 loss: 0.8633900880813599, accuracy: 0.713076651096344\n",
      "Batch 168 loss: 0.783356785774231, accuracy: 0.7131102085113525\n",
      "Batch 169 loss: 0.8641154170036316, accuracy: 0.7129595875740051\n",
      "Batch 170 loss: 0.9489918351173401, accuracy: 0.7125365734100342\n",
      "Batch 171 loss: 0.8361658453941345, accuracy: 0.7124364376068115\n",
      "Batch 172 loss: 0.7997972965240479, accuracy: 0.7123374342918396\n",
      "Batch 173 loss: 0.8673049211502075, accuracy: 0.7120150923728943\n",
      "Batch 174 loss: 0.7401957511901855, accuracy: 0.7122767567634583\n",
      "Batch 175 loss: 0.7220887541770935, accuracy: 0.7124467492103577\n",
      "Batch 176 loss: 0.883894681930542, accuracy: 0.7121734023094177\n",
      "Batch 177 loss: 0.8411872982978821, accuracy: 0.7121225595474243\n",
      "Batch 178 loss: 0.777124285697937, accuracy: 0.7121595740318298\n",
      "Batch 179 loss: 0.7809311747550964, accuracy: 0.7122395634651184\n",
      "Batch 180 loss: 0.8319781422615051, accuracy: 0.7120165824890137\n",
      "Batch 181 loss: 0.7192890048027039, accuracy: 0.7123969793319702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 182 loss: 0.7696712017059326, accuracy: 0.7122182250022888\n",
      "Batch 183 loss: 0.6393337249755859, accuracy: 0.712550938129425\n",
      "Batch 184 loss: 0.7493690252304077, accuracy: 0.7123733162879944\n",
      "Batch 185 loss: 0.9307066798210144, accuracy: 0.7122395634651184\n",
      "Batch 186 loss: 0.8187586069107056, accuracy: 0.7120655179023743\n",
      "Batch 187 loss: 0.8535577058792114, accuracy: 0.7119348645210266\n",
      "Batch 188 loss: 1.065712809562683, accuracy: 0.7115162014961243\n",
      "Batch 189 loss: 0.9143974781036377, accuracy: 0.7113487124443054\n",
      "Batch 190 loss: 0.8107351064682007, accuracy: 0.7111828923225403\n",
      "Batch 191 loss: 0.8434398174285889, accuracy: 0.7109782099723816\n",
      "Batch 192 loss: 0.7964019775390625, accuracy: 0.7111803889274597\n",
      "Batch 193 loss: 0.9490461349487305, accuracy: 0.7106958627700806\n",
      "Batch 194 loss: 0.6815457344055176, accuracy: 0.7110576629638672\n",
      "Batch 195 loss: 0.9418289065361023, accuracy: 0.7106584906578064\n",
      "Batch 196 loss: 0.7885358929634094, accuracy: 0.710659921169281\n",
      "Batch 197 loss: 0.7044428586959839, accuracy: 0.7107796669006348\n",
      "Batch 198 loss: 0.7588830590248108, accuracy: 0.7107019424438477\n",
      "Batch 199 loss: 0.7633129358291626, accuracy: 0.7106249928474426\n",
      "Batch 200 loss: 0.8792672753334045, accuracy: 0.7104710936546326\n",
      "Batch 201 loss: 0.9401693940162659, accuracy: 0.7100866436958313\n",
      "Batch 202 loss: 0.7889484763145447, accuracy: 0.7098984122276306\n",
      "Batch 203 loss: 0.7353482246398926, accuracy: 0.7100949883460999\n",
      "Batch 204 loss: 0.9308657050132751, accuracy: 0.7099466323852539\n",
      "Batch 205 loss: 0.8223004341125488, accuracy: 0.7097997665405273\n",
      "Batch 206 loss: 0.8201214075088501, accuracy: 0.7097675204277039\n",
      "Batch 207 loss: 0.8131449818611145, accuracy: 0.7097731232643127\n",
      "Batch 208 loss: 0.6985197067260742, accuracy: 0.7101151347160339\n",
      "Batch 209 loss: 0.8001692295074463, accuracy: 0.7101190686225891\n",
      "Batch 210 loss: 0.8953275680541992, accuracy: 0.709678590297699\n",
      "Batch 211 loss: 0.9552265405654907, accuracy: 0.7093160152435303\n",
      "Batch 212 loss: 0.8826689124107361, accuracy: 0.7092136144638062\n",
      "Batch 213 loss: 0.8486250042915344, accuracy: 0.7091851830482483\n",
      "Batch 214 loss: 0.7993490695953369, accuracy: 0.7091206312179565\n",
      "Batch 215 loss: 0.8551374077796936, accuracy: 0.708984375\n",
      "Batch 216 loss: 0.7360027432441711, accuracy: 0.7091013789176941\n",
      "Batch 217 loss: 0.8889286518096924, accuracy: 0.7089664340019226\n",
      "Batch 218 loss: 0.7597427368164062, accuracy: 0.7087613940238953\n",
      "Batch 219 loss: 0.838361382484436, accuracy: 0.708771288394928\n",
      "Batch 220 loss: 0.8776669502258301, accuracy: 0.7087104320526123\n",
      "Batch 221 loss: 0.7789298295974731, accuracy: 0.7087204456329346\n",
      "Batch 222 loss: 0.6978409886360168, accuracy: 0.708870530128479\n",
      "Batch 223 loss: 0.8218156695365906, accuracy: 0.7088797688484192\n",
      "Batch 224 loss: 0.6982754468917847, accuracy: 0.7088717818260193\n",
      "Training epoch: 29, train accuracy: 70.88717651367188, train loss: 0.789308025572035, valid accuracy: 64.41905975341797, valid loss: 1.0410542385331516 \n",
      "Batch 0 loss: 0.8107984662055969, accuracy: 0.6875\n",
      "Batch 1 loss: 0.7654074430465698, accuracy: 0.6875\n",
      "Batch 2 loss: 0.8682237267494202, accuracy: 0.6979166865348816\n",
      "Batch 3 loss: 0.907914936542511, accuracy: 0.69921875\n",
      "Batch 4 loss: 0.840927243232727, accuracy: 0.7046874761581421\n",
      "Batch 5 loss: 0.7842426896095276, accuracy: 0.7109375\n",
      "Batch 6 loss: 0.6830015778541565, accuracy: 0.7142857313156128\n",
      "Batch 7 loss: 0.7702053785324097, accuracy: 0.7255859375\n",
      "Batch 8 loss: 0.7058923244476318, accuracy: 0.7256944179534912\n",
      "Batch 9 loss: 0.5867006778717041, accuracy: 0.7328125238418579\n",
      "Batch 10 loss: 0.9116828441619873, accuracy: 0.7294034361839294\n",
      "Batch 11 loss: 0.7057896256446838, accuracy: 0.7298176884651184\n",
      "Batch 12 loss: 0.8976027965545654, accuracy: 0.7247596383094788\n",
      "Batch 13 loss: 0.6404820680618286, accuracy: 0.7260044813156128\n",
      "Batch 14 loss: 0.7584804892539978, accuracy: 0.7250000238418579\n",
      "Batch 15 loss: 0.8202845454216003, accuracy: 0.72119140625\n",
      "Batch 16 loss: 0.8145248889923096, accuracy: 0.7201286554336548\n",
      "Batch 17 loss: 0.7409026622772217, accuracy: 0.7213541865348816\n",
      "Batch 18 loss: 0.6280298233032227, accuracy: 0.7232730388641357\n",
      "Batch 19 loss: 0.7740961313247681, accuracy: 0.723437488079071\n",
      "Batch 20 loss: 0.6108420491218567, accuracy: 0.7250744104385376\n",
      "Batch 21 loss: 0.747787594795227, accuracy: 0.7247869372367859\n",
      "Batch 22 loss: 0.6139981150627136, accuracy: 0.7255434989929199\n",
      "Batch 23 loss: 0.6420198678970337, accuracy: 0.7262369990348816\n",
      "Batch 24 loss: 0.7015573382377625, accuracy: 0.7278125286102295\n",
      "Batch 25 loss: 0.7148131132125854, accuracy: 0.7262620329856873\n",
      "Batch 26 loss: 0.6419753432273865, accuracy: 0.7271412014961243\n",
      "Batch 27 loss: 0.8465198278427124, accuracy: 0.7262834906578064\n",
      "Batch 28 loss: 0.6841162443161011, accuracy: 0.7262930870056152\n",
      "Batch 29 loss: 0.7487143278121948, accuracy: 0.7255208492279053\n",
      "Batch 30 loss: 0.7834199070930481, accuracy: 0.7240423560142517\n",
      "Batch 31 loss: 0.5701969861984253, accuracy: 0.725830078125\n",
      "Batch 32 loss: 0.8160289525985718, accuracy: 0.7251420617103577\n",
      "Batch 33 loss: 0.8425345420837402, accuracy: 0.7240349054336548\n",
      "Batch 34 loss: 0.7346498370170593, accuracy: 0.7238839268684387\n",
      "Batch 35 loss: 0.8171641230583191, accuracy: 0.7233073115348816\n",
      "Batch 36 loss: 0.7101260423660278, accuracy: 0.7231841087341309\n",
      "Batch 37 loss: 0.8663056492805481, accuracy: 0.7222450375556946\n",
      "Batch 38 loss: 0.7627865672111511, accuracy: 0.7223557829856873\n",
      "Batch 39 loss: 0.836464524269104, accuracy: 0.7212890386581421\n",
      "Batch 40 loss: 0.7830378413200378, accuracy: 0.7217987775802612\n",
      "Batch 41 loss: 0.6823320388793945, accuracy: 0.722842276096344\n",
      "Batch 42 loss: 0.6370326280593872, accuracy: 0.7236555218696594\n",
      "Batch 43 loss: 0.6584519147872925, accuracy: 0.7238991260528564\n",
      "Batch 44 loss: 0.6814425587654114, accuracy: 0.7244791388511658\n",
      "Batch 45 loss: 0.7991114854812622, accuracy: 0.72316575050354\n",
      "Batch 46 loss: 0.8439847826957703, accuracy: 0.723071813583374\n",
      "Batch 47 loss: 0.8073301315307617, accuracy: 0.7229817509651184\n",
      "Batch 48 loss: 0.7188102602958679, accuracy: 0.7225765585899353\n",
      "Batch 49 loss: 0.7857603430747986, accuracy: 0.721875011920929\n",
      "Batch 50 loss: 0.7848289608955383, accuracy: 0.7210478186607361\n",
      "Batch 51 loss: 0.8555465340614319, accuracy: 0.7204026579856873\n",
      "Batch 52 loss: 0.7117857933044434, accuracy: 0.7202240824699402\n",
      "Batch 53 loss: 0.6635754108428955, accuracy: 0.7210648059844971\n",
      "Batch 54 loss: 0.9144462943077087, accuracy: 0.7194602489471436\n",
      "Batch 55 loss: 0.6550555229187012, accuracy: 0.720703125\n",
      "Batch 56 loss: 0.7801234722137451, accuracy: 0.7208059430122375\n",
      "Batch 57 loss: 0.8185590505599976, accuracy: 0.7207704782485962\n",
      "Batch 58 loss: 0.7465986013412476, accuracy: 0.7204713821411133\n",
      "Batch 59 loss: 0.8619225025177002, accuracy: 0.7200520634651184\n",
      "Batch 60 loss: 0.773527979850769, accuracy: 0.7205430269241333\n",
      "Batch 61 loss: 0.794367790222168, accuracy: 0.7203881144523621\n",
      "Batch 62 loss: 0.8705421686172485, accuracy: 0.719122052192688\n",
      "Batch 63 loss: 0.8858882784843445, accuracy: 0.7188720703125\n",
      "Batch 64 loss: 0.8594058752059937, accuracy: 0.7179086804389954\n",
      "Batch 65 loss: 0.8844330310821533, accuracy: 0.7179213762283325\n",
      "Batch 66 loss: 0.8049359917640686, accuracy: 0.7181670069694519\n",
      "Batch 67 loss: 0.7395182251930237, accuracy: 0.7185202240943909\n",
      "Batch 68 loss: 0.6247851252555847, accuracy: 0.7192028760910034\n",
      "Batch 69 loss: 0.7567430734634399, accuracy: 0.719531238079071\n",
      "Batch 70 loss: 0.8599027395248413, accuracy: 0.7191901206970215\n",
      "Batch 71 loss: 0.8342719674110413, accuracy: 0.7185329794883728\n",
      "Batch 72 loss: 0.6738048791885376, accuracy: 0.7186429500579834\n",
      "Batch 73 loss: 0.9751449823379517, accuracy: 0.7173775434494019\n",
      "Batch 74 loss: 0.6950729489326477, accuracy: 0.7172916531562805\n",
      "Batch 75 loss: 0.7044738531112671, accuracy: 0.7180304527282715\n",
      "Batch 76 loss: 0.743823230266571, accuracy: 0.7175324559211731\n",
      "Batch 77 loss: 0.7357825636863708, accuracy: 0.717848539352417\n",
      "Batch 78 loss: 0.6702707409858704, accuracy: 0.7180577516555786\n",
      "Batch 79 loss: 0.8261335492134094, accuracy: 0.7178710699081421\n",
      "Batch 80 loss: 0.7508631944656372, accuracy: 0.7183641791343689\n",
      "Batch 81 loss: 0.7390164732933044, accuracy: 0.71875\n",
      "Batch 82 loss: 1.0367166996002197, accuracy: 0.717526376247406\n",
      "Batch 83 loss: 0.9297148585319519, accuracy: 0.7165178656578064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 84 loss: 0.8573154211044312, accuracy: 0.715900719165802\n",
      "Batch 85 loss: 0.8106591701507568, accuracy: 0.7159338593482971\n",
      "Batch 86 loss: 0.8117204904556274, accuracy: 0.7149784564971924\n",
      "Batch 87 loss: 0.6778055429458618, accuracy: 0.7150213122367859\n",
      "Batch 88 loss: 0.7109078764915466, accuracy: 0.7149754166603088\n",
      "Batch 89 loss: 0.7104328870773315, accuracy: 0.7153645753860474\n",
      "Batch 90 loss: 0.6851909756660461, accuracy: 0.7156593203544617\n",
      "Batch 91 loss: 0.809604287147522, accuracy: 0.715438187122345\n",
      "Batch 92 loss: 0.9082575440406799, accuracy: 0.7143816947937012\n",
      "Batch 93 loss: 0.7977135181427002, accuracy: 0.7142619490623474\n",
      "Batch 94 loss: 0.9369829297065735, accuracy: 0.7136512994766235\n",
      "Batch 95 loss: 0.7383607625961304, accuracy: 0.7140299677848816\n",
      "Batch 96 loss: 0.8992263078689575, accuracy: 0.7135953903198242\n",
      "Batch 97 loss: 0.7425161600112915, accuracy: 0.713887095451355\n",
      "Batch 98 loss: 0.7755374312400818, accuracy: 0.7139362096786499\n",
      "Batch 99 loss: 0.7071938514709473, accuracy: 0.7138281464576721\n",
      "Batch 100 loss: 0.7054809331893921, accuracy: 0.7141088843345642\n",
      "Batch 101 loss: 0.6438836455345154, accuracy: 0.71484375\n",
      "Batch 102 loss: 0.8823124170303345, accuracy: 0.714426577091217\n",
      "Batch 103 loss: 0.8397863507270813, accuracy: 0.7137169241905212\n",
      "Batch 104 loss: 0.8701081275939941, accuracy: 0.7137649059295654\n",
      "Batch 105 loss: 0.7571796178817749, accuracy: 0.7138856053352356\n",
      "Batch 106 loss: 0.861482560634613, accuracy: 0.7134929895401001\n",
      "Batch 107 loss: 0.6264255046844482, accuracy: 0.7140480279922485\n",
      "Batch 108 loss: 0.7387006878852844, accuracy: 0.7143778800964355\n",
      "Batch 109 loss: 0.7084577083587646, accuracy: 0.7141335010528564\n",
      "Batch 110 loss: 0.8238463997840881, accuracy: 0.7138231992721558\n",
      "Batch 111 loss: 0.8192347288131714, accuracy: 0.7137276530265808\n",
      "Batch 112 loss: 0.7952965497970581, accuracy: 0.7137721180915833\n",
      "Batch 113 loss: 0.849841833114624, accuracy: 0.7134731411933899\n",
      "Batch 114 loss: 0.7437812685966492, accuracy: 0.7136548757553101\n",
      "Batch 115 loss: 0.8458061814308167, accuracy: 0.713496744632721\n",
      "Batch 116 loss: 0.685849666595459, accuracy: 0.713942289352417\n",
      "Batch 117 loss: 0.7044017910957336, accuracy: 0.7139830589294434\n",
      "Batch 118 loss: 0.6626494526863098, accuracy: 0.7142200469970703\n",
      "Batch 119 loss: 0.6474301815032959, accuracy: 0.7145182490348816\n",
      "Batch 120 loss: 0.7766388058662415, accuracy: 0.7144886255264282\n",
      "Batch 121 loss: 0.6067034602165222, accuracy: 0.7151639461517334\n",
      "Batch 122 loss: 0.7839983701705933, accuracy: 0.7150660753250122\n",
      "Batch 123 loss: 0.9434136748313904, accuracy: 0.7147177457809448\n",
      "Batch 124 loss: 0.7587714791297913, accuracy: 0.7148749828338623\n",
      "Batch 125 loss: 0.9160919785499573, accuracy: 0.714471697807312\n",
      "Batch 126 loss: 0.7073110342025757, accuracy: 0.7146284580230713\n",
      "Batch 127 loss: 0.788445770740509, accuracy: 0.715087890625\n",
      "Batch 128 loss: 0.8165926337242126, accuracy: 0.7146923542022705\n",
      "Batch 129 loss: 0.7603247165679932, accuracy: 0.7145432829856873\n",
      "Batch 130 loss: 0.7508292198181152, accuracy: 0.714515745639801\n",
      "Batch 131 loss: 0.6959363222122192, accuracy: 0.7145478129386902\n",
      "Batch 132 loss: 0.7039721012115479, accuracy: 0.714755654335022\n",
      "Batch 133 loss: 0.8723981976509094, accuracy: 0.7142024040222168\n",
      "Batch 134 loss: 0.8719189167022705, accuracy: 0.7137152552604675\n",
      "Batch 135 loss: 0.7656480669975281, accuracy: 0.7139246463775635\n",
      "Batch 136 loss: 0.6652746200561523, accuracy: 0.7142449617385864\n",
      "Batch 137 loss: 0.8136722445487976, accuracy: 0.7142210006713867\n",
      "Batch 138 loss: 0.8333561420440674, accuracy: 0.7139163613319397\n",
      "Batch 139 loss: 0.7084944844245911, accuracy: 0.7141740918159485\n",
      "Batch 140 loss: 0.7687599658966064, accuracy: 0.7140957713127136\n",
      "Batch 141 loss: 0.7415043711662292, accuracy: 0.7142935991287231\n",
      "Batch 142 loss: 0.660144031047821, accuracy: 0.7145978808403015\n",
      "Batch 143 loss: 0.7357603311538696, accuracy: 0.7146267294883728\n",
      "Batch 144 loss: 0.8743121027946472, accuracy: 0.7141702771186829\n",
      "Batch 145 loss: 0.8156074285507202, accuracy: 0.7137735486030579\n",
      "Batch 146 loss: 0.6398541331291199, accuracy: 0.7141793966293335\n",
      "Batch 147 loss: 0.7314516305923462, accuracy: 0.7139991521835327\n",
      "Batch 148 loss: 0.8175487518310547, accuracy: 0.7139785885810852\n",
      "Batch 149 loss: 0.7097246050834656, accuracy: 0.7144270539283752\n",
      "Batch 150 loss: 0.5963342785835266, accuracy: 0.7150248289108276\n",
      "Batch 151 loss: 0.7166791558265686, accuracy: 0.7150493264198303\n",
      "Batch 152 loss: 0.6965838074684143, accuracy: 0.7152267098426819\n",
      "Batch 153 loss: 0.8149213194847107, accuracy: 0.7153003215789795\n",
      "Batch 154 loss: 0.8678441643714905, accuracy: 0.7151713967323303\n",
      "Batch 155 loss: 0.8668239712715149, accuracy: 0.7151442170143127\n",
      "Batch 156 loss: 0.7550421357154846, accuracy: 0.7151174545288086\n",
      "Batch 157 loss: 0.6326136589050293, accuracy: 0.7153382301330566\n",
      "Batch 158 loss: 0.7230933308601379, accuracy: 0.7152122855186462\n",
      "Batch 159 loss: 0.9117782711982727, accuracy: 0.7146972417831421\n",
      "Batch 160 loss: 0.753947913646698, accuracy: 0.714625358581543\n",
      "Batch 161 loss: 0.7549648284912109, accuracy: 0.7149401903152466\n",
      "Batch 162 loss: 0.981163501739502, accuracy: 0.7146280407905579\n",
      "Batch 163 loss: 0.9099365472793579, accuracy: 0.7144150137901306\n",
      "Batch 164 loss: 0.8072208166122437, accuracy: 0.7142992615699768\n",
      "Batch 165 loss: 0.6774935126304626, accuracy: 0.7145143151283264\n",
      "Batch 166 loss: 0.7655254602432251, accuracy: 0.7140718698501587\n",
      "Batch 167 loss: 0.9169546961784363, accuracy: 0.7136346697807312\n",
      "Batch 168 loss: 0.7672103643417358, accuracy: 0.7135262489318848\n",
      "Batch 169 loss: 0.7640271186828613, accuracy: 0.7134650945663452\n",
      "Batch 170 loss: 0.9316875338554382, accuracy: 0.7129020690917969\n",
      "Batch 171 loss: 0.8136907815933228, accuracy: 0.712890625\n",
      "Batch 172 loss: 0.9000817537307739, accuracy: 0.7126535177230835\n",
      "Batch 173 loss: 0.7345889210700989, accuracy: 0.7127783894538879\n",
      "Batch 174 loss: 0.7368833422660828, accuracy: 0.7130357027053833\n",
      "Batch 175 loss: 0.7234359383583069, accuracy: 0.7131125926971436\n",
      "Batch 176 loss: 0.7879739999771118, accuracy: 0.713232696056366\n",
      "Batch 177 loss: 0.7767985463142395, accuracy: 0.713175892829895\n",
      "Batch 178 loss: 0.809087872505188, accuracy: 0.7132506966590881\n",
      "Batch 179 loss: 0.871817409992218, accuracy: 0.712890625\n",
      "Batch 180 loss: 0.7094448804855347, accuracy: 0.7131388187408447\n",
      "Batch 181 loss: 0.7267067432403564, accuracy: 0.7131696343421936\n",
      "Batch 182 loss: 0.9016444087028503, accuracy: 0.7129867076873779\n",
      "Batch 183 loss: 0.8314297795295715, accuracy: 0.712763249874115\n",
      "Batch 184 loss: 0.9109314680099487, accuracy: 0.7125844359397888\n",
      "Batch 185 loss: 0.8171443939208984, accuracy: 0.7124915719032288\n",
      "Batch 186 loss: 0.8023197650909424, accuracy: 0.7125250697135925\n",
      "Batch 187 loss: 0.7954545021057129, accuracy: 0.7125166058540344\n",
      "Batch 188 loss: 0.765978991985321, accuracy: 0.7124255895614624\n",
      "Batch 189 loss: 0.8269408941268921, accuracy: 0.7122121453285217\n",
      "Batch 190 loss: 0.7350708246231079, accuracy: 0.7119191884994507\n",
      "Batch 191 loss: 0.7249469757080078, accuracy: 0.7120768427848816\n",
      "Batch 192 loss: 0.834057629108429, accuracy: 0.7119899392127991\n",
      "Batch 193 loss: 0.8424685597419739, accuracy: 0.7120651006698608\n",
      "Batch 194 loss: 0.8471847176551819, accuracy: 0.7117788195610046\n",
      "Batch 195 loss: 0.8305310606956482, accuracy: 0.7115752696990967\n",
      "Batch 196 loss: 0.780401885509491, accuracy: 0.711215078830719\n",
      "Batch 197 loss: 0.7168824672698975, accuracy: 0.7112926244735718\n",
      "Batch 198 loss: 0.7051302194595337, accuracy: 0.7114086151123047\n",
      "Batch 199 loss: 0.7559224963188171, accuracy: 0.7116796970367432\n",
      "Batch 200 loss: 0.8323073983192444, accuracy: 0.7115204930305481\n",
      "Batch 201 loss: 0.7951629161834717, accuracy: 0.7114402651786804\n",
      "Batch 202 loss: 0.8366390466690063, accuracy: 0.7112453579902649\n",
      "Batch 203 loss: 0.8116902709007263, accuracy: 0.711052417755127\n",
      "Batch 204 loss: 0.7613167762756348, accuracy: 0.7109375\n",
      "Batch 205 loss: 0.7840721011161804, accuracy: 0.7109375\n",
      "Batch 206 loss: 0.6913940906524658, accuracy: 0.7112016677856445\n",
      "Batch 207 loss: 0.7905343174934387, accuracy: 0.7110501527786255\n",
      "Batch 208 loss: 0.8375271558761597, accuracy: 0.7109375\n",
      "Batch 209 loss: 0.735666036605835, accuracy: 0.7111607193946838\n",
      "Batch 210 loss: 0.7598644495010376, accuracy: 0.7111966609954834\n",
      "Batch 211 loss: 0.8264107704162598, accuracy: 0.7111217379570007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 212 loss: 0.7239425182342529, accuracy: 0.7111942768096924\n",
      "Batch 213 loss: 0.7433931827545166, accuracy: 0.7112295627593994\n",
      "Batch 214 loss: 0.859184980392456, accuracy: 0.7108648419380188\n",
      "Batch 215 loss: 0.6893352270126343, accuracy: 0.7109736800193787\n",
      "Batch 216 loss: 0.8609693646430969, accuracy: 0.7109375\n",
      "Batch 217 loss: 0.7350809574127197, accuracy: 0.7112241983413696\n",
      "Batch 218 loss: 0.8452309370040894, accuracy: 0.7110445499420166\n",
      "Batch 219 loss: 0.8582211136817932, accuracy: 0.7109730243682861\n",
      "Batch 220 loss: 0.9130489230155945, accuracy: 0.7105486392974854\n",
      "Batch 221 loss: 0.8901186585426331, accuracy: 0.7103744149208069\n",
      "Batch 222 loss: 0.8293457627296448, accuracy: 0.7102017998695374\n",
      "Batch 223 loss: 0.976170539855957, accuracy: 0.7100306749343872\n",
      "Batch 224 loss: 0.8235661387443542, accuracy: 0.7101605534553528\n",
      "Training epoch: 30, train accuracy: 71.01605987548828, train loss: 0.7783507521947225, valid accuracy: 63.52744674682617, valid loss: 1.0909581184387207 \n",
      "Batch 0 loss: 0.7919675707817078, accuracy: 0.703125\n",
      "Batch 1 loss: 0.8181714415550232, accuracy: 0.703125\n",
      "Batch 2 loss: 0.6965448260307312, accuracy: 0.71875\n",
      "Batch 3 loss: 0.7607411742210388, accuracy: 0.71484375\n",
      "Batch 4 loss: 0.8872413635253906, accuracy: 0.7046874761581421\n",
      "Batch 5 loss: 0.8142971992492676, accuracy: 0.7083333134651184\n",
      "Batch 6 loss: 0.8680168390274048, accuracy: 0.6975446343421936\n",
      "Batch 7 loss: 0.7750540375709534, accuracy: 0.701171875\n",
      "Batch 8 loss: 0.8528978228569031, accuracy: 0.6987847089767456\n",
      "Batch 9 loss: 0.7707444429397583, accuracy: 0.7046874761581421\n",
      "Batch 10 loss: 0.7143239378929138, accuracy: 0.7052556872367859\n",
      "Batch 11 loss: 0.728180468082428, accuracy: 0.7063801884651184\n",
      "Batch 12 loss: 0.7357573509216309, accuracy: 0.7073317170143127\n",
      "Batch 13 loss: 0.6996893882751465, accuracy: 0.7092633843421936\n",
      "Batch 14 loss: 0.7827096581459045, accuracy: 0.7072916626930237\n",
      "Batch 15 loss: 0.6990005373954773, accuracy: 0.70849609375\n",
      "Batch 16 loss: 0.719989538192749, accuracy: 0.7118566036224365\n",
      "Batch 17 loss: 0.7526041865348816, accuracy: 0.7122395634651184\n",
      "Batch 18 loss: 0.8324679136276245, accuracy: 0.7117598652839661\n",
      "Batch 19 loss: 0.761199414730072, accuracy: 0.7109375\n",
      "Batch 20 loss: 0.8335820436477661, accuracy: 0.7109375\n",
      "Batch 21 loss: 0.8291277885437012, accuracy: 0.7091619372367859\n",
      "Batch 22 loss: 0.6944002509117126, accuracy: 0.711277186870575\n",
      "Batch 23 loss: 0.6739679574966431, accuracy: 0.7112630009651184\n",
      "Batch 24 loss: 0.7459915280342102, accuracy: 0.7115625143051147\n",
      "Batch 25 loss: 0.6777544021606445, accuracy: 0.713942289352417\n",
      "Batch 26 loss: 0.7573598623275757, accuracy: 0.7152777910232544\n",
      "Batch 27 loss: 0.7827594876289368, accuracy: 0.7145647406578064\n",
      "Batch 28 loss: 0.7592104077339172, accuracy: 0.7147090435028076\n",
      "Batch 29 loss: 0.6770210266113281, accuracy: 0.7153645753860474\n",
      "Batch 30 loss: 0.9075008034706116, accuracy: 0.7137096524238586\n",
      "Batch 31 loss: 0.8182600140571594, accuracy: 0.713134765625\n",
      "Batch 32 loss: 0.7135387659072876, accuracy: 0.7123579382896423\n",
      "Batch 33 loss: 0.8553479313850403, accuracy: 0.7118566036224365\n",
      "Batch 34 loss: 0.7337257266044617, accuracy: 0.7127231955528259\n",
      "Batch 35 loss: 0.8549172282218933, accuracy: 0.7105034589767456\n",
      "Batch 36 loss: 0.8422946333885193, accuracy: 0.7096706032752991\n",
      "Batch 37 loss: 0.8410020470619202, accuracy: 0.7090871930122375\n",
      "Batch 38 loss: 0.8217774033546448, accuracy: 0.7085336446762085\n",
      "Batch 39 loss: 0.789187490940094, accuracy: 0.708789050579071\n",
      "Batch 40 loss: 0.7534182667732239, accuracy: 0.7092225551605225\n",
      "Batch 41 loss: 0.779455304145813, accuracy: 0.7085193395614624\n",
      "Batch 42 loss: 0.9548997282981873, accuracy: 0.7074854373931885\n",
      "Batch 43 loss: 0.6781108379364014, accuracy: 0.7080965638160706\n",
      "Batch 44 loss: 0.806782066822052, accuracy: 0.706944465637207\n",
      "Batch 45 loss: 0.9455678462982178, accuracy: 0.70533287525177\n",
      "Batch 46 loss: 0.8121894598007202, accuracy: 0.705285906791687\n",
      "Batch 47 loss: 0.7295392155647278, accuracy: 0.7057291865348816\n",
      "Batch 48 loss: 0.7281852960586548, accuracy: 0.7058354616165161\n",
      "Batch 49 loss: 0.7508296370506287, accuracy: 0.7059375047683716\n",
      "Batch 50 loss: 0.7612046599388123, accuracy: 0.7061887383460999\n",
      "Batch 51 loss: 0.7315161824226379, accuracy: 0.7062800526618958\n",
      "Batch 52 loss: 0.7761460542678833, accuracy: 0.7068101167678833\n",
      "Batch 53 loss: 0.6884544491767883, accuracy: 0.7071759104728699\n",
      "Batch 54 loss: 0.7982457280158997, accuracy: 0.7069602012634277\n",
      "Batch 55 loss: 0.6978460550308228, accuracy: 0.7073102593421936\n",
      "Batch 56 loss: 0.8043381571769714, accuracy: 0.7068256735801697\n",
      "Batch 57 loss: 0.5834221839904785, accuracy: 0.7082435488700867\n",
      "Batch 58 loss: 0.7680085897445679, accuracy: 0.7076271176338196\n",
      "Batch 59 loss: 0.8268147110939026, accuracy: 0.7065104246139526\n",
      "Batch 60 loss: 0.6720133423805237, accuracy: 0.7070952653884888\n",
      "Batch 61 loss: 0.7605441212654114, accuracy: 0.7074092626571655\n",
      "Batch 62 loss: 0.7042089104652405, accuracy: 0.7077133059501648\n",
      "Batch 63 loss: 0.7051534652709961, accuracy: 0.7083740234375\n",
      "Batch 64 loss: 0.9076165556907654, accuracy: 0.7076923251152039\n",
      "Batch 65 loss: 0.6889559030532837, accuracy: 0.7084516882896423\n",
      "Batch 66 loss: 0.7869532108306885, accuracy: 0.7093050479888916\n",
      "Batch 67 loss: 0.6131291389465332, accuracy: 0.7103630304336548\n",
      "Batch 68 loss: 0.7604002356529236, accuracy: 0.7111639380455017\n",
      "Batch 69 loss: 0.76749187707901, accuracy: 0.7116071581840515\n",
      "Batch 70 loss: 0.6501514315605164, accuracy: 0.7121478915214539\n",
      "Batch 71 loss: 0.7305350303649902, accuracy: 0.7126736044883728\n",
      "Batch 72 loss: 0.7970176339149475, accuracy: 0.7126498222351074\n",
      "Batch 73 loss: 0.7256644368171692, accuracy: 0.7125211358070374\n",
      "Batch 74 loss: 0.7613456845283508, accuracy: 0.7128124833106995\n",
      "Batch 75 loss: 0.7312383055686951, accuracy: 0.7124794125556946\n",
      "Batch 76 loss: 0.8078305125236511, accuracy: 0.7120535969734192\n",
      "Batch 77 loss: 0.811250627040863, accuracy: 0.7114382982254028\n",
      "Batch 78 loss: 0.7496972680091858, accuracy: 0.7112341523170471\n",
      "Batch 79 loss: 0.7290602326393127, accuracy: 0.7109375\n",
      "Batch 80 loss: 0.6459577679634094, accuracy: 0.7113233208656311\n",
      "Batch 81 loss: 0.8162346482276917, accuracy: 0.7116044163703918\n",
      "Batch 82 loss: 0.8631982207298279, accuracy: 0.7112199068069458\n",
      "Batch 83 loss: 0.9756300449371338, accuracy: 0.7106584906578064\n",
      "Batch 84 loss: 0.7059205770492554, accuracy: 0.7109375\n",
      "Batch 85 loss: 0.8592109084129333, accuracy: 0.7106649875640869\n",
      "Batch 86 loss: 0.7964953184127808, accuracy: 0.7103089094161987\n",
      "Batch 87 loss: 0.7936766147613525, accuracy: 0.7103160619735718\n",
      "Batch 88 loss: 0.7728869915008545, accuracy: 0.710147500038147\n",
      "Batch 89 loss: 0.7270305752754211, accuracy: 0.7098958492279053\n",
      "Batch 90 loss: 0.7226783633232117, accuracy: 0.7103365659713745\n",
      "Batch 91 loss: 0.771147608757019, accuracy: 0.71068274974823\n",
      "Batch 92 loss: 0.8949887156486511, accuracy: 0.7095094323158264\n",
      "Batch 93 loss: 0.6697205305099487, accuracy: 0.7097739577293396\n",
      "Batch 94 loss: 0.7904186248779297, accuracy: 0.7097039222717285\n",
      "Batch 95 loss: 0.7865067720413208, accuracy: 0.7095540165901184\n",
      "Batch 96 loss: 0.7572727203369141, accuracy: 0.7101320624351501\n",
      "Batch 97 loss: 0.6718029379844666, accuracy: 0.7107780575752258\n",
      "Batch 98 loss: 0.655776858329773, accuracy: 0.7113320827484131\n",
      "Batch 99 loss: 0.7441740036010742, accuracy: 0.7114843726158142\n",
      "Batch 100 loss: 0.9124932289123535, accuracy: 0.7111695408821106\n",
      "Batch 101 loss: 0.6976948976516724, accuracy: 0.7111672759056091\n",
      "Batch 102 loss: 0.7705776691436768, accuracy: 0.7113925814628601\n",
      "Batch 103 loss: 0.73952716588974, accuracy: 0.7115384340286255\n",
      "Batch 104 loss: 0.7379105091094971, accuracy: 0.7117559313774109\n",
      "Batch 105 loss: 0.8317289352416992, accuracy: 0.7113797068595886\n",
      "Batch 106 loss: 0.7452234625816345, accuracy: 0.711448609828949\n",
      "Batch 107 loss: 0.6838040947914124, accuracy: 0.7115885615348816\n",
      "Batch 108 loss: 0.7087857723236084, accuracy: 0.7117975950241089\n",
      "Batch 109 loss: 0.78929603099823, accuracy: 0.7115767002105713\n",
      "Batch 110 loss: 0.7730875611305237, accuracy: 0.7116413116455078\n",
      "Batch 111 loss: 0.7328115701675415, accuracy: 0.7117047905921936\n",
      "Batch 112 loss: 0.8147281408309937, accuracy: 0.7115597128868103\n",
      "Batch 113 loss: 0.7065117359161377, accuracy: 0.7118969559669495\n",
      "Batch 114 loss: 0.7297161817550659, accuracy: 0.7118886113166809\n",
      "Batch 115 loss: 0.6758683919906616, accuracy: 0.712284505367279\n",
      "Batch 116 loss: 0.7958320379257202, accuracy: 0.7120058536529541\n",
      "Batch 117 loss: 0.6714650392532349, accuracy: 0.7120630145072937\n",
      "Batch 118 loss: 0.6550671458244324, accuracy: 0.7121848464012146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 119 loss: 1.017755150794983, accuracy: 0.7113932371139526\n",
      "Batch 120 loss: 0.6582924723625183, accuracy: 0.7116477489471436\n",
      "Batch 121 loss: 0.7584342360496521, accuracy: 0.7115138173103333\n",
      "Batch 122 loss: 0.722102701663971, accuracy: 0.7116997241973877\n",
      "Batch 123 loss: 0.8685518503189087, accuracy: 0.7113155126571655\n",
      "Batch 124 loss: 0.7599908113479614, accuracy: 0.7114999890327454\n",
      "Batch 125 loss: 0.820311963558197, accuracy: 0.7113715410232544\n",
      "Batch 126 loss: 0.7268571853637695, accuracy: 0.7118602395057678\n",
      "Batch 127 loss: 0.9444028735160828, accuracy: 0.71112060546875\n",
      "Batch 128 loss: 0.7821349501609802, accuracy: 0.7113614082336426\n",
      "Batch 129 loss: 0.8002631664276123, accuracy: 0.7114182710647583\n",
      "Batch 130 loss: 0.6909730434417725, accuracy: 0.7114145755767822\n",
      "Batch 131 loss: 0.6243982315063477, accuracy: 0.7116477489471436\n",
      "Batch 132 loss: 0.9106625318527222, accuracy: 0.7116423845291138\n",
      "Batch 133 loss: 0.6643814444541931, accuracy: 0.71192866563797\n",
      "Batch 134 loss: 0.8180864453315735, accuracy: 0.7120949029922485\n",
      "Batch 135 loss: 0.8574410676956177, accuracy: 0.7119715213775635\n",
      "Batch 136 loss: 0.7139889001846313, accuracy: 0.7120780348777771\n",
      "Batch 137 loss: 0.8394694924354553, accuracy: 0.7117300629615784\n",
      "Batch 138 loss: 0.8418545722961426, accuracy: 0.7117243409156799\n",
      "Batch 139 loss: 0.744671642780304, accuracy: 0.7114955186843872\n",
      "Batch 140 loss: 0.8332400321960449, accuracy: 0.7113807797431946\n",
      "Batch 141 loss: 0.8223793506622314, accuracy: 0.7114326357841492\n",
      "Batch 142 loss: 0.7428119778633118, accuracy: 0.7112653255462646\n",
      "Batch 143 loss: 0.756559431552887, accuracy: 0.7112630009651184\n",
      "Batch 144 loss: 0.6328366994857788, accuracy: 0.7114224433898926\n",
      "Batch 145 loss: 0.8269068002700806, accuracy: 0.7111515402793884\n",
      "Batch 146 loss: 0.8472759127616882, accuracy: 0.7109375\n",
      "Batch 147 loss: 0.7791290283203125, accuracy: 0.7108319401741028\n",
      "Batch 148 loss: 0.6988853812217712, accuracy: 0.7109375\n",
      "Batch 149 loss: 0.786464512348175, accuracy: 0.7108854055404663\n",
      "Batch 150 loss: 0.7947798371315002, accuracy: 0.710885763168335\n",
      "Batch 151 loss: 0.7148653268814087, accuracy: 0.7109375\n",
      "Batch 152 loss: 0.8590624332427979, accuracy: 0.7109885811805725\n",
      "Batch 153 loss: 0.6653191447257996, accuracy: 0.7114955186843872\n",
      "Batch 154 loss: 0.7025797367095947, accuracy: 0.711693525314331\n",
      "Batch 155 loss: 0.8011738061904907, accuracy: 0.7115885615348816\n",
      "Batch 156 loss: 0.6775938272476196, accuracy: 0.7117834687232971\n",
      "Batch 157 loss: 0.643622875213623, accuracy: 0.7119264006614685\n",
      "Batch 158 loss: 0.6618245244026184, accuracy: 0.712165892124176\n",
      "Batch 159 loss: 0.7787928581237793, accuracy: 0.712207019329071\n",
      "Batch 160 loss: 0.8415282368659973, accuracy: 0.7118109464645386\n",
      "Batch 161 loss: 0.6896541118621826, accuracy: 0.711998462677002\n",
      "Batch 162 loss: 0.8457719087600708, accuracy: 0.7117522954940796\n",
      "Batch 163 loss: 0.6925778388977051, accuracy: 0.7119855284690857\n",
      "Batch 164 loss: 0.7737626433372498, accuracy: 0.7119791507720947\n",
      "Batch 165 loss: 0.6839939951896667, accuracy: 0.7122552990913391\n",
      "Batch 166 loss: 0.6838517785072327, accuracy: 0.7125280499458313\n",
      "Batch 167 loss: 0.6724984645843506, accuracy: 0.712704598903656\n",
      "Batch 168 loss: 0.8368780612945557, accuracy: 0.712509274482727\n",
      "Batch 169 loss: 0.8471218943595886, accuracy: 0.7123621106147766\n",
      "Batch 170 loss: 0.6264015436172485, accuracy: 0.7126736044883728\n",
      "Batch 171 loss: 0.7945206761360168, accuracy: 0.7124364376068115\n",
      "Batch 172 loss: 0.804537832736969, accuracy: 0.7123826146125793\n",
      "Batch 173 loss: 0.866361141204834, accuracy: 0.7122395634651184\n",
      "Batch 174 loss: 0.7754601240158081, accuracy: 0.7123660445213318\n",
      "Batch 175 loss: 0.6610879898071289, accuracy: 0.7125799059867859\n",
      "Batch 176 loss: 0.814026951789856, accuracy: 0.7125706076622009\n",
      "Batch 177 loss: 0.6938939690589905, accuracy: 0.7124736905097961\n",
      "Batch 178 loss: 0.8194844722747803, accuracy: 0.7124214172363281\n",
      "Batch 179 loss: 0.8858949542045593, accuracy: 0.7122395634651184\n",
      "Batch 180 loss: 0.7651576399803162, accuracy: 0.7121028900146484\n",
      "Batch 181 loss: 0.7230553030967712, accuracy: 0.7120535969734192\n",
      "Batch 182 loss: 0.9564505815505981, accuracy: 0.7117486596107483\n",
      "Batch 183 loss: 0.7373191118240356, accuracy: 0.7118291258811951\n",
      "Batch 184 loss: 1.0448551177978516, accuracy: 0.7111486196517944\n",
      "Batch 185 loss: 1.0267926454544067, accuracy: 0.7105594873428345\n",
      "Batch 186 loss: 0.831305205821991, accuracy: 0.710269033908844\n",
      "Batch 187 loss: 0.7954172492027283, accuracy: 0.7101895213127136\n",
      "Batch 188 loss: 0.7885954976081848, accuracy: 0.7102761268615723\n",
      "Batch 189 loss: 0.865988552570343, accuracy: 0.710197389125824\n",
      "Batch 190 loss: 0.743534505367279, accuracy: 0.7104057669639587\n",
      "Batch 191 loss: 0.6517388224601746, accuracy: 0.7106119990348816\n",
      "Batch 192 loss: 0.731740415096283, accuracy: 0.7106541395187378\n",
      "Batch 193 loss: 0.8857773542404175, accuracy: 0.710293173789978\n",
      "Batch 194 loss: 0.7172306776046753, accuracy: 0.7103766202926636\n",
      "Batch 195 loss: 0.8401200771331787, accuracy: 0.7101004719734192\n",
      "Batch 196 loss: 0.7451765537261963, accuracy: 0.7102633118629456\n",
      "Batch 197 loss: 0.7895517349243164, accuracy: 0.7101877927780151\n",
      "Batch 198 loss: 0.8068958520889282, accuracy: 0.7103093862533569\n",
      "Batch 199 loss: 0.8006379008293152, accuracy: 0.7105468511581421\n",
      "Batch 200 loss: 0.7970240116119385, accuracy: 0.7106654047966003\n",
      "Batch 201 loss: 0.6365421414375305, accuracy: 0.7108214497566223\n",
      "Batch 202 loss: 0.7596703767776489, accuracy: 0.7110529541969299\n",
      "Batch 203 loss: 0.7727685570716858, accuracy: 0.7111290097236633\n",
      "Batch 204 loss: 0.810195803642273, accuracy: 0.7111661434173584\n",
      "Batch 205 loss: 0.8516454100608826, accuracy: 0.7109754085540771\n",
      "Batch 206 loss: 0.9261304140090942, accuracy: 0.7108620405197144\n",
      "Batch 207 loss: 0.9140620231628418, accuracy: 0.7107496857643127\n",
      "Batch 208 loss: 0.8665071725845337, accuracy: 0.7106010913848877\n",
      "Batch 209 loss: 0.8822548389434814, accuracy: 0.7103794813156128\n",
      "Batch 210 loss: 0.7073498964309692, accuracy: 0.710456132888794\n",
      "Batch 211 loss: 0.6437659859657288, accuracy: 0.7107900977134705\n",
      "Batch 212 loss: 0.6822834014892578, accuracy: 0.7109741568565369\n",
      "Batch 213 loss: 0.932776689529419, accuracy: 0.7107914686203003\n",
      "Batch 214 loss: 0.784537672996521, accuracy: 0.710901141166687\n",
      "Batch 215 loss: 0.8027350902557373, accuracy: 0.7111183404922485\n",
      "Batch 216 loss: 0.7024983167648315, accuracy: 0.711261510848999\n",
      "Batch 217 loss: 0.9422011971473694, accuracy: 0.7109375\n",
      "Batch 218 loss: 0.7480800151824951, accuracy: 0.7108661532402039\n",
      "Batch 219 loss: 0.7146948575973511, accuracy: 0.7110795378684998\n",
      "Batch 220 loss: 0.6903235912322998, accuracy: 0.711255669593811\n",
      "Batch 221 loss: 0.7597376108169556, accuracy: 0.7114301919937134\n",
      "Batch 222 loss: 0.8074202537536621, accuracy: 0.7113929390907288\n",
      "Batch 223 loss: 0.7865777611732483, accuracy: 0.7113909125328064\n",
      "Batch 224 loss: 0.9139348864555359, accuracy: 0.7112403512001038\n",
      "Training epoch: 31, train accuracy: 71.12403869628906, train loss: 0.7728084988064237, valid accuracy: 63.109500885009766, valid loss: 1.0962776027876755 \n",
      "Batch 0 loss: 0.7647626996040344, accuracy: 0.6953125\n",
      "Batch 1 loss: 0.6879074573516846, accuracy: 0.7109375\n",
      "Batch 2 loss: 0.6724981665611267, accuracy: 0.7239583134651184\n",
      "Batch 3 loss: 0.8138340711593628, accuracy: 0.73046875\n",
      "Batch 4 loss: 0.8396308422088623, accuracy: 0.7265625\n",
      "Batch 5 loss: 0.6360489726066589, accuracy: 0.734375\n",
      "Batch 6 loss: 0.62953120470047, accuracy: 0.7332589030265808\n",
      "Batch 7 loss: 0.819471538066864, accuracy: 0.724609375\n",
      "Batch 8 loss: 0.8162317872047424, accuracy: 0.7248263955116272\n",
      "Batch 9 loss: 0.6903612017631531, accuracy: 0.7250000238418579\n",
      "Batch 10 loss: 0.7879148125648499, accuracy: 0.7230113744735718\n",
      "Batch 11 loss: 0.8780678510665894, accuracy: 0.7213541865348816\n",
      "Batch 12 loss: 0.8277217745780945, accuracy: 0.71875\n",
      "Batch 13 loss: 0.7012994289398193, accuracy: 0.7181919813156128\n",
      "Batch 14 loss: 0.6584986448287964, accuracy: 0.7192708253860474\n",
      "Batch 15 loss: 0.6941804885864258, accuracy: 0.71923828125\n",
      "Batch 16 loss: 0.6560879945755005, accuracy: 0.7219669222831726\n",
      "Batch 17 loss: 0.8249757885932922, accuracy: 0.71875\n",
      "Batch 18 loss: 0.8257079124450684, accuracy: 0.7158716917037964\n",
      "Batch 19 loss: 0.7867687940597534, accuracy: 0.715624988079071\n",
      "Batch 20 loss: 0.6738649010658264, accuracy: 0.715029776096344\n",
      "Batch 21 loss: 0.750599205493927, accuracy: 0.7151988744735718\n",
      "Batch 22 loss: 0.7531479597091675, accuracy: 0.7146739363670349\n",
      "Batch 23 loss: 0.7435181736946106, accuracy: 0.7161458134651184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24 loss: 0.715473473072052, accuracy: 0.7162500023841858\n",
      "Batch 25 loss: 0.6606394052505493, accuracy: 0.7169471383094788\n",
      "Batch 26 loss: 0.8591966032981873, accuracy: 0.7126736044883728\n",
      "Batch 27 loss: 0.8186101913452148, accuracy: 0.7123326063156128\n",
      "Batch 28 loss: 0.7888450622558594, accuracy: 0.7117456793785095\n",
      "Batch 29 loss: 0.7207422256469727, accuracy: 0.7124999761581421\n",
      "Batch 30 loss: 0.679041862487793, accuracy: 0.7139617204666138\n",
      "Batch 31 loss: 0.8534706234931946, accuracy: 0.71240234375\n",
      "Batch 32 loss: 0.8442184329032898, accuracy: 0.7111742496490479\n",
      "Batch 33 loss: 0.6722557544708252, accuracy: 0.712775707244873\n",
      "Batch 34 loss: 0.6787513494491577, accuracy: 0.7138392925262451\n",
      "Batch 35 loss: 0.7192426919937134, accuracy: 0.7144097089767456\n",
      "Batch 36 loss: 0.688075065612793, accuracy: 0.7162162065505981\n",
      "Batch 37 loss: 0.7326892614364624, accuracy: 0.7164884805679321\n",
      "Batch 38 loss: 0.8349372148513794, accuracy: 0.7165464758872986\n",
      "Batch 39 loss: 0.7121291160583496, accuracy: 0.7177734375\n",
      "Batch 40 loss: 0.7982670664787292, accuracy: 0.7176067233085632\n",
      "Batch 41 loss: 0.6712727546691895, accuracy: 0.7176339030265808\n",
      "Batch 42 loss: 0.7239021062850952, accuracy: 0.7180232405662537\n",
      "Batch 43 loss: 0.8478699922561646, accuracy: 0.7162641882896423\n",
      "Batch 44 loss: 0.7675809264183044, accuracy: 0.715624988079071\n",
      "Batch 45 loss: 0.9064710736274719, accuracy: 0.7150135636329651\n",
      "Batch 46 loss: 0.763957679271698, accuracy: 0.715259313583374\n",
      "Batch 47 loss: 0.757331371307373, accuracy: 0.71484375\n",
      "Batch 48 loss: 0.8577251434326172, accuracy: 0.7147640585899353\n",
      "Batch 49 loss: 0.6950613260269165, accuracy: 0.7153124809265137\n",
      "Batch 50 loss: 0.7650501728057861, accuracy: 0.7149203419685364\n",
      "Batch 51 loss: 0.6318711042404175, accuracy: 0.7160456776618958\n",
      "Batch 52 loss: 0.6433164477348328, accuracy: 0.7171285152435303\n",
      "Batch 53 loss: 0.7150470614433289, accuracy: 0.7175925970077515\n",
      "Batch 54 loss: 0.7216355800628662, accuracy: 0.7184659242630005\n",
      "Batch 55 loss: 0.6999569535255432, accuracy: 0.71875\n",
      "Batch 56 loss: 0.7453635334968567, accuracy: 0.7191612124443054\n",
      "Batch 57 loss: 0.85091632604599, accuracy: 0.7190194129943848\n",
      "Batch 58 loss: 0.7557846903800964, accuracy: 0.7186175584793091\n",
      "Batch 59 loss: 0.7689860463142395, accuracy: 0.7192708253860474\n",
      "Batch 60 loss: 0.7286255359649658, accuracy: 0.7188780903816223\n",
      "Batch 61 loss: 0.8298631310462952, accuracy: 0.717993974685669\n",
      "Batch 62 loss: 0.7376135587692261, accuracy: 0.7186259627342224\n",
      "Batch 63 loss: 0.7947968244552612, accuracy: 0.7186279296875\n",
      "Batch 64 loss: 0.6362837553024292, accuracy: 0.7195913195610046\n",
      "Batch 65 loss: 0.9249350428581238, accuracy: 0.71875\n",
      "Batch 66 loss: 0.6784024238586426, accuracy: 0.7189832329750061\n",
      "Batch 67 loss: 0.8277289271354675, accuracy: 0.7184053063392639\n",
      "Batch 68 loss: 0.7766869068145752, accuracy: 0.7182971239089966\n",
      "Batch 69 loss: 0.6948206424713135, accuracy: 0.7185267806053162\n",
      "Batch 70 loss: 0.7885033488273621, accuracy: 0.7181998491287231\n",
      "Batch 71 loss: 0.7043299674987793, accuracy: 0.7178819179534912\n",
      "Batch 72 loss: 0.710213303565979, accuracy: 0.7180008292198181\n",
      "Batch 73 loss: 0.7379659414291382, accuracy: 0.7171663641929626\n",
      "Batch 74 loss: 0.781324565410614, accuracy: 0.7162500023841858\n",
      "Batch 75 loss: 0.854888379573822, accuracy: 0.7157689332962036\n",
      "Batch 76 loss: 0.8876487612724304, accuracy: 0.7154017686843872\n",
      "Batch 77 loss: 0.8204642534255981, accuracy: 0.7146434187889099\n",
      "Batch 78 loss: 0.7694273591041565, accuracy: 0.7144976258277893\n",
      "Batch 79 loss: 0.7879925966262817, accuracy: 0.714062511920929\n",
      "Batch 80 loss: 0.6355482339859009, accuracy: 0.7146990895271301\n",
      "Batch 81 loss: 0.6279675960540771, accuracy: 0.7156059741973877\n",
      "Batch 82 loss: 0.8404253721237183, accuracy: 0.7156438231468201\n",
      "Batch 83 loss: 0.9246653318405151, accuracy: 0.7153087854385376\n",
      "Batch 84 loss: 0.8653313517570496, accuracy: 0.7146139740943909\n",
      "Batch 85 loss: 0.74549400806427, accuracy: 0.7146620750427246\n",
      "Batch 86 loss: 0.7966153621673584, accuracy: 0.7147988677024841\n",
      "Batch 87 loss: 0.7243633270263672, accuracy: 0.7145774364471436\n",
      "Batch 88 loss: 0.8215943574905396, accuracy: 0.7145364880561829\n",
      "Batch 89 loss: 0.8523460626602173, accuracy: 0.7142361402511597\n",
      "Batch 90 loss: 0.6452928781509399, accuracy: 0.7143715620040894\n",
      "Batch 91 loss: 0.8202959895133972, accuracy: 0.714249312877655\n",
      "Batch 92 loss: 0.7688766121864319, accuracy: 0.7143816947937012\n",
      "Batch 93 loss: 0.7774177193641663, accuracy: 0.7140957713127136\n",
      "Batch 94 loss: 0.8717770576477051, accuracy: 0.7138980031013489\n",
      "Batch 95 loss: 0.7111965417861938, accuracy: 0.7137858271598816\n",
      "Batch 96 loss: 0.781944990158081, accuracy: 0.7133537530899048\n",
      "Batch 97 loss: 0.7501094937324524, accuracy: 0.7138074040412903\n",
      "Batch 98 loss: 0.7751380205154419, accuracy: 0.714330792427063\n",
      "Batch 99 loss: 0.7522116899490356, accuracy: 0.7146875262260437\n",
      "Batch 100 loss: 0.7955116033554077, accuracy: 0.7141088843345642\n",
      "Batch 101 loss: 0.6932244896888733, accuracy: 0.7146139740943909\n",
      "Batch 102 loss: 0.8621336221694946, accuracy: 0.7142748832702637\n",
      "Batch 103 loss: 0.8772097826004028, accuracy: 0.713942289352417\n",
      "Batch 104 loss: 0.770018994808197, accuracy: 0.7144345045089722\n",
      "Batch 105 loss: 0.9041241407394409, accuracy: 0.7144752144813538\n",
      "Batch 106 loss: 0.8975511789321899, accuracy: 0.7141501307487488\n",
      "Batch 107 loss: 0.8347658514976501, accuracy: 0.7137587070465088\n",
      "Batch 108 loss: 0.8913640975952148, accuracy: 0.713231086730957\n",
      "Batch 109 loss: 0.8280593752861023, accuracy: 0.7130681872367859\n",
      "Batch 110 loss: 0.7137823104858398, accuracy: 0.7134712934494019\n",
      "Batch 111 loss: 0.67183917760849, accuracy: 0.7135184407234192\n",
      "Batch 112 loss: 1.0627256631851196, accuracy: 0.7124584913253784\n",
      "Batch 113 loss: 0.7720009684562683, accuracy: 0.7122395634651184\n",
      "Batch 114 loss: 0.7905492186546326, accuracy: 0.712092399597168\n",
      "Batch 115 loss: 0.830673098564148, accuracy: 0.7117456793785095\n",
      "Batch 116 loss: 0.749223530292511, accuracy: 0.7120058536529541\n",
      "Batch 117 loss: 0.754883885383606, accuracy: 0.7123278379440308\n",
      "Batch 118 loss: 0.8000761270523071, accuracy: 0.7123818397521973\n",
      "Batch 119 loss: 0.840447723865509, accuracy: 0.7123697996139526\n",
      "Batch 120 loss: 0.8043826222419739, accuracy: 0.7122288346290588\n",
      "Batch 121 loss: 0.8514529466629028, accuracy: 0.7118980288505554\n",
      "Batch 122 loss: 0.8561415672302246, accuracy: 0.7113820910453796\n",
      "Batch 123 loss: 0.8670533299446106, accuracy: 0.7111265063285828\n",
      "Batch 124 loss: 0.7371420860290527, accuracy: 0.7112500071525574\n",
      "Batch 125 loss: 0.8979691863059998, accuracy: 0.7106894850730896\n",
      "Batch 126 loss: 0.8647101521492004, accuracy: 0.7103223204612732\n",
      "Batch 127 loss: 0.7927424907684326, accuracy: 0.7103271484375\n",
      "Batch 128 loss: 0.683025598526001, accuracy: 0.7105135917663574\n",
      "Batch 129 loss: 0.9716213345527649, accuracy: 0.710036039352417\n",
      "Batch 130 loss: 0.88566654920578, accuracy: 0.7095062136650085\n",
      "Batch 131 loss: 0.6924623847007751, accuracy: 0.7098129987716675\n",
      "Batch 132 loss: 0.6909774541854858, accuracy: 0.7105262875556946\n",
      "Batch 133 loss: 0.7463161945343018, accuracy: 0.7107042670249939\n",
      "Batch 134 loss: 0.7705733776092529, accuracy: 0.7107059955596924\n",
      "Batch 135 loss: 0.8017294406890869, accuracy: 0.7107077240943909\n",
      "Batch 136 loss: 0.8233821392059326, accuracy: 0.7104812860488892\n",
      "Batch 137 loss: 0.6962857246398926, accuracy: 0.7108808755874634\n",
      "Batch 138 loss: 0.6414005756378174, accuracy: 0.7110499143600464\n",
      "Batch 139 loss: 0.7692797183990479, accuracy: 0.7111607193946838\n",
      "Batch 140 loss: 0.8960230946540833, accuracy: 0.7108266949653625\n",
      "Batch 141 loss: 0.7832831144332886, accuracy: 0.7110475301742554\n",
      "Batch 142 loss: 0.8933643698692322, accuracy: 0.7107735872268677\n",
      "Batch 143 loss: 0.7548683285713196, accuracy: 0.7108832597732544\n",
      "Batch 144 loss: 0.7361956238746643, accuracy: 0.7110991477966309\n",
      "Batch 145 loss: 0.8095807433128357, accuracy: 0.7109910249710083\n",
      "Batch 146 loss: 0.6169147491455078, accuracy: 0.7113626599311829\n",
      "Batch 147 loss: 0.6929308176040649, accuracy: 0.7115181684494019\n",
      "Batch 148 loss: 0.7451878190040588, accuracy: 0.7114618420600891\n",
      "Batch 149 loss: 0.6870679259300232, accuracy: 0.7118229269981384\n",
      "Batch 150 loss: 0.7411594390869141, accuracy: 0.7115583419799805\n",
      "Batch 151 loss: 0.6937846541404724, accuracy: 0.7117084860801697\n",
      "Batch 152 loss: 0.6796106100082397, accuracy: 0.7121119499206543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 153 loss: 1.018867015838623, accuracy: 0.7115970253944397\n",
      "Batch 154 loss: 0.7125888466835022, accuracy: 0.7115927338600159\n",
      "Batch 155 loss: 0.6933719515800476, accuracy: 0.711838960647583\n",
      "Batch 156 loss: 0.7666183114051819, accuracy: 0.7123308181762695\n",
      "Batch 157 loss: 0.6187084913253784, accuracy: 0.7129153609275818\n",
      "Batch 158 loss: 0.6746523380279541, accuracy: 0.713443398475647\n",
      "Batch 159 loss: 0.7699760794639587, accuracy: 0.7132812738418579\n",
      "Batch 160 loss: 0.6997007131576538, accuracy: 0.7134608030319214\n",
      "Batch 161 loss: 0.5981800556182861, accuracy: 0.7139274477958679\n",
      "Batch 162 loss: 0.7940993309020996, accuracy: 0.7138611674308777\n",
      "Batch 163 loss: 0.7137059569358826, accuracy: 0.7137480974197388\n",
      "Batch 164 loss: 0.7938825488090515, accuracy: 0.7136363387107849\n",
      "Batch 165 loss: 0.7638577222824097, accuracy: 0.713620126247406\n",
      "Batch 166 loss: 0.9499542713165283, accuracy: 0.7130894660949707\n",
      "Batch 167 loss: 0.7833266258239746, accuracy: 0.7133091688156128\n",
      "Batch 168 loss: 0.7876371145248413, accuracy: 0.7132026553153992\n",
      "Batch 169 loss: 0.744045078754425, accuracy: 0.7130974531173706\n",
      "Batch 170 loss: 0.7922064661979675, accuracy: 0.7128106951713562\n",
      "Batch 171 loss: 0.8563467264175415, accuracy: 0.712890625\n",
      "Batch 172 loss: 0.7971683740615845, accuracy: 0.7128793597221375\n",
      "Batch 173 loss: 0.732174813747406, accuracy: 0.7127783894538879\n",
      "Batch 174 loss: 0.8455392122268677, accuracy: 0.7125892639160156\n",
      "Batch 175 loss: 0.7728400230407715, accuracy: 0.7128462195396423\n",
      "Batch 176 loss: 0.676965594291687, accuracy: 0.7129678726196289\n",
      "Batch 177 loss: 0.8161590695381165, accuracy: 0.7128686904907227\n",
      "Batch 178 loss: 0.8327721357345581, accuracy: 0.7126833200454712\n",
      "Batch 179 loss: 0.8086972832679749, accuracy: 0.7126736044883728\n",
      "Batch 180 loss: 0.7848766446113586, accuracy: 0.712664008140564\n",
      "Batch 181 loss: 0.8285160660743713, accuracy: 0.7126116156578064\n",
      "Batch 182 loss: 0.7761525511741638, accuracy: 0.7126878499984741\n",
      "Batch 183 loss: 0.7780855894088745, accuracy: 0.7127208113670349\n",
      "Batch 184 loss: 0.7746649384498596, accuracy: 0.7126688957214355\n",
      "Batch 185 loss: 0.7442753314971924, accuracy: 0.7126176357269287\n",
      "Batch 186 loss: 0.6115893125534058, accuracy: 0.7129428386688232\n",
      "Batch 187 loss: 0.8179709911346436, accuracy: 0.7128075361251831\n",
      "Batch 188 loss: 0.6982073783874512, accuracy: 0.7131283283233643\n",
      "Batch 189 loss: 0.7032183408737183, accuracy: 0.7133223414421082\n",
      "Batch 190 loss: 0.797762930393219, accuracy: 0.7129826545715332\n",
      "Batch 191 loss: 0.7735219597816467, accuracy: 0.7130126953125\n",
      "Batch 192 loss: 0.6027364134788513, accuracy: 0.7135686278343201\n",
      "Batch 193 loss: 0.8399156332015991, accuracy: 0.7132731676101685\n",
      "Batch 194 loss: 0.7724528312683105, accuracy: 0.7133413553237915\n",
      "Batch 195 loss: 0.7459121942520142, accuracy: 0.7134885191917419\n",
      "Batch 196 loss: 0.7492772340774536, accuracy: 0.7134755849838257\n",
      "Batch 197 loss: 0.7342865467071533, accuracy: 0.7133049368858337\n",
      "Batch 198 loss: 0.7628279328346252, accuracy: 0.713528573513031\n",
      "Batch 199 loss: 0.7743036150932312, accuracy: 0.7135156393051147\n",
      "Batch 200 loss: 0.864300012588501, accuracy: 0.7133861780166626\n",
      "Batch 201 loss: 0.7833250164985657, accuracy: 0.7134127616882324\n",
      "Batch 202 loss: 0.8575723767280579, accuracy: 0.7132850885391235\n",
      "Batch 203 loss: 0.8911755681037903, accuracy: 0.7128522992134094\n",
      "Batch 204 loss: 0.5745753049850464, accuracy: 0.7132622003555298\n",
      "Batch 205 loss: 0.7490910291671753, accuracy: 0.713516354560852\n",
      "Batch 206 loss: 0.6889554858207703, accuracy: 0.7136171460151672\n",
      "Batch 207 loss: 1.1018154621124268, accuracy: 0.7131535410881042\n",
      "Batch 208 loss: 0.6194915771484375, accuracy: 0.7136662602424622\n",
      "Batch 209 loss: 0.7696986198425293, accuracy: 0.7136160731315613\n",
      "Batch 210 loss: 0.8505717515945435, accuracy: 0.7133812308311462\n",
      "Batch 211 loss: 0.6944660544395447, accuracy: 0.713443398475647\n",
      "Batch 212 loss: 0.6528666019439697, accuracy: 0.7136883735656738\n",
      "Batch 213 loss: 0.8226975202560425, accuracy: 0.7135295271873474\n",
      "Batch 214 loss: 0.8416480422019958, accuracy: 0.7133357524871826\n",
      "Batch 215 loss: 0.7854065895080566, accuracy: 0.7133608460426331\n",
      "Batch 216 loss: 0.8022878170013428, accuracy: 0.713421642780304\n",
      "Batch 217 loss: 0.785112738609314, accuracy: 0.713302731513977\n",
      "Batch 218 loss: 0.739901602268219, accuracy: 0.7132205963134766\n",
      "Batch 219 loss: 0.6139602065086365, accuracy: 0.7135298252105713\n",
      "Batch 220 loss: 0.7858409881591797, accuracy: 0.7133413553237915\n",
      "Batch 221 loss: 0.9011871814727783, accuracy: 0.7132953405380249\n",
      "Batch 222 loss: 0.7697787880897522, accuracy: 0.7131796479225159\n",
      "Batch 223 loss: 0.6342280507087708, accuracy: 0.7133091688156128\n",
      "Batch 224 loss: 0.7135976552963257, accuracy: 0.7134000062942505\n",
      "Training epoch: 32, train accuracy: 71.33999633789062, train loss: 0.7691201965014139, valid accuracy: 62.71942138671875, valid loss: 1.0892852072058052 \n",
      "Batch 0 loss: 0.7222995162010193, accuracy: 0.75\n",
      "Batch 1 loss: 0.6635047197341919, accuracy: 0.75\n",
      "Batch 2 loss: 0.7197121977806091, accuracy: 0.7473958134651184\n",
      "Batch 3 loss: 0.6557971239089966, accuracy: 0.75390625\n",
      "Batch 4 loss: 0.7738630771636963, accuracy: 0.7328125238418579\n",
      "Batch 5 loss: 0.8665484189987183, accuracy: 0.7265625\n",
      "Batch 6 loss: 0.6390587091445923, accuracy: 0.7265625\n",
      "Batch 7 loss: 0.7478340864181519, accuracy: 0.7255859375\n",
      "Batch 8 loss: 0.752264678478241, accuracy: 0.7239583134651184\n",
      "Batch 9 loss: 0.6382463574409485, accuracy: 0.7281249761581421\n",
      "Batch 10 loss: 0.638619065284729, accuracy: 0.7286931872367859\n",
      "Batch 11 loss: 0.7914918661117554, accuracy: 0.7265625\n",
      "Batch 12 loss: 0.7187400460243225, accuracy: 0.7241586446762085\n",
      "Batch 13 loss: 0.7664312720298767, accuracy: 0.7254464030265808\n",
      "Batch 14 loss: 0.8756284117698669, accuracy: 0.723437488079071\n",
      "Batch 15 loss: 0.7232747077941895, accuracy: 0.72509765625\n",
      "Batch 16 loss: 0.7499903440475464, accuracy: 0.724724292755127\n",
      "Batch 17 loss: 0.7763808965682983, accuracy: 0.7252604365348816\n",
      "Batch 18 loss: 0.6851794719696045, accuracy: 0.7261512875556946\n",
      "Batch 19 loss: 0.6847065687179565, accuracy: 0.727734386920929\n",
      "Batch 20 loss: 0.8181983828544617, accuracy: 0.7250744104385376\n",
      "Batch 21 loss: 0.6927344799041748, accuracy: 0.7254971861839294\n",
      "Batch 22 loss: 0.7631294131278992, accuracy: 0.72316575050354\n",
      "Batch 23 loss: 0.7041342258453369, accuracy: 0.72265625\n",
      "Batch 24 loss: 0.7779243588447571, accuracy: 0.7231249809265137\n",
      "Batch 25 loss: 0.6146400570869446, accuracy: 0.7250601053237915\n",
      "Batch 26 loss: 0.7714563012123108, accuracy: 0.7239583134651184\n",
      "Batch 27 loss: 0.720690906047821, accuracy: 0.7240513563156128\n",
      "Batch 28 loss: 0.6084669232368469, accuracy: 0.7260237336158752\n",
      "Batch 29 loss: 0.8405579924583435, accuracy: 0.7239583134651184\n",
      "Batch 30 loss: 0.6816231608390808, accuracy: 0.7230342626571655\n",
      "Batch 31 loss: 0.7744048237800598, accuracy: 0.7236328125\n",
      "Batch 32 loss: 0.7320051193237305, accuracy: 0.7232481241226196\n",
      "Batch 33 loss: 0.7152197360992432, accuracy: 0.7231158018112183\n",
      "Batch 34 loss: 0.6510017514228821, accuracy: 0.7238839268684387\n",
      "Batch 35 loss: 0.6928924322128296, accuracy: 0.7248263955116272\n",
      "Batch 36 loss: 0.7049990296363831, accuracy: 0.724662184715271\n",
      "Batch 37 loss: 0.605209469795227, accuracy: 0.7255345582962036\n",
      "Batch 38 loss: 0.7058537602424622, accuracy: 0.7265625\n",
      "Batch 39 loss: 0.808903694152832, accuracy: 0.7250000238418579\n",
      "Batch 40 loss: 0.6797189116477966, accuracy: 0.7261813879013062\n",
      "Batch 41 loss: 0.7396442294120789, accuracy: 0.7274925708770752\n",
      "Batch 42 loss: 0.8563587069511414, accuracy: 0.7267441749572754\n",
      "Batch 43 loss: 0.750681459903717, accuracy: 0.7270951867103577\n",
      "Batch 44 loss: 0.7205308079719543, accuracy: 0.7256944179534912\n",
      "Batch 45 loss: 0.6208171844482422, accuracy: 0.7263926863670349\n",
      "Batch 46 loss: 0.6962785720825195, accuracy: 0.727393627166748\n",
      "Batch 47 loss: 0.6940318942070007, accuracy: 0.7283528447151184\n",
      "Batch 48 loss: 0.7783727049827576, accuracy: 0.7281568646430969\n",
      "Batch 49 loss: 0.6304645538330078, accuracy: 0.7285937666893005\n",
      "Batch 50 loss: 0.6631955504417419, accuracy: 0.7282475233078003\n",
      "Batch 51 loss: 0.7943610548973083, accuracy: 0.7267127633094788\n",
      "Batch 52 loss: 0.72774338722229, accuracy: 0.7268573045730591\n",
      "Batch 53 loss: 0.845884382724762, accuracy: 0.7258391380310059\n",
      "Batch 54 loss: 1.0892810821533203, accuracy: 0.7238636612892151\n",
      "Batch 55 loss: 0.7531660795211792, accuracy: 0.7236328125\n",
      "Batch 56 loss: 0.7223193645477295, accuracy: 0.7236841917037964\n",
      "Batch 57 loss: 0.7175893783569336, accuracy: 0.7248114347457886\n",
      "Batch 58 loss: 0.7318321466445923, accuracy: 0.725900411605835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 59 loss: 0.9234035611152649, accuracy: 0.7251302003860474\n",
      "Batch 60 loss: 0.6893230676651001, accuracy: 0.7256659865379333\n",
      "Batch 61 loss: 0.7774715423583984, accuracy: 0.7253023982048035\n",
      "Batch 62 loss: 0.8530188798904419, accuracy: 0.724082350730896\n",
      "Batch 63 loss: 0.8025105595588684, accuracy: 0.7235107421875\n",
      "Batch 64 loss: 0.7799456715583801, accuracy: 0.7233173251152039\n",
      "Batch 65 loss: 0.6532188653945923, accuracy: 0.7239583134651184\n",
      "Batch 66 loss: 0.6529488563537598, accuracy: 0.725396454334259\n",
      "Batch 67 loss: 0.7250886559486389, accuracy: 0.7252987027168274\n",
      "Batch 68 loss: 0.7305316925048828, accuracy: 0.7249773740768433\n",
      "Batch 69 loss: 0.6568254828453064, accuracy: 0.7252232432365417\n",
      "Batch 70 loss: 0.6797963380813599, accuracy: 0.7257922291755676\n",
      "Batch 71 loss: 0.8074714541435242, accuracy: 0.7256944179534912\n",
      "Batch 72 loss: 0.7436008453369141, accuracy: 0.7255992889404297\n",
      "Batch 73 loss: 0.7448859214782715, accuracy: 0.7251900434494019\n",
      "Batch 74 loss: 0.7389523386955261, accuracy: 0.7250000238418579\n",
      "Batch 75 loss: 0.6822277307510376, accuracy: 0.7252261638641357\n",
      "Batch 76 loss: 0.6054816246032715, accuracy: 0.7259537577629089\n",
      "Batch 77 loss: 0.7912043333053589, accuracy: 0.7257612347602844\n",
      "Batch 78 loss: 0.8034230470657349, accuracy: 0.7256724834442139\n",
      "Batch 79 loss: 0.906968891620636, accuracy: 0.7247070074081421\n",
      "Batch 80 loss: 0.7532088160514832, accuracy: 0.7250192761421204\n",
      "Batch 81 loss: 0.7078902721405029, accuracy: 0.7253239154815674\n",
      "Batch 82 loss: 0.6723917126655579, accuracy: 0.7255271077156067\n",
      "Batch 83 loss: 0.7496477365493774, accuracy: 0.7255394458770752\n",
      "Batch 84 loss: 0.7770587205886841, accuracy: 0.7252757549285889\n",
      "Batch 85 loss: 0.6970904469490051, accuracy: 0.7255632281303406\n",
      "Batch 86 loss: 0.8553838729858398, accuracy: 0.7248563170433044\n",
      "Batch 87 loss: 0.7690240740776062, accuracy: 0.7248756885528564\n",
      "Batch 88 loss: 0.7031298279762268, accuracy: 0.7248069047927856\n",
      "Batch 89 loss: 0.7554123401641846, accuracy: 0.7251735925674438\n",
      "Batch 90 loss: 0.6339623332023621, accuracy: 0.7258756756782532\n",
      "Batch 91 loss: 0.7794021964073181, accuracy: 0.7263926863670349\n",
      "Batch 92 loss: 0.6257259249687195, accuracy: 0.7268985509872437\n",
      "Batch 93 loss: 0.770321249961853, accuracy: 0.7268949747085571\n",
      "Batch 94 loss: 0.9303692579269409, accuracy: 0.7268914580345154\n",
      "Batch 95 loss: 0.7485336065292358, accuracy: 0.7267252802848816\n",
      "Batch 96 loss: 0.7570488452911377, accuracy: 0.726884663105011\n",
      "Batch 97 loss: 0.6802412867546082, accuracy: 0.7271205186843872\n",
      "Batch 98 loss: 0.7460560202598572, accuracy: 0.7271149158477783\n",
      "Batch 99 loss: 0.7072567939758301, accuracy: 0.7276562452316284\n",
      "Batch 100 loss: 0.7604512572288513, accuracy: 0.7275680899620056\n",
      "Batch 101 loss: 0.93929123878479, accuracy: 0.7265625\n",
      "Batch 102 loss: 0.9574113488197327, accuracy: 0.7261074185371399\n",
      "Batch 103 loss: 0.672091543674469, accuracy: 0.7267127633094788\n",
      "Batch 104 loss: 0.7001706957817078, accuracy: 0.727157711982727\n",
      "Batch 105 loss: 0.7867536544799805, accuracy: 0.7272258400917053\n",
      "Batch 106 loss: 0.8254774808883667, accuracy: 0.7264164686203003\n",
      "Batch 107 loss: 0.8609532117843628, accuracy: 0.7262008190155029\n",
      "Batch 108 loss: 0.7795049548149109, accuracy: 0.7263475060462952\n",
      "Batch 109 loss: 0.7995007634162903, accuracy: 0.7261363863945007\n",
      "Batch 110 loss: 0.6533902883529663, accuracy: 0.7262809872627258\n",
      "Batch 111 loss: 0.7751895785331726, accuracy: 0.7257254719734192\n",
      "Batch 112 loss: 0.7659220695495605, accuracy: 0.7258020043373108\n",
      "Batch 113 loss: 0.5935385227203369, accuracy: 0.7262198328971863\n",
      "Batch 114 loss: 0.7659280896186829, accuracy: 0.7258152365684509\n",
      "Batch 115 loss: 0.8961004614830017, accuracy: 0.7252828478813171\n",
      "Batch 116 loss: 0.6952783465385437, accuracy: 0.7253605723381042\n",
      "Batch 117 loss: 0.7551745176315308, accuracy: 0.7251721620559692\n",
      "Batch 118 loss: 0.7863276600837708, accuracy: 0.724724292755127\n",
      "Batch 119 loss: 0.74918532371521, accuracy: 0.724804699420929\n",
      "Batch 120 loss: 0.8250893950462341, accuracy: 0.7242380976676941\n",
      "Batch 121 loss: 0.8125340938568115, accuracy: 0.7238729596138\n",
      "Batch 122 loss: 0.8297525644302368, accuracy: 0.7232596278190613\n",
      "Batch 123 loss: 0.7266123294830322, accuracy: 0.723412275314331\n",
      "Batch 124 loss: 0.7715959548950195, accuracy: 0.7232499718666077\n",
      "Batch 125 loss: 0.6873457431793213, accuracy: 0.7234002947807312\n",
      "Batch 126 loss: 0.8305529952049255, accuracy: 0.7228715419769287\n",
      "Batch 127 loss: 0.7454343438148499, accuracy: 0.72283935546875\n",
      "Batch 128 loss: 0.7774592638015747, accuracy: 0.7226865291595459\n",
      "Batch 129 loss: 0.6850591897964478, accuracy: 0.7227764129638672\n",
      "Batch 130 loss: 0.6134196519851685, accuracy: 0.7234017252922058\n",
      "Batch 131 loss: 0.892106294631958, accuracy: 0.7228929996490479\n",
      "Batch 132 loss: 0.706565260887146, accuracy: 0.7229205965995789\n",
      "Batch 133 loss: 0.7289337515830994, accuracy: 0.7231809496879578\n",
      "Batch 134 loss: 0.708806037902832, accuracy: 0.7233217358589172\n",
      "Batch 135 loss: 0.780085563659668, accuracy: 0.7234030365943909\n",
      "Batch 136 loss: 0.7823753952980042, accuracy: 0.723426103591919\n",
      "Batch 137 loss: 0.7138185501098633, accuracy: 0.7237884998321533\n",
      "Batch 138 loss: 0.8278006911277771, accuracy: 0.7235836386680603\n",
      "Batch 139 loss: 0.7080360651016235, accuracy: 0.7236607074737549\n",
      "Batch 140 loss: 0.6687310338020325, accuracy: 0.7240691781044006\n",
      "Batch 141 loss: 0.7639834880828857, accuracy: 0.7238666415214539\n",
      "Batch 142 loss: 0.891457200050354, accuracy: 0.7234484553337097\n",
      "Batch 143 loss: 0.8230714201927185, accuracy: 0.7232530117034912\n",
      "Batch 144 loss: 0.7136685848236084, accuracy: 0.7230603694915771\n",
      "Batch 145 loss: 0.734068751335144, accuracy: 0.723244845867157\n",
      "Batch 146 loss: 0.9170094132423401, accuracy: 0.7229485511779785\n",
      "Batch 147 loss: 0.6623939275741577, accuracy: 0.7231841087341309\n",
      "Batch 148 loss: 0.7260132431983948, accuracy: 0.7231543660163879\n",
      "Batch 149 loss: 0.6855298280715942, accuracy: 0.7234895825386047\n",
      "Batch 150 loss: 0.6894344687461853, accuracy: 0.723665177822113\n",
      "Batch 151 loss: 0.74278724193573, accuracy: 0.7235814332962036\n",
      "Batch 152 loss: 0.8731192946434021, accuracy: 0.7232944965362549\n",
      "Batch 153 loss: 0.655255913734436, accuracy: 0.723823070526123\n",
      "Batch 154 loss: 0.7600342035293579, accuracy: 0.7239415049552917\n",
      "Batch 155 loss: 0.6119648218154907, accuracy: 0.7241586446762085\n",
      "Batch 156 loss: 0.674256443977356, accuracy: 0.7242735028266907\n",
      "Batch 157 loss: 0.8189809322357178, accuracy: 0.723842978477478\n",
      "Batch 158 loss: 0.7734061479568481, accuracy: 0.7238109111785889\n",
      "Batch 159 loss: 0.7644932866096497, accuracy: 0.7237793207168579\n",
      "Batch 160 loss: 0.7320057153701782, accuracy: 0.7237966060638428\n",
      "Batch 161 loss: 0.5658958554267883, accuracy: 0.7241994738578796\n",
      "Batch 162 loss: 0.7019705176353455, accuracy: 0.7244536280632019\n",
      "Batch 163 loss: 0.7349107265472412, accuracy: 0.7243711948394775\n",
      "Batch 164 loss: 0.7139430642127991, accuracy: 0.7248106002807617\n",
      "Batch 165 loss: 0.6315081715583801, accuracy: 0.7251035571098328\n",
      "Batch 166 loss: 0.7975069284439087, accuracy: 0.72511225938797\n",
      "Batch 167 loss: 0.8362698554992676, accuracy: 0.7248883843421936\n",
      "Batch 168 loss: 0.7814897894859314, accuracy: 0.7248520851135254\n",
      "Batch 169 loss: 0.9325386881828308, accuracy: 0.724724292755127\n",
      "Batch 170 loss: 0.717436671257019, accuracy: 0.7247350215911865\n",
      "Batch 171 loss: 0.7446128726005554, accuracy: 0.7247456312179565\n",
      "Batch 172 loss: 0.7596688270568848, accuracy: 0.7248013019561768\n",
      "Batch 173 loss: 0.9015254378318787, accuracy: 0.7245420217514038\n",
      "Batch 174 loss: 0.6617150902748108, accuracy: 0.7247321605682373\n",
      "Batch 175 loss: 0.8834993243217468, accuracy: 0.7242986559867859\n",
      "Batch 176 loss: 0.6895665526390076, accuracy: 0.724399745464325\n",
      "Batch 177 loss: 0.7710573077201843, accuracy: 0.7242363095283508\n",
      "Batch 178 loss: 0.748275637626648, accuracy: 0.7246420979499817\n",
      "Batch 179 loss: 0.551343560218811, accuracy: 0.7250434160232544\n",
      "Batch 180 loss: 0.9660657048225403, accuracy: 0.7246633172035217\n",
      "Batch 181 loss: 0.8591330647468567, accuracy: 0.7243732810020447\n",
      "Batch 182 loss: 0.630740225315094, accuracy: 0.7243852615356445\n",
      "Batch 183 loss: 0.6751208901405334, accuracy: 0.724736750125885\n",
      "Batch 184 loss: 0.7720238566398621, accuracy: 0.724662184715271\n",
      "Batch 185 loss: 0.7011646628379822, accuracy: 0.7249664068222046\n",
      "Batch 186 loss: 0.8013631105422974, accuracy: 0.725016713142395\n",
      "Batch 187 loss: 0.7242501974105835, accuracy: 0.7248587012290955\n",
      "Batch 188 loss: 0.8035551905632019, accuracy: 0.7248677015304565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 189 loss: 0.775668203830719, accuracy: 0.7246299386024475\n",
      "Batch 190 loss: 0.657240092754364, accuracy: 0.725089967250824\n",
      "Batch 191 loss: 0.7015337944030762, accuracy: 0.7252604365348816\n",
      "Batch 192 loss: 0.7482050061225891, accuracy: 0.7251052260398865\n",
      "Batch 193 loss: 0.8030532002449036, accuracy: 0.7248308658599854\n",
      "Batch 194 loss: 0.8792532086372375, accuracy: 0.7247195243835449\n",
      "Batch 195 loss: 0.64138263463974, accuracy: 0.725007951259613\n",
      "Batch 196 loss: 0.765398383140564, accuracy: 0.7248175740242004\n",
      "Batch 197 loss: 0.6748040914535522, accuracy: 0.7249447703361511\n",
      "Batch 198 loss: 0.7496892809867859, accuracy: 0.7248743772506714\n",
      "Batch 199 loss: 0.7049832940101624, accuracy: 0.7247655987739563\n",
      "Batch 200 loss: 0.7352021932601929, accuracy: 0.7247745394706726\n",
      "Batch 201 loss: 0.6706574559211731, accuracy: 0.7248221039772034\n",
      "Batch 202 loss: 0.7465779185295105, accuracy: 0.7247921824455261\n",
      "Batch 203 loss: 0.8535562753677368, accuracy: 0.7250689268112183\n",
      "Batch 204 loss: 0.8919687271118164, accuracy: 0.7246189117431641\n",
      "Batch 205 loss: 0.7616159319877625, accuracy: 0.7245145440101624\n",
      "Batch 206 loss: 0.8034877181053162, accuracy: 0.7244489789009094\n",
      "Batch 207 loss: 0.7320860624313354, accuracy: 0.7243089079856873\n",
      "Batch 208 loss: 0.6464523673057556, accuracy: 0.7245439887046814\n",
      "Batch 209 loss: 0.6910778284072876, accuracy: 0.7244047522544861\n",
      "Batch 210 loss: 0.8659501075744629, accuracy: 0.7240077257156372\n",
      "Batch 211 loss: 0.7518855333328247, accuracy: 0.7241671681404114\n",
      "Batch 212 loss: 0.8487632870674133, accuracy: 0.7239216566085815\n",
      "Batch 213 loss: 0.7986929416656494, accuracy: 0.7238609790802002\n",
      "Batch 214 loss: 0.7719343304634094, accuracy: 0.7235101461410522\n",
      "Batch 215 loss: 0.8528477549552917, accuracy: 0.7232711315155029\n",
      "Batch 216 loss: 0.8103935122489929, accuracy: 0.7232862710952759\n",
      "Batch 217 loss: 0.8012529015541077, accuracy: 0.7231221199035645\n",
      "Batch 218 loss: 0.7729092836380005, accuracy: 0.7231021523475647\n",
      "Batch 219 loss: 0.7121091485023499, accuracy: 0.7232954502105713\n",
      "Batch 220 loss: 0.6627954244613647, accuracy: 0.723416268825531\n",
      "Batch 221 loss: 0.8355566263198853, accuracy: 0.7232193350791931\n",
      "Batch 222 loss: 0.9174845218658447, accuracy: 0.7228489518165588\n",
      "Batch 223 loss: 0.6609781384468079, accuracy: 0.7229701280593872\n",
      "Batch 224 loss: 0.6440485715866089, accuracy: 0.7230485081672668\n",
      "Training epoch: 33, train accuracy: 72.30485534667969, train loss: 0.7489141366216872, valid accuracy: 64.05683898925781, valid loss: 1.0614623715137612 \n",
      "Batch 0 loss: 0.7335261106491089, accuracy: 0.7109375\n",
      "Batch 1 loss: 0.8207224607467651, accuracy: 0.70703125\n",
      "Batch 2 loss: 0.6991506218910217, accuracy: 0.7109375\n",
      "Batch 3 loss: 0.6290320158004761, accuracy: 0.728515625\n",
      "Batch 4 loss: 0.7350077033042908, accuracy: 0.7250000238418579\n",
      "Batch 5 loss: 0.7066695094108582, accuracy: 0.7291666865348816\n",
      "Batch 6 loss: 0.6723479628562927, accuracy: 0.7299107313156128\n",
      "Batch 7 loss: 0.7967350482940674, accuracy: 0.7255859375\n",
      "Batch 8 loss: 0.6622380614280701, accuracy: 0.7282986044883728\n",
      "Batch 9 loss: 0.779443085193634, accuracy: 0.7265625\n",
      "Batch 10 loss: 0.719012439250946, accuracy: 0.7272727489471436\n",
      "Batch 11 loss: 0.6394078731536865, accuracy: 0.7291666865348816\n",
      "Batch 12 loss: 0.6264916062355042, accuracy: 0.7283653616905212\n",
      "Batch 13 loss: 0.6899280548095703, accuracy: 0.7276785969734192\n",
      "Batch 14 loss: 0.6627724170684814, accuracy: 0.7270833253860474\n",
      "Batch 15 loss: 0.5516356825828552, accuracy: 0.732421875\n",
      "Batch 16 loss: 0.7308478355407715, accuracy: 0.7329963445663452\n",
      "Batch 17 loss: 0.7210708260536194, accuracy: 0.7365451455116272\n",
      "Batch 18 loss: 0.6116692423820496, accuracy: 0.7384868264198303\n",
      "Batch 19 loss: 0.8156055808067322, accuracy: 0.7347656488418579\n",
      "Batch 20 loss: 0.7636920213699341, accuracy: 0.7332589030265808\n",
      "Batch 21 loss: 0.7454374432563782, accuracy: 0.7325994372367859\n",
      "Batch 22 loss: 0.7829728722572327, accuracy: 0.7323369383811951\n",
      "Batch 23 loss: 0.7871131300926208, accuracy: 0.7291666865348816\n",
      "Batch 24 loss: 0.8612967729568481, accuracy: 0.7253124713897705\n",
      "Batch 25 loss: 0.7583802938461304, accuracy: 0.7238581776618958\n",
      "Batch 26 loss: 0.6516057848930359, accuracy: 0.7245370149612427\n",
      "Batch 27 loss: 0.6754823327064514, accuracy: 0.7260044813156128\n",
      "Batch 28 loss: 0.6672514081001282, accuracy: 0.7268319129943848\n",
      "Batch 29 loss: 0.7431479096412659, accuracy: 0.7270833253860474\n",
      "Batch 30 loss: 0.6981419920921326, accuracy: 0.7268145084381104\n",
      "Batch 31 loss: 0.6589434146881104, accuracy: 0.727783203125\n",
      "Batch 32 loss: 0.7087321877479553, accuracy: 0.7270359992980957\n",
      "Batch 33 loss: 0.6610901355743408, accuracy: 0.728630542755127\n",
      "Batch 34 loss: 0.7210859656333923, accuracy: 0.7287946343421936\n",
      "Batch 35 loss: 0.6986427307128906, accuracy: 0.7289496660232544\n",
      "Batch 36 loss: 0.6872656345367432, accuracy: 0.7295185923576355\n",
      "Batch 37 loss: 0.6357102394104004, accuracy: 0.7310855388641357\n",
      "Batch 38 loss: 0.7411382794380188, accuracy: 0.7311698794364929\n",
      "Batch 39 loss: 0.5431941151618958, accuracy: 0.7328125238418579\n",
      "Batch 40 loss: 0.7846658229827881, accuracy: 0.7322789430618286\n",
      "Batch 41 loss: 0.7560065388679504, accuracy: 0.7321428656578064\n",
      "Batch 42 loss: 0.7714300751686096, accuracy: 0.7321947813034058\n",
      "Batch 43 loss: 0.8652458786964417, accuracy: 0.7308238744735718\n",
      "Batch 44 loss: 0.8341770172119141, accuracy: 0.7295138835906982\n",
      "Batch 45 loss: 0.7800305485725403, accuracy: 0.72877037525177\n",
      "Batch 46 loss: 0.7969202399253845, accuracy: 0.7288896441459656\n",
      "Batch 47 loss: 0.8415460586547852, accuracy: 0.7278645634651184\n",
      "Batch 48 loss: 0.8880369663238525, accuracy: 0.7267219424247742\n",
      "Batch 49 loss: 0.7979739904403687, accuracy: 0.7262499928474426\n",
      "Batch 50 loss: 0.7296065092086792, accuracy: 0.7267156839370728\n",
      "Batch 51 loss: 0.8185250759124756, accuracy: 0.7264122366905212\n",
      "Batch 52 loss: 0.7277922630310059, accuracy: 0.7268573045730591\n",
      "Batch 53 loss: 0.8543791174888611, accuracy: 0.7271412014961243\n",
      "Batch 54 loss: 0.7095066905021667, accuracy: 0.7267045378684998\n",
      "Batch 55 loss: 0.7098056077957153, accuracy: 0.7264229655265808\n",
      "Batch 56 loss: 0.7288699746131897, accuracy: 0.7258771657943726\n",
      "Batch 57 loss: 0.6985536217689514, accuracy: 0.7265625\n",
      "Batch 58 loss: 0.6645436882972717, accuracy: 0.727224588394165\n",
      "Batch 59 loss: 0.7201447486877441, accuracy: 0.7276041507720947\n",
      "Batch 60 loss: 0.8066568970680237, accuracy: 0.7277151346206665\n",
      "Batch 61 loss: 0.7817287445068359, accuracy: 0.7280746102333069\n",
      "Batch 62 loss: 0.8862797021865845, accuracy: 0.727802574634552\n",
      "Batch 63 loss: 0.8851504325866699, accuracy: 0.727783203125\n",
      "Batch 64 loss: 0.6561620831489563, accuracy: 0.7278845906257629\n",
      "Batch 65 loss: 0.7471061944961548, accuracy: 0.7278645634651184\n",
      "Batch 66 loss: 0.8288775086402893, accuracy: 0.7272621393203735\n",
      "Batch 67 loss: 0.7506763935089111, accuracy: 0.7270220518112183\n",
      "Batch 68 loss: 0.742648184299469, accuracy: 0.727921187877655\n",
      "Batch 69 loss: 0.7159039974212646, accuracy: 0.7284598350524902\n",
      "Batch 70 loss: 0.8526137471199036, accuracy: 0.7276628613471985\n",
      "Batch 71 loss: 0.8988980054855347, accuracy: 0.7265625\n",
      "Batch 72 loss: 0.5933329463005066, accuracy: 0.7276327013969421\n",
      "Batch 73 loss: 0.7928345799446106, accuracy: 0.7276182174682617\n",
      "Batch 74 loss: 0.7062022089958191, accuracy: 0.7276041507720947\n",
      "Batch 75 loss: 0.7315134406089783, accuracy: 0.7274876832962036\n",
      "Batch 76 loss: 0.8064721822738647, accuracy: 0.7275770902633667\n",
      "Batch 77 loss: 0.881621778011322, accuracy: 0.7266626358032227\n",
      "Batch 78 loss: 0.82253098487854, accuracy: 0.7269580960273743\n",
      "Batch 79 loss: 0.6939996480941772, accuracy: 0.726855456829071\n",
      "Batch 80 loss: 0.8491311073303223, accuracy: 0.7263696193695068\n",
      "Batch 81 loss: 0.808160126209259, accuracy: 0.7264672517776489\n",
      "Batch 82 loss: 0.9305035471916199, accuracy: 0.7254329919815063\n",
      "Batch 83 loss: 0.7463086843490601, accuracy: 0.7247023582458496\n",
      "Batch 84 loss: 0.9317324161529541, accuracy: 0.7238051295280457\n",
      "Batch 85 loss: 0.6892192363739014, accuracy: 0.7242914438247681\n",
      "Batch 86 loss: 0.8004706501960754, accuracy: 0.7240481376647949\n",
      "Batch 87 loss: 0.6506384015083313, accuracy: 0.7241654992103577\n",
      "Batch 88 loss: 0.8060558438301086, accuracy: 0.7241924405097961\n",
      "Batch 89 loss: 0.7762807011604309, accuracy: 0.7238715291023254\n",
      "Batch 90 loss: 0.6129274368286133, accuracy: 0.7245879173278809\n",
      "Batch 91 loss: 0.7256029844284058, accuracy: 0.7246943116188049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 92 loss: 0.675815761089325, accuracy: 0.7248823642730713\n",
      "Batch 93 loss: 0.7959122657775879, accuracy: 0.7242353558540344\n",
      "Batch 94 loss: 0.8322393894195557, accuracy: 0.7241776585578918\n",
      "Batch 95 loss: 0.7298495173454285, accuracy: 0.723876953125\n",
      "Batch 96 loss: 0.6655775308609009, accuracy: 0.7238240838050842\n",
      "Batch 97 loss: 0.6938403248786926, accuracy: 0.7240114808082581\n",
      "Batch 98 loss: 0.6458373665809631, accuracy: 0.7245107293128967\n",
      "Batch 99 loss: 0.5925666093826294, accuracy: 0.725390613079071\n",
      "Batch 100 loss: 0.8448390364646912, accuracy: 0.7246286869049072\n",
      "Batch 101 loss: 0.6875715851783752, accuracy: 0.7250306606292725\n",
      "Batch 102 loss: 0.799591600894928, accuracy: 0.7249696850776672\n",
      "Batch 103 loss: 0.8467379212379456, accuracy: 0.7242337465286255\n",
      "Batch 104 loss: 0.6829125881195068, accuracy: 0.7244791388511658\n",
      "Batch 105 loss: 0.8400831818580627, accuracy: 0.7241303324699402\n",
      "Batch 106 loss: 0.82297682762146, accuracy: 0.7233498692512512\n",
      "Batch 107 loss: 0.6589321494102478, accuracy: 0.7233796119689941\n",
      "Batch 108 loss: 0.8213073015213013, accuracy: 0.7227637767791748\n",
      "Batch 109 loss: 0.7708075642585754, accuracy: 0.72265625\n",
      "Batch 110 loss: 0.7037802934646606, accuracy: 0.7227618098258972\n",
      "Batch 111 loss: 0.8478010892868042, accuracy: 0.7225865125656128\n",
      "Batch 112 loss: 0.6906980276107788, accuracy: 0.7229673862457275\n",
      "Batch 113 loss: 0.7349576354026794, accuracy: 0.7233415842056274\n",
      "Batch 114 loss: 0.8405717611312866, accuracy: 0.7228260636329651\n",
      "Batch 115 loss: 0.5459228157997131, accuracy: 0.7238011956214905\n",
      "Batch 116 loss: 0.7697926163673401, accuracy: 0.7234241366386414\n",
      "Batch 117 loss: 0.7078653573989868, accuracy: 0.7234507203102112\n",
      "Batch 118 loss: 0.6404754519462585, accuracy: 0.7239364385604858\n",
      "Batch 119 loss: 0.7176803350448608, accuracy: 0.7240885496139526\n",
      "Batch 120 loss: 0.655655026435852, accuracy: 0.7245609760284424\n",
      "Batch 121 loss: 0.7605739831924438, accuracy: 0.7245132923126221\n",
      "Batch 122 loss: 0.7097806334495544, accuracy: 0.7243394255638123\n",
      "Batch 123 loss: 0.739898681640625, accuracy: 0.7241053581237793\n",
      "Batch 124 loss: 0.6598005294799805, accuracy: 0.7245625257492065\n",
      "Batch 125 loss: 0.8718938827514648, accuracy: 0.7244543433189392\n",
      "Batch 126 loss: 0.693557620048523, accuracy: 0.724593997001648\n",
      "Batch 127 loss: 0.658058762550354, accuracy: 0.72528076171875\n",
      "Batch 128 loss: 0.6213133335113525, accuracy: 0.7258357405662537\n",
      "Batch 129 loss: 0.7207290530204773, accuracy: 0.725781261920929\n",
      "Batch 130 loss: 0.7081524133682251, accuracy: 0.7256679534912109\n",
      "Batch 131 loss: 0.7738603949546814, accuracy: 0.7257338762283325\n",
      "Batch 132 loss: 0.7554369568824768, accuracy: 0.7255638837814331\n",
      "Batch 133 loss: 0.7751410007476807, accuracy: 0.725396454334259\n",
      "Batch 134 loss: 0.7200593948364258, accuracy: 0.7254050970077515\n",
      "Batch 135 loss: 0.6251423954963684, accuracy: 0.7257582545280457\n",
      "Batch 136 loss: 0.7506746053695679, accuracy: 0.7258781790733337\n",
      "Batch 137 loss: 0.6218579411506653, accuracy: 0.7261661887168884\n",
      "Batch 138 loss: 0.6227666735649109, accuracy: 0.7263938784599304\n",
      "Batch 139 loss: 0.6885432600975037, accuracy: 0.7264509201049805\n",
      "Batch 140 loss: 0.8546038269996643, accuracy: 0.7261746525764465\n",
      "Batch 141 loss: 0.6548025012016296, accuracy: 0.7264524698257446\n",
      "Batch 142 loss: 0.7807753682136536, accuracy: 0.726507842540741\n",
      "Batch 143 loss: 0.6400477290153503, accuracy: 0.7267252802848816\n",
      "Batch 144 loss: 0.7284067273139954, accuracy: 0.726616382598877\n",
      "Batch 145 loss: 0.7231174111366272, accuracy: 0.7267230153083801\n",
      "Batch 146 loss: 0.6781271696090698, accuracy: 0.7266156673431396\n",
      "Batch 147 loss: 0.6525558829307556, accuracy: 0.7267208695411682\n",
      "Batch 148 loss: 0.7891808152198792, accuracy: 0.7265625\n",
      "Batch 149 loss: 0.6133848428726196, accuracy: 0.7269271016120911\n",
      "Batch 150 loss: 0.7062707543373108, accuracy: 0.7270281314849854\n",
      "Batch 151 loss: 0.6775578856468201, accuracy: 0.7272306680679321\n",
      "Batch 152 loss: 0.8295153379440308, accuracy: 0.726919949054718\n",
      "Batch 153 loss: 0.7368748188018799, accuracy: 0.7267653942108154\n",
      "Batch 154 loss: 0.6465399265289307, accuracy: 0.7268145084381104\n",
      "Batch 155 loss: 0.8040478825569153, accuracy: 0.7265625\n",
      "Batch 156 loss: 0.7250936031341553, accuracy: 0.7269605994224548\n",
      "Batch 157 loss: 0.7431584596633911, accuracy: 0.7270075082778931\n",
      "Batch 158 loss: 0.6455696821212769, accuracy: 0.7271521091461182\n",
      "Batch 159 loss: 0.7681020498275757, accuracy: 0.7271972894668579\n",
      "Batch 160 loss: 0.844132661819458, accuracy: 0.7267080545425415\n",
      "Batch 161 loss: 0.5374234914779663, accuracy: 0.7271894216537476\n",
      "Batch 162 loss: 0.6688931584358215, accuracy: 0.7272814512252808\n",
      "Batch 163 loss: 0.7610870599746704, accuracy: 0.7275629043579102\n",
      "Batch 164 loss: 1.0078562498092651, accuracy: 0.7269886136054993\n",
      "Batch 165 loss: 0.7139934301376343, accuracy: 0.7270331382751465\n",
      "Batch 166 loss: 0.776866614818573, accuracy: 0.7268431782722473\n",
      "Batch 167 loss: 0.6898691654205322, accuracy: 0.726934552192688\n",
      "Batch 168 loss: 0.8116381168365479, accuracy: 0.7264700531959534\n",
      "Batch 169 loss: 0.8006097674369812, accuracy: 0.7261948585510254\n",
      "Batch 170 loss: 0.697343647480011, accuracy: 0.726288378238678\n",
      "Batch 171 loss: 0.6838734149932861, accuracy: 0.7263354063034058\n",
      "Batch 172 loss: 0.5866925716400146, accuracy: 0.7266979813575745\n",
      "Batch 173 loss: 0.7633255124092102, accuracy: 0.7267869710922241\n",
      "Batch 174 loss: 0.802493155002594, accuracy: 0.7264285683631897\n",
      "Batch 175 loss: 0.7267754673957825, accuracy: 0.7267400622367859\n",
      "Batch 176 loss: 0.8862665295600891, accuracy: 0.7264300584793091\n",
      "Batch 177 loss: 0.8061994314193726, accuracy: 0.7264308333396912\n",
      "Batch 178 loss: 0.6903707385063171, accuracy: 0.7264315485954285\n",
      "Batch 179 loss: 0.7135413885116577, accuracy: 0.7262153029441833\n",
      "Batch 180 loss: 0.8366177678108215, accuracy: 0.7262603640556335\n",
      "Batch 181 loss: 0.8208016157150269, accuracy: 0.726219117641449\n",
      "Batch 182 loss: 0.9833256006240845, accuracy: 0.7258367538452148\n",
      "Batch 183 loss: 0.7992610335350037, accuracy: 0.7256283760070801\n",
      "Batch 184 loss: 0.7392776608467102, accuracy: 0.725633442401886\n",
      "Batch 185 loss: 0.6981809735298157, accuracy: 0.7256384491920471\n",
      "Batch 186 loss: 0.7058160901069641, accuracy: 0.7258522510528564\n",
      "Batch 187 loss: 0.732190728187561, accuracy: 0.7257729172706604\n",
      "Batch 188 loss: 0.8434076309204102, accuracy: 0.7254464030265808\n",
      "Batch 189 loss: 0.7515950798988342, accuracy: 0.7253700494766235\n",
      "Batch 190 loss: 0.7291921377182007, accuracy: 0.725376307964325\n",
      "Batch 191 loss: 0.7011016607284546, accuracy: 0.7254231572151184\n",
      "Batch 192 loss: 0.7507097721099854, accuracy: 0.7254695892333984\n",
      "Batch 193 loss: 0.6125690340995789, accuracy: 0.7254348993301392\n",
      "Batch 194 loss: 0.7253210544586182, accuracy: 0.7254006266593933\n",
      "Batch 195 loss: 0.7403241991996765, accuracy: 0.7254862785339355\n",
      "Batch 196 loss: 0.673709511756897, accuracy: 0.7258090376853943\n",
      "Batch 197 loss: 0.7053645849227905, accuracy: 0.7259706258773804\n",
      "Batch 198 loss: 0.7300499081611633, accuracy: 0.7259736061096191\n",
      "Batch 199 loss: 0.7497299313545227, accuracy: 0.7262499928474426\n",
      "Batch 200 loss: 0.9221683144569397, accuracy: 0.7259795069694519\n",
      "Batch 201 loss: 0.799363911151886, accuracy: 0.7258276343345642\n",
      "Batch 202 loss: 0.9211570620536804, accuracy: 0.7255234122276306\n",
      "Batch 203 loss: 0.7119361162185669, accuracy: 0.7256433963775635\n",
      "Batch 204 loss: 0.6930619478225708, accuracy: 0.7257621884346008\n",
      "Batch 205 loss: 0.782615065574646, accuracy: 0.7253868579864502\n",
      "Batch 206 loss: 0.9009853601455688, accuracy: 0.7251660823822021\n",
      "Batch 207 loss: 0.74275803565979, accuracy: 0.7250601053237915\n",
      "Batch 208 loss: 0.7456014156341553, accuracy: 0.7249925136566162\n",
      "Batch 209 loss: 0.761900782585144, accuracy: 0.7248511910438538\n",
      "Batch 210 loss: 0.681706964969635, accuracy: 0.7249704003334045\n",
      "Batch 211 loss: 0.7480977773666382, accuracy: 0.7247567772865295\n",
      "Batch 212 loss: 0.6556931138038635, accuracy: 0.7248019576072693\n",
      "Batch 213 loss: 0.7575049996376038, accuracy: 0.7248831987380981\n",
      "Batch 214 loss: 0.6676634550094604, accuracy: 0.7251816987991333\n",
      "Batch 215 loss: 0.7814277410507202, accuracy: 0.7250072360038757\n",
      "Batch 216 loss: 0.8452936410903931, accuracy: 0.7247983813285828\n",
      "Batch 217 loss: 0.8335397839546204, accuracy: 0.7247347831726074\n",
      "Batch 218 loss: 0.6283098459243774, accuracy: 0.724743127822876\n",
      "Batch 219 loss: 0.7038391828536987, accuracy: 0.7247514128684998\n",
      "Batch 220 loss: 0.6181497573852539, accuracy: 0.7249717116355896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 221 loss: 0.7159815430641174, accuracy: 0.7250492572784424\n",
      "Batch 222 loss: 0.7352315187454224, accuracy: 0.7249509692192078\n",
      "Batch 223 loss: 0.740275502204895, accuracy: 0.7249232530593872\n",
      "Batch 224 loss: 0.6815377473831177, accuracy: 0.7250339388847351\n",
      "Training epoch: 34, train accuracy: 72.5033950805664, train loss: 0.7399591612815857, valid accuracy: 63.7782096862793, valid loss: 1.1229356198475278 \n",
      "Batch 0 loss: 0.6142444610595703, accuracy: 0.796875\n",
      "Batch 1 loss: 0.6408260464668274, accuracy: 0.765625\n",
      "Batch 2 loss: 0.8204448819160461, accuracy: 0.7317708134651184\n",
      "Batch 3 loss: 0.6462548971176147, accuracy: 0.74609375\n",
      "Batch 4 loss: 0.7783946990966797, accuracy: 0.7328125238418579\n",
      "Batch 5 loss: 0.7454696893692017, accuracy: 0.7356770634651184\n",
      "Batch 6 loss: 0.7167628407478333, accuracy: 0.7321428656578064\n",
      "Batch 7 loss: 0.7826106548309326, accuracy: 0.728515625\n",
      "Batch 8 loss: 0.7458109259605408, accuracy: 0.7309027910232544\n",
      "Batch 9 loss: 0.6993345022201538, accuracy: 0.7328125238418579\n",
      "Batch 10 loss: 0.7748436331748962, accuracy: 0.7294034361839294\n",
      "Batch 11 loss: 0.7510106563568115, accuracy: 0.7311198115348816\n",
      "Batch 12 loss: 0.6429072022438049, accuracy: 0.7307692170143127\n",
      "Batch 13 loss: 0.6221850514411926, accuracy: 0.7338169813156128\n",
      "Batch 14 loss: 0.7045210003852844, accuracy: 0.7333333492279053\n",
      "Batch 15 loss: 0.6001099944114685, accuracy: 0.73486328125\n",
      "Batch 16 loss: 0.714166522026062, accuracy: 0.7320771813392639\n",
      "Batch 17 loss: 0.6677421927452087, accuracy: 0.7339409589767456\n",
      "Batch 18 loss: 0.7629197239875793, accuracy: 0.7331414222717285\n",
      "Batch 19 loss: 0.5635050535202026, accuracy: 0.737109363079071\n",
      "Batch 20 loss: 0.8460243344306946, accuracy: 0.7354910969734192\n",
      "Batch 21 loss: 0.6192310452461243, accuracy: 0.7354403138160706\n",
      "Batch 22 loss: 0.8963229656219482, accuracy: 0.7333559989929199\n",
      "Batch 23 loss: 0.7769622206687927, accuracy: 0.7301432490348816\n",
      "Batch 24 loss: 0.7576544284820557, accuracy: 0.7300000190734863\n",
      "Batch 25 loss: 0.5875647664070129, accuracy: 0.7322716116905212\n",
      "Batch 26 loss: 0.7256094217300415, accuracy: 0.7340856194496155\n",
      "Batch 27 loss: 0.6548222303390503, accuracy: 0.7340959906578064\n",
      "Batch 28 loss: 0.687594473361969, accuracy: 0.7349137663841248\n",
      "Batch 29 loss: 0.6781280040740967, accuracy: 0.7346354126930237\n",
      "Batch 30 loss: 0.760125994682312, accuracy: 0.7326108813285828\n",
      "Batch 31 loss: 0.7162085175514221, accuracy: 0.73193359375\n",
      "Batch 32 loss: 0.8467641472816467, accuracy: 0.7312973737716675\n",
      "Batch 33 loss: 0.7770922780036926, accuracy: 0.7300091981887817\n",
      "Batch 34 loss: 0.6652474999427795, accuracy: 0.729687511920929\n",
      "Batch 35 loss: 0.5256990790367126, accuracy: 0.7328559160232544\n",
      "Batch 36 loss: 0.7228298187255859, accuracy: 0.7328969836235046\n",
      "Batch 37 loss: 0.8571428656578064, accuracy: 0.7312911152839661\n",
      "Batch 38 loss: 0.9664691090583801, accuracy: 0.7293669581413269\n",
      "Batch 39 loss: 0.6619858741760254, accuracy: 0.7289062738418579\n",
      "Batch 40 loss: 0.6346630454063416, accuracy: 0.730182945728302\n",
      "Batch 41 loss: 0.8652950525283813, accuracy: 0.7289806604385376\n",
      "Batch 42 loss: 0.709740400314331, accuracy: 0.7298328280448914\n",
      "Batch 43 loss: 0.6879724264144897, accuracy: 0.7301136255264282\n",
      "Batch 44 loss: 0.9091524481773376, accuracy: 0.7277777791023254\n",
      "Batch 45 loss: 0.6232768297195435, accuracy: 0.7284306883811951\n",
      "Batch 46 loss: 0.8039687275886536, accuracy: 0.727892279624939\n",
      "Batch 47 loss: 0.7286935448646545, accuracy: 0.728515625\n",
      "Batch 48 loss: 0.6724328398704529, accuracy: 0.7291135191917419\n",
      "Batch 49 loss: 0.7123640179634094, accuracy: 0.7287499904632568\n",
      "Batch 50 loss: 0.673358678817749, accuracy: 0.7293198704719543\n",
      "Batch 51 loss: 0.6394944190979004, accuracy: 0.7298678159713745\n",
      "Batch 52 loss: 0.6939557790756226, accuracy: 0.7302476167678833\n",
      "Batch 53 loss: 0.9060882925987244, accuracy: 0.7296006679534912\n",
      "Batch 54 loss: 0.8678482174873352, accuracy: 0.7281249761581421\n",
      "Batch 55 loss: 0.7116124033927917, accuracy: 0.728515625\n",
      "Batch 56 loss: 0.7210397124290466, accuracy: 0.7283443212509155\n",
      "Batch 57 loss: 0.6242682933807373, accuracy: 0.728852391242981\n",
      "Batch 58 loss: 0.8675373196601868, accuracy: 0.7282838821411133\n",
      "Batch 59 loss: 0.6681095957756042, accuracy: 0.728515625\n",
      "Batch 60 loss: 0.7233301997184753, accuracy: 0.728227436542511\n",
      "Batch 61 loss: 0.6887316703796387, accuracy: 0.7283266186714172\n",
      "Batch 62 loss: 0.8173085451126099, accuracy: 0.7276785969734192\n",
      "Batch 63 loss: 0.7835192084312439, accuracy: 0.7269287109375\n",
      "Batch 64 loss: 0.7437920570373535, accuracy: 0.7268028855323792\n",
      "Batch 65 loss: 0.6383062601089478, accuracy: 0.7277461886405945\n",
      "Batch 66 loss: 0.7254255414009094, accuracy: 0.7272621393203735\n",
      "Batch 67 loss: 0.715403139591217, accuracy: 0.7270220518112183\n",
      "Batch 68 loss: 0.7044569849967957, accuracy: 0.7271286249160767\n",
      "Batch 69 loss: 0.7304654717445374, accuracy: 0.727343738079071\n",
      "Batch 70 loss: 0.7772748470306396, accuracy: 0.7275528311729431\n",
      "Batch 71 loss: 0.7743321061134338, accuracy: 0.7275390625\n",
      "Batch 72 loss: 0.6554784774780273, accuracy: 0.7277397513389587\n",
      "Batch 73 loss: 0.6043251156806946, accuracy: 0.7278293967247009\n",
      "Batch 74 loss: 0.7654275894165039, accuracy: 0.7270833253860474\n",
      "Batch 75 loss: 0.7794018387794495, accuracy: 0.7273848652839661\n",
      "Batch 76 loss: 0.6502310037612915, accuracy: 0.7282873392105103\n",
      "Batch 77 loss: 0.8625161647796631, accuracy: 0.7278645634651184\n",
      "Batch 78 loss: 0.74566251039505, accuracy: 0.7281447649002075\n",
      "Batch 79 loss: 0.7086964845657349, accuracy: 0.728222668170929\n",
      "Batch 80 loss: 0.6420741677284241, accuracy: 0.7287808656692505\n",
      "Batch 81 loss: 0.7845487594604492, accuracy: 0.7283726930618286\n",
      "Batch 82 loss: 0.7070856690406799, accuracy: 0.7284450531005859\n",
      "Batch 83 loss: 0.6885783672332764, accuracy: 0.7289806604385376\n",
      "Batch 84 loss: 0.7543319463729858, accuracy: 0.729136049747467\n",
      "Batch 85 loss: 0.6917346715927124, accuracy: 0.7288335561752319\n",
      "Batch 86 loss: 0.7483494877815247, accuracy: 0.7288972735404968\n",
      "Batch 87 loss: 0.7437541484832764, accuracy: 0.7282493114471436\n",
      "Batch 88 loss: 0.6823111176490784, accuracy: 0.7287570238113403\n",
      "Batch 89 loss: 0.6642554998397827, accuracy: 0.7287326455116272\n",
      "Batch 90 loss: 0.7748895287513733, accuracy: 0.7283653616905212\n",
      "Batch 91 loss: 0.809924840927124, accuracy: 0.72800612449646\n",
      "Batch 92 loss: 0.6841197609901428, accuracy: 0.7284946441650391\n",
      "Batch 93 loss: 0.6441864371299744, accuracy: 0.7295545339584351\n",
      "Batch 94 loss: 0.8199382424354553, accuracy: 0.7291941046714783\n",
      "Batch 95 loss: 0.8171539902687073, accuracy: 0.7284342646598816\n",
      "Batch 96 loss: 0.6337738037109375, accuracy: 0.7286565899848938\n",
      "Batch 97 loss: 0.6590770483016968, accuracy: 0.7290338277816772\n",
      "Batch 98 loss: 0.816571831703186, accuracy: 0.7286931872367859\n",
      "Batch 99 loss: 0.6371490955352783, accuracy: 0.7293750047683716\n",
      "Batch 100 loss: 0.7447312474250793, accuracy: 0.7291924357414246\n",
      "Batch 101 loss: 0.6432442665100098, accuracy: 0.7295496463775635\n",
      "Batch 102 loss: 0.6761634945869446, accuracy: 0.7298998832702637\n",
      "Batch 103 loss: 0.7466087341308594, accuracy: 0.7294921875\n",
      "Batch 104 loss: 0.7833997011184692, accuracy: 0.7287202477455139\n",
      "Batch 105 loss: 0.8550817966461182, accuracy: 0.7284787893295288\n",
      "Batch 106 loss: 0.7490135431289673, accuracy: 0.7284608483314514\n",
      "Batch 107 loss: 0.7538836002349854, accuracy: 0.728515625\n",
      "Batch 108 loss: 0.960495114326477, accuracy: 0.7280676364898682\n",
      "Batch 109 loss: 0.7594488859176636, accuracy: 0.7279829382896423\n",
      "Batch 110 loss: 0.7821570038795471, accuracy: 0.7280405163764954\n",
      "Batch 111 loss: 0.7593889236450195, accuracy: 0.7275390625\n",
      "Batch 112 loss: 0.7767981886863708, accuracy: 0.7269773483276367\n",
      "Batch 113 loss: 0.6758472323417664, accuracy: 0.7270421981811523\n",
      "Batch 114 loss: 0.8105185031890869, accuracy: 0.726902186870575\n",
      "Batch 115 loss: 0.838945209980011, accuracy: 0.7267645597457886\n",
      "Batch 116 loss: 0.8387755155563354, accuracy: 0.726896345615387\n",
      "Batch 117 loss: 0.7312732934951782, accuracy: 0.7270921468734741\n",
      "Batch 118 loss: 0.7754310369491577, accuracy: 0.7268907427787781\n",
      "Batch 119 loss: 0.856407880783081, accuracy: 0.7261718511581421\n",
      "Batch 120 loss: 0.8385099172592163, accuracy: 0.7262396812438965\n",
      "Batch 121 loss: 0.7482843399047852, accuracy: 0.7264984846115112\n",
      "Batch 122 loss: 0.6341400742530823, accuracy: 0.7268165946006775\n",
      "Batch 123 loss: 0.7321910262107849, accuracy: 0.7269405126571655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 124 loss: 0.7658593654632568, accuracy: 0.7267500162124634\n",
      "Batch 125 loss: 0.813217043876648, accuracy: 0.7260044813156128\n",
      "Batch 126 loss: 0.7775161266326904, accuracy: 0.7260088324546814\n",
      "Batch 127 loss: 0.6967830061912537, accuracy: 0.72625732421875\n",
      "Batch 128 loss: 0.7347627878189087, accuracy: 0.7261991500854492\n",
      "Batch 129 loss: 0.8045517206192017, accuracy: 0.7259615659713745\n",
      "Batch 130 loss: 0.6613132953643799, accuracy: 0.7262642979621887\n",
      "Batch 131 loss: 0.8194386959075928, accuracy: 0.7263849377632141\n",
      "Batch 132 loss: 0.8386839032173157, accuracy: 0.7260925769805908\n",
      "Batch 133 loss: 0.6914035677909851, accuracy: 0.7260378003120422\n",
      "Batch 134 loss: 0.8345983624458313, accuracy: 0.7258680462837219\n",
      "Batch 135 loss: 0.7858594655990601, accuracy: 0.7258157134056091\n",
      "Batch 136 loss: 0.7364996671676636, accuracy: 0.7261062860488892\n",
      "Batch 137 loss: 0.705518901348114, accuracy: 0.7263360619544983\n",
      "Batch 138 loss: 0.8454638123512268, accuracy: 0.7262252569198608\n",
      "Batch 139 loss: 0.8345400094985962, accuracy: 0.7260044813156128\n",
      "Batch 140 loss: 0.7006756663322449, accuracy: 0.7263962626457214\n",
      "Batch 141 loss: 0.7974531054496765, accuracy: 0.7262324094772339\n",
      "Batch 142 loss: 0.7576696872711182, accuracy: 0.7260708212852478\n",
      "Batch 143 loss: 0.671438455581665, accuracy: 0.72607421875\n",
      "Batch 144 loss: 0.631212592124939, accuracy: 0.7263469696044922\n",
      "Batch 145 loss: 0.6512224078178406, accuracy: 0.7267230153083801\n",
      "Batch 146 loss: 0.8394313454627991, accuracy: 0.7266687750816345\n",
      "Batch 147 loss: 0.7710040211677551, accuracy: 0.7268264293670654\n",
      "Batch 148 loss: 0.712008535861969, accuracy: 0.7267722487449646\n",
      "Batch 149 loss: 0.7154093980789185, accuracy: 0.7267187237739563\n",
      "Batch 150 loss: 0.7821731567382812, accuracy: 0.7268729209899902\n",
      "Batch 151 loss: 0.5264864563941956, accuracy: 0.7276932597160339\n",
      "Batch 152 loss: 0.8392980694770813, accuracy: 0.7277369499206543\n",
      "Batch 153 loss: 0.7542641758918762, accuracy: 0.7273741960525513\n",
      "Batch 154 loss: 0.796124279499054, accuracy: 0.7274697422981262\n",
      "Batch 155 loss: 0.7606567740440369, accuracy: 0.7272636294364929\n",
      "Batch 156 loss: 0.7720895409584045, accuracy: 0.7273586988449097\n",
      "Batch 157 loss: 0.6969560980796814, accuracy: 0.727304220199585\n",
      "Batch 158 loss: 0.8394042253494263, accuracy: 0.7272012829780579\n",
      "Batch 159 loss: 0.6712788343429565, accuracy: 0.7273925542831421\n",
      "Batch 160 loss: 0.6977810859680176, accuracy: 0.7270477414131165\n",
      "Batch 161 loss: 0.757318377494812, accuracy: 0.7274305820465088\n",
      "Batch 162 loss: 0.8173699975013733, accuracy: 0.727425217628479\n",
      "Batch 163 loss: 0.8547901511192322, accuracy: 0.7272294163703918\n",
      "Batch 164 loss: 0.7154574394226074, accuracy: 0.7270833253860474\n",
      "Batch 165 loss: 0.6591208577156067, accuracy: 0.7269860506057739\n",
      "Batch 166 loss: 0.7913106679916382, accuracy: 0.7266560792922974\n",
      "Batch 167 loss: 0.653901994228363, accuracy: 0.7270740270614624\n",
      "Batch 168 loss: 0.7532944679260254, accuracy: 0.7269785404205322\n",
      "Batch 169 loss: 0.7590791583061218, accuracy: 0.7268841862678528\n",
      "Batch 170 loss: 0.7956541180610657, accuracy: 0.7270650863647461\n",
      "Batch 171 loss: 0.6606922149658203, accuracy: 0.727380096912384\n",
      "Batch 172 loss: 0.7071310877799988, accuracy: 0.7274205088615417\n",
      "Batch 173 loss: 0.6879268884658813, accuracy: 0.7278196811676025\n",
      "Batch 174 loss: 0.8984777331352234, accuracy: 0.7274107336997986\n",
      "Batch 175 loss: 0.8655685186386108, accuracy: 0.72705078125\n",
      "Batch 176 loss: 0.7709980607032776, accuracy: 0.726959764957428\n",
      "Batch 177 loss: 0.7236708998680115, accuracy: 0.7268258333206177\n",
      "Batch 178 loss: 0.7397773265838623, accuracy: 0.7267370820045471\n",
      "Batch 179 loss: 0.6437265276908875, accuracy: 0.7269096970558167\n",
      "Batch 180 loss: 0.6769155263900757, accuracy: 0.7268646359443665\n",
      "Batch 181 loss: 0.7358478307723999, accuracy: 0.726905882358551\n",
      "Batch 182 loss: 0.6222151517868042, accuracy: 0.7270748019218445\n",
      "Batch 183 loss: 0.7163594961166382, accuracy: 0.7272418737411499\n",
      "Batch 184 loss: 0.705452561378479, accuracy: 0.7273648381233215\n",
      "Batch 185 loss: 0.7171101570129395, accuracy: 0.727150559425354\n",
      "Batch 186 loss: 0.7974743843078613, accuracy: 0.727230966091156\n",
      "Batch 187 loss: 0.6586580872535706, accuracy: 0.7276014089584351\n",
      "Batch 188 loss: 0.7269749641418457, accuracy: 0.7277199029922485\n",
      "Batch 189 loss: 0.7700768113136292, accuracy: 0.7273848652839661\n",
      "Batch 190 loss: 0.6209074854850769, accuracy: 0.7275850772857666\n",
      "Batch 191 loss: 0.6032732725143433, accuracy: 0.72802734375\n",
      "Batch 192 loss: 0.6602410674095154, accuracy: 0.728141188621521\n",
      "Batch 193 loss: 0.8506680727005005, accuracy: 0.7280122637748718\n",
      "Batch 194 loss: 0.7924134731292725, accuracy: 0.7279247045516968\n",
      "Batch 195 loss: 0.8665647506713867, accuracy: 0.7275988459587097\n",
      "Batch 196 loss: 0.7737358808517456, accuracy: 0.7275539636611938\n",
      "Batch 197 loss: 0.8181592226028442, accuracy: 0.7272727489471436\n",
      "Batch 198 loss: 0.7953846454620361, accuracy: 0.7274654507637024\n",
      "Batch 199 loss: 0.7087103724479675, accuracy: 0.7275000214576721\n",
      "Batch 200 loss: 0.7355522513389587, accuracy: 0.72722327709198\n",
      "Batch 201 loss: 0.8147032260894775, accuracy: 0.7270652651786804\n",
      "Batch 202 loss: 0.7869128584861755, accuracy: 0.7267934083938599\n",
      "Batch 203 loss: 0.7351519465446472, accuracy: 0.7267540097236633\n",
      "Batch 204 loss: 0.6983944177627563, accuracy: 0.7268673777580261\n",
      "Batch 205 loss: 0.7368356585502625, accuracy: 0.7269037961959839\n",
      "Batch 206 loss: 0.6378071308135986, accuracy: 0.727053165435791\n",
      "Batch 207 loss: 0.787557065486908, accuracy: 0.7268629670143127\n",
      "Batch 208 loss: 0.5853970646858215, accuracy: 0.7270858287811279\n",
      "Batch 209 loss: 0.8426123261451721, accuracy: 0.7270461320877075\n",
      "Batch 210 loss: 0.9787980318069458, accuracy: 0.7270808815956116\n",
      "Batch 211 loss: 0.9185606241226196, accuracy: 0.7270784378051758\n",
      "Batch 212 loss: 0.6386553645133972, accuracy: 0.7272593975067139\n",
      "Batch 213 loss: 0.7122458815574646, accuracy: 0.7272561192512512\n",
      "Batch 214 loss: 0.6846813559532166, accuracy: 0.7272165417671204\n",
      "Batch 215 loss: 0.83659428358078, accuracy: 0.7268518805503845\n",
      "Batch 216 loss: 0.896172285079956, accuracy: 0.726670503616333\n",
      "Batch 217 loss: 0.7823914289474487, accuracy: 0.7267058491706848\n",
      "Batch 218 loss: 0.7983542084693909, accuracy: 0.7267765402793884\n",
      "Batch 219 loss: 0.8000711798667908, accuracy: 0.7263139486312866\n",
      "Batch 220 loss: 0.7804958820343018, accuracy: 0.7263503670692444\n",
      "Batch 221 loss: 0.6306748986244202, accuracy: 0.7265625\n",
      "Batch 222 loss: 0.6600936651229858, accuracy: 0.7265625\n",
      "Batch 223 loss: 0.7691709399223328, accuracy: 0.7265276312828064\n",
      "Batch 224 loss: 0.6799230575561523, accuracy: 0.7265665531158447\n",
      "Training epoch: 35, train accuracy: 72.65666198730469, train loss: 0.7397480546103583, valid accuracy: 63.19308853149414, valid loss: 1.089421438759771 \n",
      "Batch 0 loss: 0.7621335387229919, accuracy: 0.6796875\n",
      "Batch 1 loss: 0.632803738117218, accuracy: 0.73046875\n",
      "Batch 2 loss: 0.6159355044364929, accuracy: 0.734375\n",
      "Batch 3 loss: 0.9546486139297485, accuracy: 0.701171875\n",
      "Batch 4 loss: 0.7635775208473206, accuracy: 0.707812488079071\n",
      "Batch 5 loss: 0.8142526149749756, accuracy: 0.7096354365348816\n",
      "Batch 6 loss: 0.6274874806404114, accuracy: 0.71875\n",
      "Batch 7 loss: 0.6841872930526733, accuracy: 0.7236328125\n",
      "Batch 8 loss: 0.6624144911766052, accuracy: 0.7282986044883728\n",
      "Batch 9 loss: 0.684638500213623, accuracy: 0.7289062738418579\n",
      "Batch 10 loss: 0.586556077003479, accuracy: 0.7357954382896423\n",
      "Batch 11 loss: 0.7972673177719116, accuracy: 0.7311198115348816\n",
      "Batch 12 loss: 0.9014748930931091, accuracy: 0.7271634340286255\n",
      "Batch 13 loss: 0.5821075439453125, accuracy: 0.7315848469734192\n",
      "Batch 14 loss: 0.7232277989387512, accuracy: 0.729687511920929\n",
      "Batch 15 loss: 0.7429970502853394, accuracy: 0.72998046875\n",
      "Batch 16 loss: 0.6536645889282227, accuracy: 0.732536792755127\n",
      "Batch 17 loss: 0.6236698627471924, accuracy: 0.7335069179534912\n",
      "Batch 18 loss: 0.7014793753623962, accuracy: 0.7306743264198303\n",
      "Batch 19 loss: 0.760265588760376, accuracy: 0.731249988079071\n",
      "Batch 20 loss: 0.860605776309967, accuracy: 0.7299107313156128\n",
      "Batch 21 loss: 0.8617978692054749, accuracy: 0.7283380627632141\n",
      "Batch 22 loss: 0.6695100665092468, accuracy: 0.73097825050354\n",
      "Batch 23 loss: 0.7263209819793701, accuracy: 0.7301432490348816\n",
      "Batch 24 loss: 0.804913341999054, accuracy: 0.7300000190734863\n",
      "Batch 25 loss: 0.5879231095314026, accuracy: 0.7310696840286255\n",
      "Batch 26 loss: 0.7744669318199158, accuracy: 0.7320601940155029\n",
      "Batch 27 loss: 0.880596399307251, accuracy: 0.7307477593421936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28 loss: 0.6625490188598633, accuracy: 0.7316810488700867\n",
      "Batch 29 loss: 0.653930127620697, accuracy: 0.7317708134651184\n",
      "Batch 30 loss: 0.7225560545921326, accuracy: 0.7321068644523621\n",
      "Batch 31 loss: 0.6225762963294983, accuracy: 0.732177734375\n",
      "Batch 32 loss: 0.728141188621521, accuracy: 0.7322443127632141\n",
      "Batch 33 loss: 0.6709624528884888, accuracy: 0.732306957244873\n",
      "Batch 34 loss: 0.7846066951751709, accuracy: 0.7330357432365417\n",
      "Batch 35 loss: 0.7882657647132874, accuracy: 0.732421875\n",
      "Batch 36 loss: 0.6768587231636047, accuracy: 0.7326858043670654\n",
      "Batch 37 loss: 0.8669730424880981, accuracy: 0.7325246930122375\n",
      "Batch 38 loss: 0.5654897689819336, accuracy: 0.7339743375778198\n",
      "Batch 39 loss: 0.7113118767738342, accuracy: 0.7333984375\n",
      "Batch 40 loss: 0.8186770677566528, accuracy: 0.7330411672592163\n",
      "Batch 41 loss: 0.6830282211303711, accuracy: 0.7332589030265808\n",
      "Batch 42 loss: 0.7393136620521545, accuracy: 0.7340116500854492\n",
      "Batch 43 loss: 0.738884449005127, accuracy: 0.7341974377632141\n",
      "Batch 44 loss: 0.776570200920105, accuracy: 0.7340278029441833\n",
      "Batch 45 loss: 0.7895084023475647, accuracy: 0.7328464388847351\n",
      "Batch 46 loss: 0.6409085392951965, accuracy: 0.733045220375061\n",
      "Batch 47 loss: 0.8079311847686768, accuracy: 0.7325846552848816\n",
      "Batch 48 loss: 0.6201388239860535, accuracy: 0.7338966727256775\n",
      "Batch 49 loss: 0.6316857933998108, accuracy: 0.734375\n",
      "Batch 50 loss: 0.7551017999649048, accuracy: 0.7333027124404907\n",
      "Batch 51 loss: 0.795168936252594, accuracy: 0.7330228090286255\n",
      "Batch 52 loss: 0.6518236994743347, accuracy: 0.7327535152435303\n",
      "Batch 53 loss: 0.7582054138183594, accuracy: 0.7323495149612427\n",
      "Batch 54 loss: 0.6505720019340515, accuracy: 0.7340909242630005\n",
      "Batch 55 loss: 0.6551215648651123, accuracy: 0.7350725531578064\n",
      "Batch 56 loss: 0.7271186709403992, accuracy: 0.7353344559669495\n",
      "Batch 57 loss: 0.5945050120353699, accuracy: 0.736664891242981\n",
      "Batch 58 loss: 0.8244109153747559, accuracy: 0.7353019118309021\n",
      "Batch 59 loss: 0.7655522227287292, accuracy: 0.7339843511581421\n",
      "Batch 60 loss: 0.6669890880584717, accuracy: 0.734375\n",
      "Batch 61 loss: 0.6542395949363708, accuracy: 0.7346270084381104\n",
      "Batch 62 loss: 0.8342025876045227, accuracy: 0.734002947807312\n",
      "Batch 63 loss: 0.7387610673904419, accuracy: 0.733642578125\n",
      "Batch 64 loss: 0.7199115753173828, accuracy: 0.7347355484962463\n",
      "Batch 65 loss: 0.8263717293739319, accuracy: 0.7347301244735718\n",
      "Batch 66 loss: 0.6839409470558167, accuracy: 0.7356576323509216\n",
      "Batch 67 loss: 0.7621284127235413, accuracy: 0.7357536554336548\n",
      "Batch 68 loss: 0.6989148855209351, accuracy: 0.73607337474823\n",
      "Batch 69 loss: 0.5900630950927734, accuracy: 0.7368303537368774\n",
      "Batch 70 loss: 0.8615974187850952, accuracy: 0.7356954216957092\n",
      "Batch 71 loss: 0.5912241339683533, accuracy: 0.7365451455116272\n",
      "Batch 72 loss: 0.7674740552902222, accuracy: 0.735873281955719\n",
      "Batch 73 loss: 0.7108520865440369, accuracy: 0.7359586358070374\n",
      "Batch 74 loss: 0.6466536521911621, accuracy: 0.7359374761581421\n",
      "Batch 75 loss: 0.8232889175415039, accuracy: 0.7353001832962036\n",
      "Batch 76 loss: 0.7673158049583435, accuracy: 0.7346794009208679\n",
      "Batch 77 loss: 0.8959643840789795, accuracy: 0.7346754670143127\n",
      "Batch 78 loss: 0.6655048131942749, accuracy: 0.7347705960273743\n",
      "Batch 79 loss: 0.7936683297157288, accuracy: 0.734667956829071\n",
      "Batch 80 loss: 0.7568857669830322, accuracy: 0.7347608208656311\n",
      "Batch 81 loss: 0.7281182408332825, accuracy: 0.734660804271698\n",
      "Batch 82 loss: 0.6211360096931458, accuracy: 0.7350338697433472\n",
      "Batch 83 loss: 0.809648871421814, accuracy: 0.7348400354385376\n",
      "Batch 84 loss: 0.6884194612503052, accuracy: 0.7351102828979492\n",
      "Batch 85 loss: 0.7284650206565857, accuracy: 0.7352834343910217\n",
      "Batch 86 loss: 0.7429976463317871, accuracy: 0.735542356967926\n",
      "Batch 87 loss: 0.8330210447311401, accuracy: 0.7346413135528564\n",
      "Batch 88 loss: 0.7187005877494812, accuracy: 0.7342872023582458\n",
      "Batch 89 loss: 0.7357327342033386, accuracy: 0.734461784362793\n",
      "Batch 90 loss: 0.7717798948287964, accuracy: 0.7339457273483276\n",
      "Batch 91 loss: 0.767611026763916, accuracy: 0.733525812625885\n",
      "Batch 92 loss: 0.8096343874931335, accuracy: 0.7333669066429138\n",
      "Batch 93 loss: 0.7074955701828003, accuracy: 0.7332114577293396\n",
      "Batch 94 loss: 0.7174034714698792, accuracy: 0.7328947186470032\n",
      "Batch 95 loss: 0.7459853887557983, accuracy: 0.733154296875\n",
      "Batch 96 loss: 0.7839900851249695, accuracy: 0.7331668734550476\n",
      "Batch 97 loss: 0.7563956379890442, accuracy: 0.7330994606018066\n",
      "Batch 98 loss: 0.6757763028144836, accuracy: 0.7333491444587708\n",
      "Batch 99 loss: 0.7547647953033447, accuracy: 0.7331249713897705\n",
      "Batch 100 loss: 0.6423273086547852, accuracy: 0.7333694100379944\n",
      "Batch 101 loss: 0.734115481376648, accuracy: 0.7333793044090271\n",
      "Batch 102 loss: 0.7716796398162842, accuracy: 0.7333130836486816\n",
      "Batch 103 loss: 0.7260869145393372, accuracy: 0.7331730723381042\n",
      "Batch 104 loss: 0.888850748538971, accuracy: 0.7326636910438538\n",
      "Batch 105 loss: 0.74839848279953, accuracy: 0.7323850393295288\n",
      "Batch 106 loss: 0.7086173295974731, accuracy: 0.7323306202888489\n",
      "Batch 107 loss: 0.6191954612731934, accuracy: 0.7322772145271301\n",
      "Batch 108 loss: 0.7276721596717834, accuracy: 0.7323681116104126\n",
      "Batch 109 loss: 0.6745972633361816, accuracy: 0.7326704263687134\n",
      "Batch 110 loss: 0.7152138948440552, accuracy: 0.7327561974525452\n",
      "Batch 111 loss: 0.5544303059577942, accuracy: 0.7332589030265808\n",
      "Batch 112 loss: 0.7821071147918701, accuracy: 0.7329922318458557\n",
      "Batch 113 loss: 0.6779785752296448, accuracy: 0.7334840893745422\n",
      "Batch 114 loss: 0.8188950419425964, accuracy: 0.7330842614173889\n",
      "Batch 115 loss: 0.57358717918396, accuracy: 0.7332300543785095\n",
      "Batch 116 loss: 0.6991766095161438, accuracy: 0.7329059839248657\n",
      "Batch 117 loss: 0.790421187877655, accuracy: 0.7327198386192322\n",
      "Batch 118 loss: 0.8799543380737305, accuracy: 0.7324054837226868\n",
      "Batch 119 loss: 0.660889744758606, accuracy: 0.7321614623069763\n",
      "Batch 120 loss: 0.6894658207893372, accuracy: 0.7317923307418823\n",
      "Batch 121 loss: 0.7307249307632446, accuracy: 0.7318135499954224\n",
      "Batch 122 loss: 0.6202583909034729, accuracy: 0.7319613695144653\n",
      "Batch 123 loss: 0.7558043003082275, accuracy: 0.7317918539047241\n",
      "Batch 124 loss: 0.7994116544723511, accuracy: 0.7315000295639038\n",
      "Batch 125 loss: 0.7714594602584839, accuracy: 0.731708824634552\n",
      "Batch 126 loss: 0.5693346261978149, accuracy: 0.7320374250411987\n",
      "Batch 127 loss: 0.6836708784103394, accuracy: 0.73223876953125\n",
      "Batch 128 loss: 0.6468458771705627, accuracy: 0.7321947813034058\n",
      "Batch 129 loss: 0.7094646096229553, accuracy: 0.7323317527770996\n",
      "Batch 130 loss: 0.8201615810394287, accuracy: 0.7322877049446106\n",
      "Batch 131 loss: 0.6986588835716248, accuracy: 0.7325402498245239\n",
      "Batch 132 loss: 0.696709156036377, accuracy: 0.732554018497467\n",
      "Batch 133 loss: 0.745181143283844, accuracy: 0.7322177886962891\n",
      "Batch 134 loss: 0.7916056513786316, accuracy: 0.7315972447395325\n",
      "Batch 135 loss: 0.7841687798500061, accuracy: 0.7313878536224365\n",
      "Batch 136 loss: 0.7517572045326233, accuracy: 0.7316948175430298\n",
      "Batch 137 loss: 0.7308499813079834, accuracy: 0.731827437877655\n",
      "Batch 138 loss: 0.849988579750061, accuracy: 0.7313961386680603\n",
      "Batch 139 loss: 0.6914649605751038, accuracy: 0.7315848469734192\n",
      "Batch 140 loss: 0.77666836977005, accuracy: 0.7314937710762024\n",
      "Batch 141 loss: 0.6907356381416321, accuracy: 0.7316241264343262\n",
      "Batch 142 loss: 0.9549078345298767, accuracy: 0.7309331297874451\n",
      "Batch 143 loss: 0.8168162107467651, accuracy: 0.7306857705116272\n",
      "Batch 144 loss: 0.6751290559768677, accuracy: 0.7307112216949463\n",
      "Batch 145 loss: 0.6945360898971558, accuracy: 0.7308968305587769\n",
      "Batch 146 loss: 0.7926434278488159, accuracy: 0.7307078838348389\n",
      "Batch 147 loss: 0.7287241220474243, accuracy: 0.7303103804588318\n",
      "Batch 148 loss: 0.6385159492492676, accuracy: 0.7304425239562988\n",
      "Batch 149 loss: 0.7162125110626221, accuracy: 0.7304166555404663\n",
      "Batch 150 loss: 0.7859092950820923, accuracy: 0.7303394079208374\n",
      "Batch 151 loss: 0.7512124180793762, accuracy: 0.7304173707962036\n",
      "Batch 152 loss: 0.7698550224304199, accuracy: 0.7303410768508911\n",
      "Batch 153 loss: 0.8420106172561646, accuracy: 0.7301136255264282\n",
      "Batch 154 loss: 0.7591283321380615, accuracy: 0.7302419543266296\n",
      "Batch 155 loss: 0.7390567064285278, accuracy: 0.7301682829856873\n",
      "Batch 156 loss: 0.6546368598937988, accuracy: 0.7305433750152588\n",
      "Batch 157 loss: 0.7212340831756592, accuracy: 0.7302709817886353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 158 loss: 0.5857589840888977, accuracy: 0.7306898832321167\n",
      "Batch 159 loss: 0.8040726780891418, accuracy: 0.730419933795929\n",
      "Batch 160 loss: 0.8029435276985168, accuracy: 0.730250358581543\n",
      "Batch 161 loss: 0.6814186573028564, accuracy: 0.7302276492118835\n",
      "Batch 162 loss: 0.8599531650543213, accuracy: 0.7299655079841614\n",
      "Batch 163 loss: 0.6612022519111633, accuracy: 0.7300400137901306\n",
      "Batch 164 loss: 0.5996175408363342, accuracy: 0.7303503751754761\n",
      "Batch 165 loss: 0.6523252129554749, accuracy: 0.7306099534034729\n",
      "Batch 166 loss: 0.803731381893158, accuracy: 0.7303985953330994\n",
      "Batch 167 loss: 0.6373153328895569, accuracy: 0.7306082844734192\n",
      "Batch 168 loss: 0.7173835635185242, accuracy: 0.7305843234062195\n",
      "Batch 169 loss: 0.6411548256874084, accuracy: 0.7306525707244873\n",
      "Batch 170 loss: 0.6581001877784729, accuracy: 0.7306286692619324\n",
      "Batch 171 loss: 0.7824839353561401, accuracy: 0.7306958436965942\n",
      "Batch 172 loss: 0.704408586025238, accuracy: 0.7310783863067627\n",
      "Batch 173 loss: 0.7307623028755188, accuracy: 0.7313218116760254\n",
      "Batch 174 loss: 0.7231240272521973, accuracy: 0.7314285635948181\n",
      "Batch 175 loss: 0.7654039263725281, accuracy: 0.7313121557235718\n",
      "Batch 176 loss: 0.6098693013191223, accuracy: 0.7315942645072937\n",
      "Batch 177 loss: 0.6857689023017883, accuracy: 0.7316538095474243\n",
      "Batch 178 loss: 0.7248411178588867, accuracy: 0.7314507961273193\n",
      "Batch 179 loss: 0.7114477753639221, accuracy: 0.7315972447395325\n",
      "Batch 180 loss: 0.7213071584701538, accuracy: 0.731526255607605\n",
      "Batch 181 loss: 0.7613150477409363, accuracy: 0.731370210647583\n",
      "Batch 182 loss: 0.5749210119247437, accuracy: 0.7316427826881409\n",
      "Batch 183 loss: 0.4950120747089386, accuracy: 0.73216712474823\n",
      "Batch 184 loss: 0.7379969358444214, accuracy: 0.7320523858070374\n",
      "Batch 185 loss: 0.7460506558418274, accuracy: 0.7322748899459839\n",
      "Batch 186 loss: 0.8413560390472412, accuracy: 0.732160747051239\n",
      "Batch 187 loss: 0.9012669324874878, accuracy: 0.731798529624939\n",
      "Batch 188 loss: 0.662228524684906, accuracy: 0.7321015000343323\n",
      "Batch 189 loss: 0.6968713402748108, accuracy: 0.7321957349777222\n",
      "Batch 190 loss: 0.6135038733482361, accuracy: 0.7324525713920593\n",
      "Batch 191 loss: 0.6648816466331482, accuracy: 0.7327880859375\n",
      "Batch 192 loss: 0.8338627219200134, accuracy: 0.7328367829322815\n",
      "Batch 193 loss: 0.5867549777030945, accuracy: 0.7331668734550476\n",
      "Batch 194 loss: 0.6887162923812866, accuracy: 0.7331330180168152\n",
      "Batch 195 loss: 0.8505408763885498, accuracy: 0.7328603267669678\n",
      "Batch 196 loss: 0.755810022354126, accuracy: 0.7325507402420044\n",
      "Batch 197 loss: 0.6998645663261414, accuracy: 0.7324021458625793\n",
      "Batch 198 loss: 0.5978529453277588, accuracy: 0.7325298190116882\n",
      "Batch 199 loss: 0.7025536298751831, accuracy: 0.7326562404632568\n",
      "Batch 200 loss: 0.7559798955917358, accuracy: 0.732548177242279\n",
      "Batch 201 loss: 0.7011218070983887, accuracy: 0.7326732873916626\n",
      "Batch 202 loss: 0.6582745909690857, accuracy: 0.732912540435791\n",
      "Batch 203 loss: 0.7957602739334106, accuracy: 0.7328814268112183\n",
      "Batch 204 loss: 0.7636942267417908, accuracy: 0.7326981425285339\n",
      "Batch 205 loss: 0.7166372537612915, accuracy: 0.7328580021858215\n",
      "Batch 206 loss: 0.8592830300331116, accuracy: 0.7329030632972717\n",
      "Batch 207 loss: 0.763819694519043, accuracy: 0.73291015625\n",
      "Batch 208 loss: 0.7064597606658936, accuracy: 0.7329171895980835\n",
      "Batch 209 loss: 0.7219218015670776, accuracy: 0.7328869104385376\n",
      "Batch 210 loss: 0.7913802266120911, accuracy: 0.7328569293022156\n",
      "Batch 211 loss: 0.7942531704902649, accuracy: 0.7324587106704712\n",
      "Batch 212 loss: 0.7554618716239929, accuracy: 0.7324310541152954\n",
      "Batch 213 loss: 0.7639310956001282, accuracy: 0.7322940826416016\n",
      "Batch 214 loss: 0.6896085143089294, accuracy: 0.7321584224700928\n",
      "Batch 215 loss: 0.6421847939491272, accuracy: 0.732421875\n",
      "Batch 216 loss: 0.7371382713317871, accuracy: 0.7324668765068054\n",
      "Batch 217 loss: 0.6510152816772461, accuracy: 0.7325114607810974\n",
      "Batch 218 loss: 0.7811697721481323, accuracy: 0.7324486374855042\n",
      "Batch 219 loss: 0.5326545834541321, accuracy: 0.7327769994735718\n",
      "Batch 220 loss: 0.6709273457527161, accuracy: 0.7329963445663452\n",
      "Batch 221 loss: 0.696304202079773, accuracy: 0.7326858043670654\n",
      "Batch 222 loss: 0.8173822164535522, accuracy: 0.7325882911682129\n",
      "Batch 223 loss: 0.7507160902023315, accuracy: 0.7324916124343872\n",
      "Batch 224 loss: 0.6433334946632385, accuracy: 0.7325229048728943\n",
      "Training epoch: 36, train accuracy: 73.25228881835938, train loss: 0.7251008066866133, valid accuracy: 64.89273071289062, valid loss: 1.0576463526692883 \n",
      "Batch 0 loss: 0.6409191489219666, accuracy: 0.75\n",
      "Batch 1 loss: 0.6194698810577393, accuracy: 0.765625\n",
      "Batch 2 loss: 0.584220290184021, accuracy: 0.7786458134651184\n",
      "Batch 3 loss: 0.8517955541610718, accuracy: 0.751953125\n",
      "Batch 4 loss: 0.8570080399513245, accuracy: 0.7406250238418579\n",
      "Batch 5 loss: 0.8075607419013977, accuracy: 0.7317708134651184\n",
      "Batch 6 loss: 0.5237823724746704, accuracy: 0.7377232313156128\n",
      "Batch 7 loss: 0.6527875661849976, accuracy: 0.744140625\n",
      "Batch 8 loss: 0.6374491453170776, accuracy: 0.7395833134651184\n",
      "Batch 9 loss: 0.7014052271842957, accuracy: 0.739062488079071\n",
      "Batch 10 loss: 0.7541871070861816, accuracy: 0.7336647510528564\n",
      "Batch 11 loss: 0.6746488809585571, accuracy: 0.7330729365348816\n",
      "Batch 12 loss: 0.5958412289619446, accuracy: 0.737379789352417\n",
      "Batch 13 loss: 0.6335267424583435, accuracy: 0.7410714030265808\n",
      "Batch 14 loss: 0.7411850690841675, accuracy: 0.7395833134651184\n",
      "Batch 15 loss: 0.772354781627655, accuracy: 0.73974609375\n",
      "Batch 16 loss: 0.7769678235054016, accuracy: 0.7375919222831726\n",
      "Batch 17 loss: 0.6058460474014282, accuracy: 0.7387152910232544\n",
      "Batch 18 loss: 0.6677861213684082, accuracy: 0.7380756735801697\n",
      "Batch 19 loss: 0.7816012501716614, accuracy: 0.7359374761581421\n",
      "Batch 20 loss: 0.7506089806556702, accuracy: 0.7354910969734192\n",
      "Batch 21 loss: 0.8801481127738953, accuracy: 0.7340198755264282\n",
      "Batch 22 loss: 0.8333167433738708, accuracy: 0.73097825050354\n",
      "Batch 23 loss: 0.7014104127883911, accuracy: 0.7314453125\n",
      "Batch 24 loss: 0.5962681174278259, accuracy: 0.7318750023841858\n",
      "Batch 25 loss: 0.7595106363296509, accuracy: 0.7310696840286255\n",
      "Batch 26 loss: 0.7471372485160828, accuracy: 0.7300347089767456\n",
      "Batch 27 loss: 0.7278357148170471, accuracy: 0.73046875\n",
      "Batch 28 loss: 0.6083618998527527, accuracy: 0.732758641242981\n",
      "Batch 29 loss: 0.6339600086212158, accuracy: 0.7348958253860474\n",
      "Batch 30 loss: 0.8156962394714355, accuracy: 0.7338709831237793\n",
      "Batch 31 loss: 0.5556743741035461, accuracy: 0.736083984375\n",
      "Batch 32 loss: 0.7972065806388855, accuracy: 0.7336647510528564\n",
      "Batch 33 loss: 0.7399312257766724, accuracy: 0.7334558963775635\n",
      "Batch 34 loss: 0.6982187032699585, accuracy: 0.7325893044471741\n",
      "Batch 35 loss: 0.6254797577857971, accuracy: 0.7335069179534912\n",
      "Batch 36 loss: 0.5910341143608093, accuracy: 0.7350084185600281\n",
      "Batch 37 loss: 0.6299397945404053, accuracy: 0.7349917888641357\n",
      "Batch 38 loss: 0.847930908203125, accuracy: 0.7327724099159241\n",
      "Batch 39 loss: 0.7415558695793152, accuracy: 0.732226550579071\n",
      "Batch 40 loss: 0.8132641315460205, accuracy: 0.7317073345184326\n",
      "Batch 41 loss: 0.8723931908607483, accuracy: 0.7299107313156128\n",
      "Batch 42 loss: 0.6835312247276306, accuracy: 0.7300145626068115\n",
      "Batch 43 loss: 0.7146076560020447, accuracy: 0.7317116260528564\n",
      "Batch 44 loss: 0.604853630065918, accuracy: 0.7335069179534912\n",
      "Batch 45 loss: 0.6327069997787476, accuracy: 0.734375\n",
      "Batch 46 loss: 0.6156697869300842, accuracy: 0.7345412373542786\n",
      "Batch 47 loss: 0.7490975856781006, accuracy: 0.7347005009651184\n",
      "Batch 48 loss: 0.9722166061401367, accuracy: 0.733418345451355\n",
      "Batch 49 loss: 0.6777873039245605, accuracy: 0.7337499856948853\n",
      "Batch 50 loss: 0.7915682196617126, accuracy: 0.732536792755127\n",
      "Batch 51 loss: 0.6093068718910217, accuracy: 0.733473539352417\n",
      "Batch 52 loss: 0.7664636969566345, accuracy: 0.7327535152435303\n",
      "Batch 53 loss: 0.8432947397232056, accuracy: 0.7322048544883728\n",
      "Batch 54 loss: 0.8082257509231567, accuracy: 0.731249988079071\n",
      "Batch 55 loss: 0.8567107319831848, accuracy: 0.7301897406578064\n",
      "Batch 56 loss: 0.6458104848861694, accuracy: 0.7304002046585083\n",
      "Batch 57 loss: 0.6738260388374329, accuracy: 0.7312769293785095\n",
      "Batch 58 loss: 0.7566875219345093, accuracy: 0.7314618825912476\n",
      "Batch 59 loss: 0.6641266345977783, accuracy: 0.732421875\n",
      "Batch 60 loss: 0.7402222752571106, accuracy: 0.7328380942344666\n",
      "Batch 61 loss: 0.6640076041221619, accuracy: 0.7332409024238586\n",
      "Batch 62 loss: 0.6670616865158081, accuracy: 0.7328869104385376\n",
      "Batch 63 loss: 0.7138537168502808, accuracy: 0.73291015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 64 loss: 0.7307267189025879, accuracy: 0.7324519157409668\n",
      "Batch 65 loss: 0.5777199864387512, accuracy: 0.7337831258773804\n",
      "Batch 66 loss: 0.709237277507782, accuracy: 0.7334421873092651\n",
      "Batch 67 loss: 0.721218466758728, accuracy: 0.7340303063392639\n",
      "Batch 68 loss: 0.5804382562637329, accuracy: 0.734375\n",
      "Batch 69 loss: 0.7693955898284912, accuracy: 0.7339285612106323\n",
      "Batch 70 loss: 0.6153040528297424, accuracy: 0.734375\n",
      "Batch 71 loss: 0.7349093556404114, accuracy: 0.7344834804534912\n",
      "Batch 72 loss: 0.7495458126068115, accuracy: 0.7341609597206116\n",
      "Batch 73 loss: 0.7315142154693604, accuracy: 0.7341638803482056\n",
      "Batch 74 loss: 0.6184946894645691, accuracy: 0.7346875071525574\n",
      "Batch 75 loss: 0.7812027335166931, accuracy: 0.734375\n",
      "Batch 76 loss: 0.7716007232666016, accuracy: 0.7336647510528564\n",
      "Batch 77 loss: 0.6243656873703003, accuracy: 0.7344751358032227\n",
      "Batch 78 loss: 0.8466092348098755, accuracy: 0.7338805198669434\n",
      "Batch 79 loss: 0.6193444132804871, accuracy: 0.734179675579071\n",
      "Batch 80 loss: 0.8278869390487671, accuracy: 0.7335069179534912\n",
      "Batch 81 loss: 0.7523484230041504, accuracy: 0.7332317233085632\n",
      "Batch 82 loss: 0.7592911124229431, accuracy: 0.7328689694404602\n",
      "Batch 83 loss: 0.6642820239067078, accuracy: 0.7328869104385376\n",
      "Batch 84 loss: 0.5729882717132568, accuracy: 0.7335478067398071\n",
      "Batch 85 loss: 0.8527140617370605, accuracy: 0.7328306436538696\n",
      "Batch 86 loss: 0.5161440968513489, accuracy: 0.7337464094161987\n",
      "Batch 87 loss: 0.6352286338806152, accuracy: 0.7344637513160706\n",
      "Batch 88 loss: 0.6447759866714478, accuracy: 0.7342872023582458\n",
      "Batch 89 loss: 0.7269275784492493, accuracy: 0.734375\n",
      "Batch 90 loss: 0.8589940667152405, accuracy: 0.7341174483299255\n",
      "Batch 91 loss: 0.6892926096916199, accuracy: 0.73386549949646\n",
      "Batch 92 loss: 0.6320453882217407, accuracy: 0.7344589829444885\n",
      "Batch 93 loss: 0.640326738357544, accuracy: 0.734790563583374\n",
      "Batch 94 loss: 0.6277441382408142, accuracy: 0.7352796196937561\n",
      "Batch 95 loss: 0.9069387912750244, accuracy: 0.7340494990348816\n",
      "Batch 96 loss: 0.7578908801078796, accuracy: 0.733408510684967\n",
      "Batch 97 loss: 0.7328534126281738, accuracy: 0.7329400777816772\n",
      "Batch 98 loss: 0.7566407322883606, accuracy: 0.7331913113594055\n",
      "Batch 99 loss: 0.8395296335220337, accuracy: 0.7328906059265137\n",
      "Batch 100 loss: 0.8543494343757629, accuracy: 0.7324411869049072\n",
      "Batch 101 loss: 0.7193378806114197, accuracy: 0.7326899766921997\n",
      "Batch 102 loss: 0.7348764538764954, accuracy: 0.7324029207229614\n",
      "Batch 103 loss: 0.7764460444450378, accuracy: 0.732346773147583\n",
      "Batch 104 loss: 0.6359766125679016, accuracy: 0.7327380776405334\n",
      "Batch 105 loss: 0.7765599489212036, accuracy: 0.7325324416160583\n",
      "Batch 106 loss: 0.6402701735496521, accuracy: 0.7331337332725525\n",
      "Batch 107 loss: 0.9770933389663696, accuracy: 0.7325665354728699\n",
      "Batch 108 loss: 0.7197853922843933, accuracy: 0.7322964668273926\n",
      "Batch 109 loss: 0.6869433522224426, accuracy: 0.7326704263687134\n",
      "Batch 110 loss: 0.8769693374633789, accuracy: 0.7319115996360779\n",
      "Batch 111 loss: 0.7534338235855103, accuracy: 0.7317940592765808\n",
      "Batch 112 loss: 0.7911896705627441, accuracy: 0.7317478060722351\n",
      "Batch 113 loss: 0.6650044918060303, accuracy: 0.7319079041481018\n",
      "Batch 114 loss: 0.6801751852035522, accuracy: 0.732472836971283\n",
      "Batch 115 loss: 0.7588303089141846, accuracy: 0.7324892282485962\n",
      "Batch 116 loss: 0.879269003868103, accuracy: 0.7320379018783569\n",
      "Batch 117 loss: 0.6739124655723572, accuracy: 0.7320577502250671\n",
      "Batch 118 loss: 0.8377931714057922, accuracy: 0.7321428656578064\n",
      "Batch 119 loss: 0.736503541469574, accuracy: 0.7321614623069763\n",
      "Batch 120 loss: 0.6318102478981018, accuracy: 0.7325671315193176\n",
      "Batch 121 loss: 0.6669316291809082, accuracy: 0.7325819730758667\n",
      "Batch 122 loss: 0.7022153735160828, accuracy: 0.7328506112098694\n",
      "Batch 123 loss: 0.7327127456665039, accuracy: 0.7326738834381104\n",
      "Batch 124 loss: 0.6797707676887512, accuracy: 0.7324374914169312\n",
      "Batch 125 loss: 0.679746687412262, accuracy: 0.7324528694152832\n",
      "Batch 126 loss: 0.5762834548950195, accuracy: 0.7328370809555054\n",
      "Batch 127 loss: 0.5721186399459839, accuracy: 0.73333740234375\n",
      "Batch 128 loss: 0.7002239227294922, accuracy: 0.7334665656089783\n",
      "Batch 129 loss: 0.7998698353767395, accuracy: 0.7331129908561707\n",
      "Batch 130 loss: 0.6619457006454468, accuracy: 0.7331822514533997\n",
      "Batch 131 loss: 0.7480365037918091, accuracy: 0.7331913113594055\n",
      "Batch 132 loss: 0.6690948009490967, accuracy: 0.7334351539611816\n",
      "Batch 133 loss: 0.8265592455863953, accuracy: 0.7330923676490784\n",
      "Batch 134 loss: 0.7784079909324646, accuracy: 0.7331018447875977\n",
      "Batch 135 loss: 0.7168322801589966, accuracy: 0.7329963445663452\n",
      "Batch 136 loss: 0.6156138181686401, accuracy: 0.733405590057373\n",
      "Batch 137 loss: 0.647490382194519, accuracy: 0.7336390614509583\n",
      "Batch 138 loss: 0.7722956538200378, accuracy: 0.7332509160041809\n",
      "Batch 139 loss: 0.6690037846565247, accuracy: 0.7333705425262451\n",
      "Batch 140 loss: 0.8497059345245361, accuracy: 0.7329344153404236\n",
      "Batch 141 loss: 0.6595239639282227, accuracy: 0.7331646084785461\n",
      "Batch 142 loss: 0.6596972942352295, accuracy: 0.7332277297973633\n",
      "Batch 143 loss: 0.705490231513977, accuracy: 0.7333441972732544\n",
      "Batch 144 loss: 0.7089272737503052, accuracy: 0.7332974076271057\n",
      "Batch 145 loss: 0.6532700061798096, accuracy: 0.733465313911438\n",
      "Batch 146 loss: 0.8030524849891663, accuracy: 0.7330994606018066\n",
      "Batch 147 loss: 0.6221856474876404, accuracy: 0.7334248423576355\n",
      "Batch 148 loss: 0.7182601094245911, accuracy: 0.7335885167121887\n",
      "Batch 149 loss: 0.6653499603271484, accuracy: 0.7336978912353516\n",
      "Batch 150 loss: 0.72808438539505, accuracy: 0.7337541580200195\n",
      "Batch 151 loss: 0.79408860206604, accuracy: 0.7335526347160339\n",
      "Batch 152 loss: 0.6885016560554504, accuracy: 0.7335579991340637\n",
      "Batch 153 loss: 0.7493317127227783, accuracy: 0.7335633039474487\n",
      "Batch 154 loss: 0.68104487657547, accuracy: 0.7337197661399841\n",
      "Batch 155 loss: 0.650072455406189, accuracy: 0.7337239384651184\n",
      "Batch 156 loss: 0.6159985065460205, accuracy: 0.7337778806686401\n",
      "Batch 157 loss: 0.5865403413772583, accuracy: 0.7343255281448364\n",
      "Batch 158 loss: 0.8232033252716064, accuracy: 0.7340801954269409\n",
      "Batch 159 loss: 0.6237245798110962, accuracy: 0.7342284917831421\n",
      "Batch 160 loss: 0.7272101044654846, accuracy: 0.7339867949485779\n",
      "Batch 161 loss: 0.6463063359260559, accuracy: 0.7340373992919922\n",
      "Batch 162 loss: 0.7317492365837097, accuracy: 0.7339436411857605\n",
      "Batch 163 loss: 0.7430972456932068, accuracy: 0.734089195728302\n",
      "Batch 164 loss: 0.6974754929542542, accuracy: 0.7341382503509521\n",
      "Batch 165 loss: 0.7881860136985779, accuracy: 0.7341867685317993\n",
      "Batch 166 loss: 0.6827763915061951, accuracy: 0.734375\n",
      "Batch 167 loss: 0.611056923866272, accuracy: 0.734747052192688\n",
      "Batch 168 loss: 0.76643967628479, accuracy: 0.7344674468040466\n",
      "Batch 169 loss: 0.7926286458969116, accuracy: 0.734375\n",
      "Batch 170 loss: 0.7362719178199768, accuracy: 0.7343292832374573\n",
      "Batch 171 loss: 0.6852094531059265, accuracy: 0.7345112562179565\n",
      "Batch 172 loss: 0.8165794014930725, accuracy: 0.7342846989631653\n",
      "Batch 173 loss: 0.655910313129425, accuracy: 0.7343301177024841\n",
      "Batch 174 loss: 0.5996208786964417, accuracy: 0.7346875071525574\n",
      "Batch 175 loss: 0.7048330307006836, accuracy: 0.7346857190132141\n",
      "Batch 176 loss: 0.6064220070838928, accuracy: 0.735037088394165\n",
      "Batch 177 loss: 0.7475025653839111, accuracy: 0.734813928604126\n",
      "Batch 178 loss: 0.7112277746200562, accuracy: 0.7348987460136414\n",
      "Batch 179 loss: 0.7436747550964355, accuracy: 0.7349392175674438\n",
      "Batch 180 loss: 0.8098771572113037, accuracy: 0.7348929643630981\n",
      "Batch 181 loss: 0.7425771951675415, accuracy: 0.7349330186843872\n",
      "Batch 182 loss: 0.6601755023002625, accuracy: 0.7351434230804443\n",
      "Batch 183 loss: 0.7540405988693237, accuracy: 0.7350543737411499\n",
      "Batch 184 loss: 0.7023043632507324, accuracy: 0.7351773381233215\n",
      "Batch 185 loss: 0.7689372301101685, accuracy: 0.7350050210952759\n",
      "Batch 186 loss: 0.7397351861000061, accuracy: 0.7350016832351685\n",
      "Batch 187 loss: 0.555272102355957, accuracy: 0.7353307604789734\n",
      "Batch 188 loss: 0.5716935992240906, accuracy: 0.7356977462768555\n",
      "Batch 189 loss: 0.7078254818916321, accuracy: 0.7354851961135864\n",
      "Batch 190 loss: 0.7133262753486633, accuracy: 0.7354794144630432\n",
      "Batch 191 loss: 0.6304999589920044, accuracy: 0.7357177734375\n",
      "Batch 192 loss: 0.5797934532165527, accuracy: 0.735872745513916\n",
      "Batch 193 loss: 0.5260560512542725, accuracy: 0.7361871600151062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 194 loss: 0.825870931148529, accuracy: 0.735857367515564\n",
      "Batch 195 loss: 0.7725869417190552, accuracy: 0.7358497977256775\n",
      "Batch 196 loss: 0.8260090351104736, accuracy: 0.7358026504516602\n",
      "Batch 197 loss: 0.6504051089286804, accuracy: 0.7359927296638489\n",
      "Batch 198 loss: 0.8036483526229858, accuracy: 0.7357097864151001\n",
      "Batch 199 loss: 0.7270427942276001, accuracy: 0.7357812523841858\n",
      "Batch 200 loss: 0.6824706196784973, accuracy: 0.7357353568077087\n",
      "Batch 201 loss: 0.621353805065155, accuracy: 0.735883355140686\n",
      "Batch 202 loss: 0.7541639804840088, accuracy: 0.7357989549636841\n",
      "Batch 203 loss: 0.7256595492362976, accuracy: 0.7358302474021912\n",
      "Batch 204 loss: 0.6916921138763428, accuracy: 0.7360899448394775\n",
      "Batch 205 loss: 0.6957592368125916, accuracy: 0.7361574769020081\n",
      "Batch 206 loss: 0.7135037779808044, accuracy: 0.7361866235733032\n",
      "Batch 207 loss: 0.6282615065574646, accuracy: 0.7365534901618958\n",
      "Batch 208 loss: 0.7927222847938538, accuracy: 0.7363935112953186\n",
      "Batch 209 loss: 0.6743200421333313, accuracy: 0.7362351417541504\n",
      "Batch 210 loss: 0.6424079537391663, accuracy: 0.736411452293396\n",
      "Batch 211 loss: 0.6205716133117676, accuracy: 0.7366597652435303\n",
      "Batch 212 loss: 0.8243986964225769, accuracy: 0.7362455725669861\n",
      "Batch 213 loss: 0.7892251014709473, accuracy: 0.736163854598999\n",
      "Batch 214 loss: 0.5903116464614868, accuracy: 0.736518919467926\n",
      "Batch 215 loss: 0.7918429970741272, accuracy: 0.7363643050193787\n",
      "Batch 216 loss: 0.6092123985290527, accuracy: 0.7364631295204163\n",
      "Batch 217 loss: 0.7449234127998352, accuracy: 0.7361310124397278\n",
      "Batch 218 loss: 0.6442559957504272, accuracy: 0.7363370656967163\n",
      "Batch 219 loss: 0.7106983661651611, accuracy: 0.7363991737365723\n",
      "Batch 220 loss: 0.6843165755271912, accuracy: 0.7363193035125732\n",
      "Batch 221 loss: 0.6340047121047974, accuracy: 0.736556887626648\n",
      "Batch 222 loss: 0.7128228545188904, accuracy: 0.7365471124649048\n",
      "Batch 223 loss: 0.7427896857261658, accuracy: 0.7363629937171936\n",
      "Batch 224 loss: 0.636934757232666, accuracy: 0.7363892793655396\n",
      "Training epoch: 37, train accuracy: 73.63893127441406, train loss: 0.7106824395391677, valid accuracy: 63.304542541503906, valid loss: 1.1449790309215415 \n",
      "Batch 0 loss: 0.6023140549659729, accuracy: 0.8046875\n",
      "Batch 1 loss: 0.6509677767753601, accuracy: 0.8046875\n",
      "Batch 2 loss: 0.86000657081604, accuracy: 0.7708333134651184\n",
      "Batch 3 loss: 0.676987886428833, accuracy: 0.759765625\n",
      "Batch 4 loss: 0.7337368130683899, accuracy: 0.7578125\n",
      "Batch 5 loss: 0.6653246879577637, accuracy: 0.7604166865348816\n",
      "Batch 6 loss: 0.6631675362586975, accuracy: 0.7611607313156128\n",
      "Batch 7 loss: 0.6862765550613403, accuracy: 0.7587890625\n",
      "Batch 8 loss: 0.7727738618850708, accuracy: 0.7517361044883728\n",
      "Batch 9 loss: 0.7271637916564941, accuracy: 0.746874988079071\n",
      "Batch 10 loss: 0.7802187204360962, accuracy: 0.7450284361839294\n",
      "Batch 11 loss: 0.6461496353149414, accuracy: 0.7454426884651184\n",
      "Batch 12 loss: 0.8674649596214294, accuracy: 0.7379807829856873\n",
      "Batch 13 loss: 0.8719536066055298, accuracy: 0.7349330186843872\n",
      "Batch 14 loss: 0.6661579608917236, accuracy: 0.737500011920929\n",
      "Batch 15 loss: 0.6904438138008118, accuracy: 0.73779296875\n",
      "Batch 16 loss: 0.6061362624168396, accuracy: 0.7398896813392639\n",
      "Batch 17 loss: 0.6588021516799927, accuracy: 0.7417534589767456\n",
      "Batch 18 loss: 0.8204700350761414, accuracy: 0.7397204041481018\n",
      "Batch 19 loss: 0.8015890121459961, accuracy: 0.7406250238418579\n",
      "Batch 20 loss: 0.7494895458221436, accuracy: 0.7392113208770752\n",
      "Batch 21 loss: 0.7421615719795227, accuracy: 0.7375710010528564\n",
      "Batch 22 loss: 0.5990899205207825, accuracy: 0.74048912525177\n",
      "Batch 23 loss: 0.7850355505943298, accuracy: 0.7386067509651184\n",
      "Batch 24 loss: 0.8122595548629761, accuracy: 0.7371875047683716\n",
      "Batch 25 loss: 0.6144527792930603, accuracy: 0.7385817170143127\n",
      "Batch 26 loss: 0.6889039874076843, accuracy: 0.7381365895271301\n",
      "Batch 27 loss: 0.7478404641151428, accuracy: 0.7380022406578064\n",
      "Batch 28 loss: 0.7025890946388245, accuracy: 0.7381465435028076\n",
      "Batch 29 loss: 0.6113485097885132, accuracy: 0.7395833134651184\n",
      "Batch 30 loss: 0.7385953664779663, accuracy: 0.7399193644523621\n",
      "Batch 31 loss: 0.7458081245422363, accuracy: 0.73779296875\n",
      "Batch 32 loss: 0.811008632183075, accuracy: 0.7348484992980957\n",
      "Batch 33 loss: 0.515260636806488, accuracy: 0.7366728186607361\n",
      "Batch 34 loss: 0.6459684371948242, accuracy: 0.7363839149475098\n",
      "Batch 35 loss: 0.8444359302520752, accuracy: 0.7354600429534912\n",
      "Batch 36 loss: 0.7203203439712524, accuracy: 0.7333192825317383\n",
      "Batch 37 loss: 0.6630730032920837, accuracy: 0.7323190569877625\n",
      "Batch 38 loss: 0.7476708292961121, accuracy: 0.7319711446762085\n",
      "Batch 39 loss: 0.6874040961265564, accuracy: 0.732617199420929\n",
      "Batch 40 loss: 0.5806382894515991, accuracy: 0.7339938879013062\n",
      "Batch 41 loss: 0.6981104016304016, accuracy: 0.7349330186843872\n",
      "Batch 42 loss: 0.6547331809997559, accuracy: 0.7351017594337463\n",
      "Batch 43 loss: 0.6555067896842957, accuracy: 0.7350852489471436\n",
      "Batch 44 loss: 0.7447265386581421, accuracy: 0.7347221970558167\n",
      "Batch 45 loss: 0.6355368494987488, accuracy: 0.73488450050354\n",
      "Batch 46 loss: 0.635498046875, accuracy: 0.7358710169792175\n",
      "Batch 47 loss: 0.6776218414306641, accuracy: 0.73681640625\n",
      "Batch 48 loss: 0.6912304162979126, accuracy: 0.7361288070678711\n",
      "Batch 49 loss: 0.7267013788223267, accuracy: 0.7356250286102295\n",
      "Batch 50 loss: 0.8406964540481567, accuracy: 0.734987735748291\n",
      "Batch 51 loss: 0.6325704455375671, accuracy: 0.7358773946762085\n",
      "Batch 52 loss: 0.6519404649734497, accuracy: 0.7365860939025879\n",
      "Batch 53 loss: 0.5972867012023926, accuracy: 0.7368344664573669\n",
      "Batch 54 loss: 0.7097336053848267, accuracy: 0.7359374761581421\n",
      "Batch 55 loss: 0.6798843741416931, accuracy: 0.7364676594734192\n",
      "Batch 56 loss: 0.6722794771194458, accuracy: 0.7367050647735596\n",
      "Batch 57 loss: 0.6662840247154236, accuracy: 0.736664891242981\n",
      "Batch 58 loss: 0.6720792651176453, accuracy: 0.7362288236618042\n",
      "Batch 59 loss: 0.671583354473114, accuracy: 0.7369791865348816\n",
      "Batch 60 loss: 0.8671455383300781, accuracy: 0.7361680269241333\n",
      "Batch 61 loss: 0.6189842820167542, accuracy: 0.7366431355476379\n",
      "Batch 62 loss: 0.6788948774337769, accuracy: 0.736855149269104\n",
      "Batch 63 loss: 0.665802538394928, accuracy: 0.736328125\n",
      "Batch 64 loss: 0.6172145009040833, accuracy: 0.7368990182876587\n",
      "Batch 65 loss: 0.6420353651046753, accuracy: 0.7374526262283325\n",
      "Batch 66 loss: 0.6423654556274414, accuracy: 0.7376399040222168\n",
      "Batch 67 loss: 0.6005429029464722, accuracy: 0.7380514740943909\n",
      "Batch 68 loss: 0.8423810601234436, accuracy: 0.7376585006713867\n",
      "Batch 69 loss: 0.954206109046936, accuracy: 0.7362723350524902\n",
      "Batch 70 loss: 0.6902023553848267, accuracy: 0.7367957830429077\n",
      "Batch 71 loss: 0.7239474654197693, accuracy: 0.7364366054534912\n",
      "Batch 72 loss: 0.7066062688827515, accuracy: 0.736194372177124\n",
      "Batch 73 loss: 0.7178741693496704, accuracy: 0.7365920543670654\n",
      "Batch 74 loss: 0.8696680665016174, accuracy: 0.7358333468437195\n",
      "Batch 75 loss: 0.8062615394592285, accuracy: 0.7349917888641357\n",
      "Batch 76 loss: 0.8391975164413452, accuracy: 0.7341721057891846\n",
      "Batch 77 loss: 0.8392303586006165, accuracy: 0.7336738705635071\n",
      "Batch 78 loss: 0.5579441785812378, accuracy: 0.7344738841056824\n",
      "Batch 79 loss: 0.767143726348877, accuracy: 0.7344726324081421\n",
      "Batch 80 loss: 0.7945236563682556, accuracy: 0.7344714403152466\n",
      "Batch 81 loss: 0.6291273236274719, accuracy: 0.7351372241973877\n",
      "Batch 82 loss: 0.829971194267273, accuracy: 0.7347515225410461\n",
      "Batch 83 loss: 0.6818219423294067, accuracy: 0.734561026096344\n",
      "Batch 84 loss: 0.7799132466316223, accuracy: 0.7348345518112183\n",
      "Batch 85 loss: 0.6905827522277832, accuracy: 0.7351017594337463\n",
      "Batch 86 loss: 0.7814609408378601, accuracy: 0.7349137663841248\n",
      "Batch 87 loss: 0.6773254871368408, accuracy: 0.7349964380264282\n",
      "Batch 88 loss: 0.6235533952713013, accuracy: 0.7354283928871155\n",
      "Batch 89 loss: 0.7472109794616699, accuracy: 0.7352430820465088\n",
      "Batch 90 loss: 0.641768753528595, accuracy: 0.7351476550102234\n",
      "Batch 91 loss: 0.7555195093154907, accuracy: 0.73488450050354\n",
      "Batch 92 loss: 0.6857953667640686, accuracy: 0.7347950339317322\n",
      "Batch 93 loss: 0.698769211769104, accuracy: 0.7348736524581909\n",
      "Batch 94 loss: 0.6511306166648865, accuracy: 0.7349506616592407\n",
      "Batch 95 loss: 0.6015263795852661, accuracy: 0.7356770634651184\n",
      "Batch 96 loss: 0.8670344352722168, accuracy: 0.735019326210022\n",
      "Batch 97 loss: 0.6817805171012878, accuracy: 0.7352519035339355\n",
      "Batch 98 loss: 0.8056319952011108, accuracy: 0.7349274158477783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 99 loss: 0.7176050543785095, accuracy: 0.7349218726158142\n",
      "Batch 100 loss: 0.7266538739204407, accuracy: 0.7347617745399475\n",
      "Batch 101 loss: 0.6538318395614624, accuracy: 0.7350643277168274\n",
      "Batch 102 loss: 0.6654850840568542, accuracy: 0.7355886101722717\n",
      "Batch 103 loss: 0.6759588718414307, accuracy: 0.7359524965286255\n",
      "Batch 104 loss: 0.6962777972221375, accuracy: 0.7357887029647827\n",
      "Batch 105 loss: 0.778176486492157, accuracy: 0.7357016801834106\n",
      "Batch 106 loss: 0.781898021697998, accuracy: 0.7356892228126526\n",
      "Batch 107 loss: 0.6973680853843689, accuracy: 0.7356770634651184\n",
      "Batch 108 loss: 0.5847211480140686, accuracy: 0.7365252375602722\n",
      "Batch 109 loss: 0.6678501963615417, accuracy: 0.7365056872367859\n",
      "Batch 110 loss: 0.7130123376846313, accuracy: 0.736556887626648\n",
      "Batch 111 loss: 0.8096088171005249, accuracy: 0.7361188530921936\n",
      "Batch 112 loss: 0.7834008932113647, accuracy: 0.7357577681541443\n",
      "Batch 113 loss: 0.5387524366378784, accuracy: 0.7364994287490845\n",
      "Batch 114 loss: 0.7230299115180969, accuracy: 0.736548900604248\n",
      "Batch 115 loss: 0.8002942204475403, accuracy: 0.7362607717514038\n",
      "Batch 116 loss: 0.6093389391899109, accuracy: 0.7367120981216431\n",
      "Batch 117 loss: 0.6942214369773865, accuracy: 0.7365598678588867\n",
      "Batch 118 loss: 0.7601286768913269, accuracy: 0.7365415096282959\n",
      "Batch 119 loss: 0.9560415148735046, accuracy: 0.7359374761581421\n",
      "Batch 120 loss: 0.7982515692710876, accuracy: 0.7354080677032471\n",
      "Batch 121 loss: 0.8059313297271729, accuracy: 0.7348232865333557\n",
      "Batch 122 loss: 0.6519865989685059, accuracy: 0.7347561120986938\n",
      "Batch 123 loss: 0.6614711284637451, accuracy: 0.7351940274238586\n",
      "Batch 124 loss: 0.7142418622970581, accuracy: 0.7353125214576721\n",
      "Batch 125 loss: 0.76456618309021, accuracy: 0.7349330186843872\n",
      "Batch 126 loss: 0.6220160722732544, accuracy: 0.7352977395057678\n",
      "Batch 127 loss: 0.7789003252983093, accuracy: 0.73516845703125\n",
      "Batch 128 loss: 0.7082135677337646, accuracy: 0.7349806427955627\n",
      "Batch 129 loss: 0.6364155411720276, accuracy: 0.735276460647583\n",
      "Batch 130 loss: 0.6811840534210205, accuracy: 0.7354484796524048\n",
      "Batch 131 loss: 0.7950658202171326, accuracy: 0.7351444363594055\n",
      "Batch 132 loss: 0.8723058104515076, accuracy: 0.7345512509346008\n",
      "Batch 133 loss: 0.6595136523246765, accuracy: 0.7342584133148193\n",
      "Batch 134 loss: 0.8712270259857178, accuracy: 0.7337384223937988\n",
      "Batch 135 loss: 0.9193856120109558, accuracy: 0.7329963445663452\n",
      "Batch 136 loss: 0.7403817176818848, accuracy: 0.7329493761062622\n",
      "Batch 137 loss: 0.6214909553527832, accuracy: 0.7333559989929199\n",
      "Batch 138 loss: 0.8550468683242798, accuracy: 0.7328574657440186\n",
      "Batch 139 loss: 0.6724057793617249, accuracy: 0.7328683137893677\n",
      "Batch 140 loss: 0.6503311395645142, accuracy: 0.733045220375061\n",
      "Batch 141 loss: 0.6056657433509827, accuracy: 0.7334946990013123\n",
      "Batch 142 loss: 0.6557688117027283, accuracy: 0.7338286638259888\n",
      "Batch 143 loss: 0.6281551122665405, accuracy: 0.7339951992034912\n",
      "Batch 144 loss: 0.6198263764381409, accuracy: 0.7341594696044922\n",
      "Batch 145 loss: 0.7183130383491516, accuracy: 0.7338933944702148\n",
      "Batch 146 loss: 0.7162297368049622, accuracy: 0.7339498400688171\n",
      "Batch 147 loss: 0.7318995594978333, accuracy: 0.7338998913764954\n",
      "Batch 148 loss: 0.6758924722671509, accuracy: 0.7336933612823486\n",
      "Batch 149 loss: 0.6984691619873047, accuracy: 0.7338541746139526\n",
      "Batch 150 loss: 0.715252697467804, accuracy: 0.7338058948516846\n",
      "Batch 151 loss: 0.8987435698509216, accuracy: 0.7332442402839661\n",
      "Batch 152 loss: 0.6434725522994995, accuracy: 0.7332005500793457\n",
      "Batch 153 loss: 0.7173590660095215, accuracy: 0.7330052852630615\n",
      "Batch 154 loss: 0.5758391618728638, accuracy: 0.7332661151885986\n",
      "Batch 155 loss: 0.6645333766937256, accuracy: 0.7335236668586731\n",
      "Batch 156 loss: 0.706091046333313, accuracy: 0.7337778806686401\n",
      "Batch 157 loss: 0.7476237416267395, accuracy: 0.7333860993385315\n",
      "Batch 158 loss: 0.7401008605957031, accuracy: 0.7336871027946472\n",
      "Batch 159 loss: 0.633429229259491, accuracy: 0.733691394329071\n",
      "Batch 160 loss: 0.5985783934593201, accuracy: 0.7341809272766113\n",
      "Batch 161 loss: 0.7351914048194885, accuracy: 0.7344232201576233\n",
      "Batch 162 loss: 0.7904360294342041, accuracy: 0.734375\n",
      "Batch 163 loss: 0.7039442658424377, accuracy: 0.7342797517776489\n",
      "Batch 164 loss: 0.7166792154312134, accuracy: 0.734375\n",
      "Batch 165 loss: 0.8701807260513306, accuracy: 0.7338102459907532\n",
      "Batch 166 loss: 0.5383453369140625, accuracy: 0.73418790102005\n",
      "Batch 167 loss: 0.5651552081108093, accuracy: 0.7345145344734192\n",
      "Batch 168 loss: 0.6303497552871704, accuracy: 0.7346523404121399\n",
      "Batch 169 loss: 0.7232263684272766, accuracy: 0.7345128655433655\n",
      "Batch 170 loss: 0.6510080695152283, accuracy: 0.7347404956817627\n",
      "Batch 171 loss: 0.6568732261657715, accuracy: 0.7348746657371521\n",
      "Batch 172 loss: 0.7371681332588196, accuracy: 0.7347814440727234\n",
      "Batch 173 loss: 0.66325843334198, accuracy: 0.7350035905838013\n",
      "Batch 174 loss: 0.6103833317756653, accuracy: 0.735133945941925\n",
      "Batch 175 loss: 0.7144010066986084, accuracy: 0.7351295948028564\n",
      "Batch 176 loss: 0.6803033947944641, accuracy: 0.7355667352676392\n",
      "Batch 177 loss: 0.8128571510314941, accuracy: 0.7354283928871155\n",
      "Batch 178 loss: 0.773189902305603, accuracy: 0.7355534434318542\n",
      "Batch 179 loss: 0.5883206129074097, accuracy: 0.7356770634651184\n",
      "Batch 180 loss: 0.7011473178863525, accuracy: 0.7356699109077454\n",
      "Batch 181 loss: 0.8485012054443359, accuracy: 0.7354052066802979\n",
      "Batch 182 loss: 0.5900965332984924, accuracy: 0.735698401927948\n",
      "Batch 183 loss: 0.6123127341270447, accuracy: 0.7357761263847351\n",
      "Batch 184 loss: 0.8259177207946777, accuracy: 0.7355151772499084\n",
      "Batch 185 loss: 0.708833634853363, accuracy: 0.7355090975761414\n",
      "Batch 186 loss: 0.6391766667366028, accuracy: 0.7355030179023743\n",
      "Batch 187 loss: 0.6800995469093323, accuracy: 0.7354139089584351\n",
      "Batch 188 loss: 0.6502975225448608, accuracy: 0.7353257536888123\n",
      "Batch 189 loss: 0.7301375865936279, accuracy: 0.7354440689086914\n",
      "Batch 190 loss: 0.6989728212356567, accuracy: 0.7354794144630432\n",
      "Batch 191 loss: 0.7139338254928589, accuracy: 0.7353515625\n",
      "Batch 192 loss: 0.7019640803337097, accuracy: 0.7352655529975891\n",
      "Batch 193 loss: 0.7648674845695496, accuracy: 0.7351804375648499\n",
      "Batch 194 loss: 0.5808634757995605, accuracy: 0.7354968190193176\n",
      "Batch 195 loss: 0.6171308755874634, accuracy: 0.7355707883834839\n",
      "Batch 196 loss: 0.8914313912391663, accuracy: 0.7352077960968018\n",
      "Batch 197 loss: 0.817729115486145, accuracy: 0.7350457906723022\n",
      "Batch 198 loss: 0.73987877368927, accuracy: 0.7351208925247192\n",
      "Batch 199 loss: 0.7587454319000244, accuracy: 0.7351953387260437\n",
      "Batch 200 loss: 0.6475232839584351, accuracy: 0.7351135015487671\n",
      "Batch 201 loss: 0.7821422219276428, accuracy: 0.7347617745399475\n",
      "Batch 202 loss: 0.763363778591156, accuracy: 0.7347213625907898\n",
      "Batch 203 loss: 0.7332902550697327, accuracy: 0.7347579598426819\n",
      "Batch 204 loss: 0.6519359350204468, accuracy: 0.7347941994667053\n",
      "Batch 205 loss: 0.6433493494987488, accuracy: 0.7349817752838135\n",
      "Batch 206 loss: 0.7673888802528381, accuracy: 0.734714686870575\n",
      "Batch 207 loss: 0.615915060043335, accuracy: 0.734788179397583\n",
      "Batch 208 loss: 0.6302649974822998, accuracy: 0.7349730730056763\n",
      "Batch 209 loss: 0.7334160208702087, accuracy: 0.7348586320877075\n",
      "Batch 210 loss: 0.7648798823356628, accuracy: 0.7349674105644226\n",
      "Batch 211 loss: 0.7833726406097412, accuracy: 0.7347066402435303\n",
      "Batch 212 loss: 0.74002605676651, accuracy: 0.7345216870307922\n",
      "Batch 213 loss: 0.7536377310752869, accuracy: 0.7343385219573975\n",
      "Batch 214 loss: 0.8176513910293579, accuracy: 0.734338641166687\n",
      "Batch 215 loss: 0.7388172149658203, accuracy: 0.7342303395271301\n",
      "Batch 216 loss: 0.557300329208374, accuracy: 0.7346270084381104\n",
      "Batch 217 loss: 0.63177490234375, accuracy: 0.7347333431243896\n",
      "Batch 218 loss: 0.7070950269699097, accuracy: 0.7346603870391846\n",
      "Batch 219 loss: 0.6636155247688293, accuracy: 0.7347656488418579\n",
      "Batch 220 loss: 0.5568403601646423, accuracy: 0.7349405884742737\n",
      "Batch 221 loss: 0.8340101838111877, accuracy: 0.7348324656486511\n",
      "Batch 222 loss: 0.7478969097137451, accuracy: 0.7348304390907288\n",
      "Batch 223 loss: 0.6120902895927429, accuracy: 0.7349678874015808\n",
      "Batch 224 loss: 0.648277997970581, accuracy: 0.7349611520767212\n",
      "Training epoch: 38, train accuracy: 73.4961166381836, train loss: 0.7091198998027378, valid accuracy: 64.89273071289062, valid loss: 1.128731941354686 \n",
      "Batch 0 loss: 0.6082831621170044, accuracy: 0.7421875\n",
      "Batch 1 loss: 0.7085789442062378, accuracy: 0.734375\n",
      "Batch 2 loss: 0.7453984022140503, accuracy: 0.7239583134651184\n",
      "Batch 3 loss: 0.6975803375244141, accuracy: 0.728515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 loss: 0.6452723145484924, accuracy: 0.7359374761581421\n",
      "Batch 5 loss: 0.7162214517593384, accuracy: 0.7330729365348816\n",
      "Batch 6 loss: 0.6869479417800903, accuracy: 0.734375\n",
      "Batch 7 loss: 0.5950810313224792, accuracy: 0.7392578125\n",
      "Batch 8 loss: 0.5680942535400391, accuracy: 0.7439236044883728\n",
      "Batch 9 loss: 0.8252190947532654, accuracy: 0.7421875\n",
      "Batch 10 loss: 0.7084107995033264, accuracy: 0.7450284361839294\n",
      "Batch 11 loss: 0.5050137639045715, accuracy: 0.75\n",
      "Batch 12 loss: 0.7211291790008545, accuracy: 0.746995210647583\n",
      "Batch 13 loss: 0.6472245454788208, accuracy: 0.7477678656578064\n",
      "Batch 14 loss: 0.7735803127288818, accuracy: 0.7458333373069763\n",
      "Batch 15 loss: 0.6943526268005371, accuracy: 0.744140625\n",
      "Batch 16 loss: 0.7317999005317688, accuracy: 0.7449448704719543\n",
      "Batch 17 loss: 0.736422061920166, accuracy: 0.7426215410232544\n",
      "Batch 18 loss: 0.6414241194725037, accuracy: 0.7425987124443054\n",
      "Batch 19 loss: 0.4975017309188843, accuracy: 0.7457031011581421\n",
      "Batch 20 loss: 0.6802566647529602, accuracy: 0.7455357313156128\n",
      "Batch 21 loss: 0.7938328981399536, accuracy: 0.7432528138160706\n",
      "Batch 22 loss: 0.7433698177337646, accuracy: 0.7428668737411499\n",
      "Batch 23 loss: 0.7284765243530273, accuracy: 0.7434895634651184\n",
      "Batch 24 loss: 0.6515883207321167, accuracy: 0.7434375286102295\n",
      "Batch 25 loss: 0.6864903569221497, accuracy: 0.7427884340286255\n",
      "Batch 26 loss: 0.6582068204879761, accuracy: 0.7418981194496155\n",
      "Batch 27 loss: 0.6715725660324097, accuracy: 0.7407923936843872\n",
      "Batch 28 loss: 0.6436561346054077, accuracy: 0.7427262663841248\n",
      "Batch 29 loss: 0.5874807238578796, accuracy: 0.7434895634651184\n",
      "Batch 30 loss: 0.6159462332725525, accuracy: 0.7452117204666138\n",
      "Batch 31 loss: 0.5227941870689392, accuracy: 0.747314453125\n",
      "Batch 32 loss: 0.794081449508667, accuracy: 0.7462121248245239\n",
      "Batch 33 loss: 0.6051791310310364, accuracy: 0.7472426295280457\n",
      "Batch 34 loss: 0.5873312950134277, accuracy: 0.7479910850524902\n",
      "Batch 35 loss: 0.667604386806488, accuracy: 0.748046875\n",
      "Batch 36 loss: 0.4773995876312256, accuracy: 0.75\n",
      "Batch 37 loss: 0.6118295192718506, accuracy: 0.7520559430122375\n",
      "Batch 38 loss: 0.6874884963035583, accuracy: 0.7516025900840759\n",
      "Batch 39 loss: 0.7850090265274048, accuracy: 0.7513672113418579\n",
      "Batch 40 loss: 0.7748172283172607, accuracy: 0.7511432766914368\n",
      "Batch 41 loss: 0.7210357189178467, accuracy: 0.7507440447807312\n",
      "Batch 42 loss: 0.6793947219848633, accuracy: 0.75\n",
      "Batch 43 loss: 0.7699854969978333, accuracy: 0.7485795617103577\n",
      "Batch 44 loss: 0.6215800046920776, accuracy: 0.7487847208976746\n",
      "Batch 45 loss: 0.6835425496101379, accuracy: 0.7488111257553101\n",
      "Batch 46 loss: 0.5697429776191711, accuracy: 0.750831127166748\n",
      "Batch 47 loss: 0.5745213627815247, accuracy: 0.7511393427848816\n",
      "Batch 48 loss: 0.6767582297325134, accuracy: 0.7501594424247742\n",
      "Batch 49 loss: 0.5969752073287964, accuracy: 0.7506250143051147\n",
      "Batch 50 loss: 0.5845344066619873, accuracy: 0.7510722875595093\n",
      "Batch 51 loss: 0.6614411473274231, accuracy: 0.7506009340286255\n",
      "Batch 52 loss: 0.6562784314155579, accuracy: 0.7495577931404114\n",
      "Batch 53 loss: 0.6922920942306519, accuracy: 0.7491319179534912\n",
      "Batch 54 loss: 0.7276477217674255, accuracy: 0.7490056753158569\n",
      "Batch 55 loss: 0.7648140788078308, accuracy: 0.7484654188156128\n",
      "Batch 56 loss: 0.6373528242111206, accuracy: 0.7486293911933899\n",
      "Batch 57 loss: 0.5992568135261536, accuracy: 0.748652994632721\n",
      "Batch 58 loss: 0.7223017811775208, accuracy: 0.7489407062530518\n",
      "Batch 59 loss: 0.7268907427787781, accuracy: 0.7484375238418579\n",
      "Batch 60 loss: 0.578639805316925, accuracy: 0.7484630942344666\n",
      "Batch 61 loss: 0.7063918113708496, accuracy: 0.7484878897666931\n",
      "Batch 62 loss: 0.775920569896698, accuracy: 0.7480158805847168\n",
      "Batch 63 loss: 0.7948622703552246, accuracy: 0.7474365234375\n",
      "Batch 64 loss: 0.6436257362365723, accuracy: 0.7479567527770996\n",
      "Batch 65 loss: 0.7477465867996216, accuracy: 0.7472774386405945\n",
      "Batch 66 loss: 0.6005887985229492, accuracy: 0.7483675479888916\n",
      "Batch 67 loss: 0.5653415322303772, accuracy: 0.7494255304336548\n",
      "Batch 68 loss: 0.5203750729560852, accuracy: 0.7501132488250732\n",
      "Batch 69 loss: 0.6169462203979492, accuracy: 0.7502232193946838\n",
      "Batch 70 loss: 0.7540611624717712, accuracy: 0.7497799396514893\n",
      "Batch 71 loss: 0.7838470935821533, accuracy: 0.7496744990348816\n",
      "Batch 72 loss: 0.6566944122314453, accuracy: 0.7495719194412231\n",
      "Batch 73 loss: 0.8742817640304565, accuracy: 0.7489442825317383\n",
      "Batch 74 loss: 0.6954835057258606, accuracy: 0.7494791746139526\n",
      "Batch 75 loss: 0.48460134863853455, accuracy: 0.7503083944320679\n",
      "Batch 76 loss: 0.6054011583328247, accuracy: 0.7510145902633667\n",
      "Batch 77 loss: 0.9633824229240417, accuracy: 0.7502003312110901\n",
      "Batch 78 loss: 0.4346727430820465, accuracy: 0.7516811490058899\n",
      "Batch 79 loss: 0.6521445512771606, accuracy: 0.7515624761581421\n",
      "Batch 80 loss: 0.7943041920661926, accuracy: 0.7509645223617554\n",
      "Batch 81 loss: 0.8521589040756226, accuracy: 0.7507622241973877\n",
      "Batch 82 loss: 0.7740432620048523, accuracy: 0.7505647540092468\n",
      "Batch 83 loss: 0.5688318014144897, accuracy: 0.7512090802192688\n",
      "Batch 84 loss: 0.8203561305999756, accuracy: 0.7508271932601929\n",
      "Batch 85 loss: 0.7809120416641235, accuracy: 0.750545084476471\n",
      "Batch 86 loss: 0.6960413455963135, accuracy: 0.75\n",
      "Batch 87 loss: 0.6893739104270935, accuracy: 0.7499112486839294\n",
      "Batch 88 loss: 0.5525957942008972, accuracy: 0.7506144642829895\n",
      "Batch 89 loss: 0.7741933465003967, accuracy: 0.7502604126930237\n",
      "Batch 90 loss: 0.7164629101753235, accuracy: 0.75\n",
      "Batch 91 loss: 0.7932635545730591, accuracy: 0.749660313129425\n",
      "Batch 92 loss: 0.6853435635566711, accuracy: 0.7503360509872437\n",
      "Batch 93 loss: 0.6878537535667419, accuracy: 0.7507479786872864\n",
      "Batch 94 loss: 0.8271357417106628, accuracy: 0.75\n",
      "Batch 95 loss: 0.6754702925682068, accuracy: 0.7501627802848816\n",
      "Batch 96 loss: 0.6754894852638245, accuracy: 0.7500805258750916\n",
      "Batch 97 loss: 0.7363166213035583, accuracy: 0.75\n",
      "Batch 98 loss: 0.778935968875885, accuracy: 0.7497632503509521\n",
      "Batch 99 loss: 0.6215261816978455, accuracy: 0.7497656345367432\n",
      "Batch 100 loss: 0.7201796770095825, accuracy: 0.7496132254600525\n",
      "Batch 101 loss: 0.7368594408035278, accuracy: 0.7493106722831726\n",
      "Batch 102 loss: 0.7070105075836182, accuracy: 0.7495449185371399\n",
      "Batch 103 loss: 0.5706215500831604, accuracy: 0.7497746348381042\n",
      "Batch 104 loss: 0.7266743183135986, accuracy: 0.7495535612106323\n",
      "Batch 105 loss: 0.8224042654037476, accuracy: 0.7491892576217651\n",
      "Batch 106 loss: 0.9124181866645813, accuracy: 0.748393714427948\n",
      "Batch 107 loss: 0.6966208815574646, accuracy: 0.7482638955116272\n",
      "Batch 108 loss: 0.8769696354866028, accuracy: 0.7478497624397278\n",
      "Batch 109 loss: 0.8605966567993164, accuracy: 0.7469460368156433\n",
      "Batch 110 loss: 0.670494556427002, accuracy: 0.747184693813324\n",
      "Batch 111 loss: 0.6609267592430115, accuracy: 0.7470703125\n",
      "Batch 112 loss: 0.5830377340316772, accuracy: 0.7473728060722351\n",
      "Batch 113 loss: 0.6033458709716797, accuracy: 0.7476014494895935\n",
      "Batch 114 loss: 0.7316703796386719, accuracy: 0.7474184632301331\n",
      "Batch 115 loss: 0.7896546125411987, accuracy: 0.7470366358757019\n",
      "Batch 116 loss: 0.7634280920028687, accuracy: 0.7469283938407898\n",
      "Batch 117 loss: 0.6794363856315613, accuracy: 0.7470868825912476\n",
      "Batch 118 loss: 0.6103425025939941, accuracy: 0.7471113204956055\n",
      "Batch 119 loss: 0.6883418560028076, accuracy: 0.7470052242279053\n",
      "Batch 120 loss: 0.5122875571250916, accuracy: 0.7472882270812988\n",
      "Batch 121 loss: 0.7558501362800598, accuracy: 0.7470542788505554\n",
      "Batch 122 loss: 0.7212163805961609, accuracy: 0.7469512224197388\n",
      "Batch 123 loss: 0.6553897857666016, accuracy: 0.7467237710952759\n",
      "Batch 124 loss: 0.6769702434539795, accuracy: 0.746749997138977\n",
      "Batch 125 loss: 0.8510494828224182, accuracy: 0.7462177872657776\n",
      "Batch 126 loss: 0.669927716255188, accuracy: 0.7460014820098877\n",
      "Batch 127 loss: 0.7472299933433533, accuracy: 0.74591064453125\n",
      "Batch 128 loss: 0.6654269099235535, accuracy: 0.7461240291595459\n",
      "Batch 129 loss: 0.6463959813117981, accuracy: 0.7465144395828247\n",
      "Batch 130 loss: 0.7256808876991272, accuracy: 0.7463024854660034\n",
      "Batch 131 loss: 0.6242318153381348, accuracy: 0.7465080618858337\n",
      "Batch 132 loss: 0.8013187050819397, accuracy: 0.746181845664978\n",
      "Batch 133 loss: 0.6597132682800293, accuracy: 0.7463269829750061\n",
      "Batch 134 loss: 0.7643651962280273, accuracy: 0.7459490895271301\n",
      "Batch 135 loss: 0.686795175075531, accuracy: 0.745978832244873\n",
      "Batch 136 loss: 0.7111458778381348, accuracy: 0.7459511756896973\n",
      "Batch 137 loss: 0.6845991611480713, accuracy: 0.7458673119544983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 138 loss: 0.5549319386482239, accuracy: 0.7459532618522644\n",
      "Batch 139 loss: 0.8434090614318848, accuracy: 0.7454798817634583\n",
      "Batch 140 loss: 0.7285808324813843, accuracy: 0.7455119490623474\n",
      "Batch 141 loss: 0.6319652199745178, accuracy: 0.7456536293029785\n",
      "Batch 142 loss: 0.6801672577857971, accuracy: 0.7459571957588196\n",
      "Batch 143 loss: 0.6087862849235535, accuracy: 0.7462022304534912\n",
      "Batch 144 loss: 0.8385201692581177, accuracy: 0.7457973957061768\n",
      "Batch 145 loss: 0.6657094359397888, accuracy: 0.7456656694412231\n",
      "Batch 146 loss: 0.7477606534957886, accuracy: 0.7453762888908386\n",
      "Batch 147 loss: 0.7233551144599915, accuracy: 0.7452491521835327\n",
      "Batch 148 loss: 0.7186885476112366, accuracy: 0.7450188994407654\n",
      "Batch 149 loss: 0.6933086514472961, accuracy: 0.7450520992279053\n",
      "Batch 150 loss: 0.6736034154891968, accuracy: 0.7452400922775269\n",
      "Batch 151 loss: 0.6765635013580322, accuracy: 0.7453227639198303\n",
      "Batch 152 loss: 0.5951459407806396, accuracy: 0.7456597089767456\n",
      "Batch 153 loss: 0.7292947769165039, accuracy: 0.7456371784210205\n",
      "Batch 154 loss: 0.6782461404800415, accuracy: 0.7457661032676697\n",
      "Batch 155 loss: 0.8034271597862244, accuracy: 0.7451422214508057\n",
      "Batch 156 loss: 0.6838511228561401, accuracy: 0.7450239062309265\n",
      "Batch 157 loss: 0.6569343209266663, accuracy: 0.7450553774833679\n",
      "Batch 158 loss: 0.7851581573486328, accuracy: 0.744693398475647\n",
      "Batch 159 loss: 0.7098177075386047, accuracy: 0.744580090045929\n",
      "Batch 160 loss: 0.8002762794494629, accuracy: 0.7444196343421936\n",
      "Batch 161 loss: 0.6619512438774109, accuracy: 0.7444540858268738\n",
      "Batch 162 loss: 0.6673650145530701, accuracy: 0.7444401979446411\n",
      "Batch 163 loss: 0.6360921263694763, accuracy: 0.7443788051605225\n",
      "Batch 164 loss: 0.6361259818077087, accuracy: 0.7447443008422852\n",
      "Batch 165 loss: 0.6906948089599609, accuracy: 0.7447289228439331\n",
      "Batch 166 loss: 0.6106042265892029, accuracy: 0.744620144367218\n",
      "Batch 167 loss: 0.6780354976654053, accuracy: 0.7445591688156128\n",
      "Batch 168 loss: 0.7030112147331238, accuracy: 0.7443139553070068\n",
      "Batch 169 loss: 0.680689811706543, accuracy: 0.7443934082984924\n",
      "Batch 170 loss: 0.5505152940750122, accuracy: 0.7449744343757629\n",
      "Batch 171 loss: 0.7188806533813477, accuracy: 0.7451398968696594\n",
      "Batch 172 loss: 0.7558000087738037, accuracy: 0.7451679706573486\n",
      "Batch 173 loss: 0.6898314952850342, accuracy: 0.7449712753295898\n",
      "Batch 174 loss: 0.8241866230964661, accuracy: 0.7445535659790039\n",
      "Batch 175 loss: 0.6613431572914124, accuracy: 0.7443625926971436\n",
      "Batch 176 loss: 0.8746729493141174, accuracy: 0.7439530491828918\n",
      "Batch 177 loss: 0.6812109351158142, accuracy: 0.7441625595092773\n",
      "Batch 178 loss: 0.6036838293075562, accuracy: 0.7443697452545166\n",
      "Batch 179 loss: 0.7957722544670105, accuracy: 0.744140625\n",
      "Batch 180 loss: 0.7324389219284058, accuracy: 0.7440866827964783\n",
      "Batch 181 loss: 0.6615484952926636, accuracy: 0.7442479133605957\n",
      "Batch 182 loss: 0.6130127310752869, accuracy: 0.7444501519203186\n",
      "Batch 183 loss: 0.6711127758026123, accuracy: 0.7446926236152649\n",
      "Batch 184 loss: 0.7919269800186157, accuracy: 0.7446790337562561\n",
      "Batch 185 loss: 0.697221577167511, accuracy: 0.7447916865348816\n",
      "Batch 186 loss: 0.7657464146614075, accuracy: 0.7447359561920166\n",
      "Batch 187 loss: 0.652497410774231, accuracy: 0.7448470592498779\n",
      "Batch 188 loss: 0.7499173879623413, accuracy: 0.7447090148925781\n",
      "Batch 189 loss: 0.6607052683830261, accuracy: 0.7449012994766235\n",
      "Batch 190 loss: 0.6167920231819153, accuracy: 0.7449280023574829\n",
      "Batch 191 loss: 0.8893118500709534, accuracy: 0.7444254755973816\n",
      "Batch 192 loss: 0.7322829961776733, accuracy: 0.7444543242454529\n",
      "Batch 193 loss: 0.6549388766288757, accuracy: 0.7444426417350769\n",
      "Batch 194 loss: 0.7422987222671509, accuracy: 0.7443109154701233\n",
      "Batch 195 loss: 0.8185741901397705, accuracy: 0.7442203164100647\n",
      "Batch 196 loss: 0.6810047626495361, accuracy: 0.7442893385887146\n",
      "Batch 197 loss: 0.7460646629333496, accuracy: 0.7443181872367859\n",
      "Batch 198 loss: 0.7323201298713684, accuracy: 0.7441504597663879\n",
      "Batch 199 loss: 0.8023062944412231, accuracy: 0.7439844012260437\n",
      "Batch 200 loss: 0.6663339734077454, accuracy: 0.7441309094429016\n",
      "Batch 201 loss: 0.6590214967727661, accuracy: 0.7442759871482849\n",
      "Batch 202 loss: 0.7193253636360168, accuracy: 0.7442272305488586\n",
      "Batch 203 loss: 0.7531186938285828, accuracy: 0.744025707244873\n",
      "Batch 204 loss: 0.8447046875953674, accuracy: 0.7435213327407837\n",
      "Batch 205 loss: 0.5514664053916931, accuracy: 0.7439699769020081\n",
      "Batch 206 loss: 0.686342179775238, accuracy: 0.74388587474823\n",
      "Batch 207 loss: 0.7477821707725525, accuracy: 0.7436147928237915\n",
      "Batch 208 loss: 0.6954752206802368, accuracy: 0.7436827421188354\n",
      "Batch 209 loss: 0.6449359655380249, accuracy: 0.7437127828598022\n",
      "Batch 210 loss: 0.767426609992981, accuracy: 0.7436315417289734\n",
      "Batch 211 loss: 0.8213842511177063, accuracy: 0.7434772849082947\n",
      "Batch 212 loss: 0.5962982177734375, accuracy: 0.7436913251876831\n",
      "Batch 213 loss: 0.7262459397315979, accuracy: 0.7435382604598999\n",
      "Batch 214 loss: 0.6572379469871521, accuracy: 0.7438226938247681\n",
      "Batch 215 loss: 0.7108247876167297, accuracy: 0.7438512444496155\n",
      "Batch 216 loss: 0.6238653659820557, accuracy: 0.7440236210823059\n",
      "Batch 217 loss: 0.7368215322494507, accuracy: 0.7438718676567078\n",
      "Batch 218 loss: 0.8066359162330627, accuracy: 0.7437928318977356\n",
      "Batch 219 loss: 0.655664324760437, accuracy: 0.7438565492630005\n",
      "Batch 220 loss: 0.6201562881469727, accuracy: 0.7438136339187622\n",
      "Batch 221 loss: 0.7000053524971008, accuracy: 0.7436303496360779\n",
      "Batch 222 loss: 0.7760547399520874, accuracy: 0.7435538172721863\n",
      "Batch 223 loss: 0.9820261001586914, accuracy: 0.7432337999343872\n",
      "Batch 224 loss: 0.5703416466712952, accuracy: 0.7432512640953064\n",
      "Training epoch: 39, train accuracy: 74.32512664794922, train loss: 0.6955556694666545, valid accuracy: 63.86180114746094, valid loss: 1.1487319243365322 \n",
      "Batch 0 loss: 0.6488869786262512, accuracy: 0.734375\n",
      "Batch 1 loss: 0.6708292961120605, accuracy: 0.73046875\n",
      "Batch 2 loss: 0.5888779163360596, accuracy: 0.7552083134651184\n",
      "Batch 3 loss: 0.6776761412620544, accuracy: 0.748046875\n",
      "Batch 4 loss: 0.7338077425956726, accuracy: 0.745312511920929\n",
      "Batch 5 loss: 0.6139049530029297, accuracy: 0.7513020634651184\n",
      "Batch 6 loss: 0.6681755781173706, accuracy: 0.7455357313156128\n",
      "Batch 7 loss: 0.7174938917160034, accuracy: 0.7470703125\n",
      "Batch 8 loss: 0.7104247808456421, accuracy: 0.7447916865348816\n",
      "Batch 9 loss: 0.6611140966415405, accuracy: 0.7484375238418579\n",
      "Batch 10 loss: 0.6394063830375671, accuracy: 0.7457386255264282\n",
      "Batch 11 loss: 0.7448238134384155, accuracy: 0.7415364384651184\n",
      "Batch 12 loss: 0.7294831275939941, accuracy: 0.7403846383094788\n",
      "Batch 13 loss: 0.7324554920196533, accuracy: 0.7366071343421936\n",
      "Batch 14 loss: 0.6582184433937073, accuracy: 0.739062488079071\n",
      "Batch 15 loss: 0.605247437953949, accuracy: 0.7421875\n",
      "Batch 16 loss: 0.6466861367225647, accuracy: 0.7408088445663452\n",
      "Batch 17 loss: 0.6966923475265503, accuracy: 0.73828125\n",
      "Batch 18 loss: 0.7015582919120789, accuracy: 0.7376644611358643\n",
      "Batch 19 loss: 0.5445391535758972, accuracy: 0.739062488079071\n",
      "Batch 20 loss: 0.5854862928390503, accuracy: 0.7406994104385376\n",
      "Batch 21 loss: 0.505656898021698, accuracy: 0.7453835010528564\n",
      "Batch 22 loss: 0.6590641736984253, accuracy: 0.746942937374115\n",
      "Batch 23 loss: 0.6802366375923157, accuracy: 0.7473958134651184\n",
      "Batch 24 loss: 0.7587035894393921, accuracy: 0.7456250190734863\n",
      "Batch 25 loss: 0.816872775554657, accuracy: 0.7445913553237915\n",
      "Batch 26 loss: 0.8253747820854187, accuracy: 0.7450810074806213\n",
      "Batch 27 loss: 0.7841183543205261, accuracy: 0.7419084906578064\n",
      "Batch 28 loss: 0.750362753868103, accuracy: 0.7400323152542114\n",
      "Batch 29 loss: 0.7189404964447021, accuracy: 0.739062488079071\n",
      "Batch 30 loss: 0.6622405052185059, accuracy: 0.7394153475761414\n",
      "Batch 31 loss: 0.8220956921577454, accuracy: 0.73876953125\n",
      "Batch 32 loss: 0.7346444725990295, accuracy: 0.7379261255264282\n",
      "Batch 33 loss: 0.6028357148170471, accuracy: 0.73828125\n",
      "Batch 34 loss: 0.6726318001747131, accuracy: 0.7381696701049805\n",
      "Batch 35 loss: 0.6761782169342041, accuracy: 0.7378472089767456\n",
      "Batch 36 loss: 0.7704943418502808, accuracy: 0.7369087934494019\n",
      "Batch 37 loss: 0.6531580686569214, accuracy: 0.7376644611358643\n",
      "Batch 38 loss: 0.6334139108657837, accuracy: 0.7387820482254028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 39 loss: 0.6305763125419617, accuracy: 0.7396484613418579\n",
      "Batch 40 loss: 0.7574186325073242, accuracy: 0.7397103905677795\n",
      "Batch 41 loss: 0.5467655658721924, accuracy: 0.7412574291229248\n",
      "Batch 42 loss: 0.6554180979728699, accuracy: 0.741642415523529\n",
      "Batch 43 loss: 0.6035324931144714, accuracy: 0.7421875\n",
      "Batch 44 loss: 0.7037874460220337, accuracy: 0.7414930462837219\n",
      "Batch 45 loss: 0.640507698059082, accuracy: 0.7421875\n",
      "Batch 46 loss: 0.6505423188209534, accuracy: 0.743517279624939\n",
      "Batch 47 loss: 0.661246657371521, accuracy: 0.7439778447151184\n",
      "Batch 48 loss: 0.5888622999191284, accuracy: 0.7448979616165161\n",
      "Batch 49 loss: 0.5898707509040833, accuracy: 0.745312511920929\n",
      "Batch 50 loss: 0.6230999231338501, accuracy: 0.7455576062202454\n",
      "Batch 51 loss: 0.7201051712036133, accuracy: 0.7456430196762085\n",
      "Batch 52 loss: 0.6397017240524292, accuracy: 0.7467570900917053\n",
      "Batch 53 loss: 0.6697790622711182, accuracy: 0.7472511529922485\n",
      "Batch 54 loss: 0.678378701210022, accuracy: 0.746874988079071\n",
      "Batch 55 loss: 0.6683134436607361, accuracy: 0.7467913031578064\n",
      "Batch 56 loss: 0.7047345638275146, accuracy: 0.7471216917037964\n",
      "Batch 57 loss: 0.6382308006286621, accuracy: 0.7467672228813171\n",
      "Batch 58 loss: 0.6765254139900208, accuracy: 0.7472192645072937\n",
      "Batch 59 loss: 0.713610827922821, accuracy: 0.746874988079071\n",
      "Batch 60 loss: 0.5817630887031555, accuracy: 0.7475665807723999\n",
      "Batch 61 loss: 0.6391898393630981, accuracy: 0.7483618855476379\n",
      "Batch 62 loss: 0.6655113101005554, accuracy: 0.7481398582458496\n",
      "Batch 63 loss: 0.6493584513664246, accuracy: 0.748046875\n",
      "Batch 64 loss: 0.8259174823760986, accuracy: 0.7475961446762085\n",
      "Batch 65 loss: 0.6861444711685181, accuracy: 0.7475141882896423\n",
      "Batch 66 loss: 0.6932500004768372, accuracy: 0.7475513219833374\n",
      "Batch 67 loss: 0.7429312467575073, accuracy: 0.7473575472831726\n",
      "Batch 68 loss: 0.6762893795967102, accuracy: 0.7476223111152649\n",
      "Batch 69 loss: 0.667177677154541, accuracy: 0.7483258843421936\n",
      "Batch 70 loss: 0.6376309394836426, accuracy: 0.7482394576072693\n",
      "Batch 71 loss: 0.5888899564743042, accuracy: 0.7488064169883728\n",
      "Batch 72 loss: 0.6330679655075073, accuracy: 0.7489297986030579\n",
      "Batch 73 loss: 0.6372272372245789, accuracy: 0.7495777010917664\n",
      "Batch 74 loss: 0.7697361707687378, accuracy: 0.7491666674613953\n",
      "Batch 75 loss: 0.7437302470207214, accuracy: 0.7489720582962036\n",
      "Batch 76 loss: 0.8004080057144165, accuracy: 0.7480722665786743\n",
      "Batch 77 loss: 0.5354622602462769, accuracy: 0.7485977411270142\n",
      "Batch 78 loss: 0.6479142904281616, accuracy: 0.7485166192054749\n",
      "Batch 79 loss: 0.6930713653564453, accuracy: 0.74853515625\n",
      "Batch 80 loss: 0.6937137842178345, accuracy: 0.7481674551963806\n",
      "Batch 81 loss: 0.6382941007614136, accuracy: 0.7483803629875183\n",
      "Batch 82 loss: 0.6060529947280884, accuracy: 0.7486822009086609\n",
      "Batch 83 loss: 0.6523051261901855, accuracy: 0.748232901096344\n",
      "Batch 84 loss: 0.639083981513977, accuracy: 0.7485294342041016\n",
      "Batch 85 loss: 0.6168373823165894, accuracy: 0.7485465407371521\n",
      "Batch 86 loss: 0.6210427284240723, accuracy: 0.749101996421814\n",
      "Batch 87 loss: 0.5635684132575989, accuracy: 0.7496448755264282\n",
      "Batch 88 loss: 0.7532418370246887, accuracy: 0.749561071395874\n",
      "Batch 89 loss: 0.6019545793533325, accuracy: 0.75\n",
      "Batch 90 loss: 0.6955146789550781, accuracy: 0.75\n",
      "Batch 91 loss: 0.7180073261260986, accuracy: 0.74974524974823\n",
      "Batch 92 loss: 0.7256919145584106, accuracy: 0.7491599321365356\n",
      "Batch 93 loss: 0.6938580274581909, accuracy: 0.7490857839584351\n",
      "Batch 94 loss: 0.731657087802887, accuracy: 0.7486019730567932\n",
      "Batch 95 loss: 0.7098278403282166, accuracy: 0.7484537959098816\n",
      "Batch 96 loss: 0.8083367943763733, accuracy: 0.7481475472450256\n",
      "Batch 97 loss: 0.6141049861907959, accuracy: 0.7484056353569031\n",
      "Batch 98 loss: 0.8597439527511597, accuracy: 0.7481849789619446\n",
      "Batch 99 loss: 0.5692256689071655, accuracy: 0.7486718893051147\n",
      "Batch 100 loss: 0.650071918964386, accuracy: 0.7485303282737732\n",
      "Batch 101 loss: 0.5679197311401367, accuracy: 0.7489277124404907\n",
      "Batch 102 loss: 0.6660187244415283, accuracy: 0.749089777469635\n",
      "Batch 103 loss: 0.7231011390686035, accuracy: 0.7485727071762085\n",
      "Batch 104 loss: 0.7301881909370422, accuracy: 0.7482886910438538\n",
      "Batch 105 loss: 0.5196802616119385, accuracy: 0.7489681839942932\n",
      "Batch 106 loss: 0.700059711933136, accuracy: 0.7492698431015015\n",
      "Batch 107 loss: 0.8071709275245667, accuracy: 0.7484809160232544\n",
      "Batch 108 loss: 0.7264705300331116, accuracy: 0.7479214668273926\n",
      "Batch 109 loss: 0.6093704700469971, accuracy: 0.7482954263687134\n",
      "Batch 110 loss: 0.6666642427444458, accuracy: 0.7479588985443115\n",
      "Batch 111 loss: 0.6482324600219727, accuracy: 0.7474888563156128\n",
      "Batch 112 loss: 0.6300675868988037, accuracy: 0.7475801706314087\n",
      "Batch 113 loss: 0.6590644717216492, accuracy: 0.7476014494895935\n",
      "Batch 114 loss: 0.6981792449951172, accuracy: 0.7474184632301331\n",
      "Batch 115 loss: 0.6136963367462158, accuracy: 0.747440755367279\n",
      "Batch 116 loss: 0.5784595012664795, accuracy: 0.747863233089447\n",
      "Batch 117 loss: 0.592789888381958, accuracy: 0.7478813529014587\n",
      "Batch 118 loss: 0.7697071433067322, accuracy: 0.7478334903717041\n",
      "Batch 119 loss: 0.8875779509544373, accuracy: 0.7472005486488342\n",
      "Batch 120 loss: 0.6245412230491638, accuracy: 0.7472882270812988\n",
      "Batch 121 loss: 0.653218686580658, accuracy: 0.7475025653839111\n",
      "Batch 122 loss: 0.5739802122116089, accuracy: 0.7479039430618286\n",
      "Batch 123 loss: 0.6419777274131775, accuracy: 0.7477318644523621\n",
      "Batch 124 loss: 0.6973661780357361, accuracy: 0.7476875185966492\n",
      "Batch 125 loss: 0.5847089290618896, accuracy: 0.7479538917541504\n",
      "Batch 126 loss: 0.6791967749595642, accuracy: 0.7478469610214233\n",
      "Batch 127 loss: 0.7361724376678467, accuracy: 0.7476806640625\n",
      "Batch 128 loss: 0.5904648303985596, accuracy: 0.7479408979415894\n",
      "Batch 129 loss: 0.7866373062133789, accuracy: 0.7477163672447205\n",
      "Batch 130 loss: 0.6917129755020142, accuracy: 0.7478530406951904\n",
      "Batch 131 loss: 0.7217094302177429, accuracy: 0.7476325631141663\n",
      "Batch 132 loss: 0.6237462759017944, accuracy: 0.7478853464126587\n",
      "Batch 133 loss: 0.8608034253120422, accuracy: 0.747434675693512\n",
      "Batch 134 loss: 0.6346205472946167, accuracy: 0.7473379373550415\n",
      "Batch 135 loss: 0.6749497652053833, accuracy: 0.7474150061607361\n",
      "Batch 136 loss: 0.582595705986023, accuracy: 0.7478330135345459\n",
      "Batch 137 loss: 0.6265608072280884, accuracy: 0.7482450008392334\n",
      "Batch 138 loss: 0.9258970022201538, accuracy: 0.748032808303833\n",
      "Batch 139 loss: 0.8138027787208557, accuracy: 0.7478236556053162\n",
      "Batch 140 loss: 0.7559365630149841, accuracy: 0.7478945255279541\n",
      "Batch 141 loss: 0.6843476295471191, accuracy: 0.7479643225669861\n",
      "Batch 142 loss: 0.4929090738296509, accuracy: 0.7485795617103577\n",
      "Batch 143 loss: 0.70002281665802, accuracy: 0.7483181357383728\n",
      "Batch 144 loss: 0.7753679752349854, accuracy: 0.7481142282485962\n",
      "Batch 145 loss: 0.7509897351264954, accuracy: 0.7480736374855042\n",
      "Batch 146 loss: 0.6309179067611694, accuracy: 0.7482461929321289\n",
      "Batch 147 loss: 0.7740821242332458, accuracy: 0.748046875\n",
      "Batch 148 loss: 0.6585367918014526, accuracy: 0.7479551434516907\n",
      "Batch 149 loss: 0.850311815738678, accuracy: 0.7472916841506958\n",
      "Batch 150 loss: 0.6547247171401978, accuracy: 0.7473096251487732\n",
      "Batch 151 loss: 0.7573289275169373, accuracy: 0.7470703125\n",
      "Batch 152 loss: 0.6450742483139038, accuracy: 0.7472426295280457\n",
      "Batch 153 loss: 0.8762409687042236, accuracy: 0.7467532753944397\n",
      "Batch 154 loss: 0.8004974126815796, accuracy: 0.7463205456733704\n",
      "Batch 155 loss: 0.8582221865653992, accuracy: 0.7456930875778198\n",
      "Batch 156 loss: 0.6799560785293579, accuracy: 0.7459195852279663\n",
      "Batch 157 loss: 0.654278039932251, accuracy: 0.746242105960846\n",
      "Batch 158 loss: 0.6336867809295654, accuracy: 0.7464131116867065\n",
      "Batch 159 loss: 0.6266199946403503, accuracy: 0.746386706829071\n",
      "Batch 160 loss: 0.6392186880111694, accuracy: 0.7465547323226929\n",
      "Batch 161 loss: 0.666330873966217, accuracy: 0.7466724514961243\n",
      "Batch 162 loss: 0.6572095155715942, accuracy: 0.7465490698814392\n",
      "Batch 163 loss: 0.7204604744911194, accuracy: 0.7465701103210449\n",
      "Batch 164 loss: 0.635768711566925, accuracy: 0.7466856241226196\n",
      "Batch 165 loss: 0.5330584049224854, accuracy: 0.7469879388809204\n",
      "Batch 166 loss: 0.6816579103469849, accuracy: 0.7468188405036926\n",
      "Batch 167 loss: 0.7043789625167847, accuracy: 0.7467913031578064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 168 loss: 0.646172285079956, accuracy: 0.746995210647583\n",
      "Batch 169 loss: 0.592075526714325, accuracy: 0.747150719165802\n",
      "Batch 170 loss: 0.6406516432762146, accuracy: 0.7471216917037964\n",
      "Batch 171 loss: 0.6658033132553101, accuracy: 0.7471384406089783\n",
      "Batch 172 loss: 0.6524419784545898, accuracy: 0.7470646500587463\n",
      "Batch 173 loss: 0.7956677079200745, accuracy: 0.7471713423728943\n",
      "Batch 174 loss: 0.7978782653808594, accuracy: 0.7470982074737549\n",
      "Batch 175 loss: 0.8546318411827087, accuracy: 0.7467595934867859\n",
      "Batch 176 loss: 0.7186347246170044, accuracy: 0.7468220591545105\n",
      "Batch 177 loss: 0.6480746269226074, accuracy: 0.7466643452644348\n",
      "Batch 178 loss: 0.7819285988807678, accuracy: 0.7466393113136292\n",
      "Batch 179 loss: 0.631845235824585, accuracy: 0.7467882037162781\n",
      "Batch 180 loss: 0.8650749921798706, accuracy: 0.7464606165885925\n",
      "Batch 181 loss: 0.7754221558570862, accuracy: 0.7461795806884766\n",
      "Batch 182 loss: 0.6513956785202026, accuracy: 0.7461577653884888\n",
      "Batch 183 loss: 0.654018223285675, accuracy: 0.7462635636329651\n",
      "Batch 184 loss: 0.6591539978981018, accuracy: 0.7463260293006897\n",
      "Batch 185 loss: 0.6694830656051636, accuracy: 0.746387779712677\n",
      "Batch 186 loss: 0.7600867748260498, accuracy: 0.746239960193634\n",
      "Batch 187 loss: 0.668297529220581, accuracy: 0.7461768388748169\n",
      "Batch 188 loss: 0.564237117767334, accuracy: 0.7464037537574768\n",
      "Batch 189 loss: 0.620954155921936, accuracy: 0.7466694116592407\n",
      "Batch 190 loss: 0.765676736831665, accuracy: 0.7467686533927917\n",
      "Batch 191 loss: 0.647835373878479, accuracy: 0.74658203125\n",
      "Batch 192 loss: 0.6595380306243896, accuracy: 0.7465187907218933\n",
      "Batch 193 loss: 0.6782801151275635, accuracy: 0.7467783689498901\n",
      "Batch 194 loss: 0.8650622963905334, accuracy: 0.7463541626930237\n",
      "Batch 195 loss: 0.6046653985977173, accuracy: 0.746492326259613\n",
      "Batch 196 loss: 0.704658031463623, accuracy: 0.7463911771774292\n",
      "Batch 197 loss: 0.6122697591781616, accuracy: 0.7465672492980957\n",
      "Batch 198 loss: 0.5455701351165771, accuracy: 0.7467807531356812\n",
      "Batch 199 loss: 0.7184053063392639, accuracy: 0.746874988079071\n",
      "Batch 200 loss: 0.6573632955551147, accuracy: 0.7467350959777832\n",
      "Batch 201 loss: 0.6059324741363525, accuracy: 0.7469832897186279\n",
      "Batch 202 loss: 0.6540059447288513, accuracy: 0.7468826770782471\n",
      "Batch 203 loss: 0.8157397508621216, accuracy: 0.7466682195663452\n",
      "Batch 204 loss: 0.7261317372322083, accuracy: 0.7467225790023804\n",
      "Batch 205 loss: 0.8082535862922668, accuracy: 0.7463971376419067\n",
      "Batch 206 loss: 0.6847314834594727, accuracy: 0.7464145421981812\n",
      "Batch 207 loss: 0.8151423335075378, accuracy: 0.746168851852417\n",
      "Batch 208 loss: 0.7777615189552307, accuracy: 0.7459255456924438\n",
      "Batch 209 loss: 0.628831684589386, accuracy: 0.7458705306053162\n",
      "Batch 210 loss: 0.6729153990745544, accuracy: 0.7459271550178528\n",
      "Batch 211 loss: 0.596342146396637, accuracy: 0.7461305856704712\n",
      "Batch 212 loss: 0.8945473432540894, accuracy: 0.7457453012466431\n",
      "Batch 213 loss: 0.6105875372886658, accuracy: 0.7459477186203003\n",
      "Batch 214 loss: 0.6723078489303589, accuracy: 0.7459665536880493\n",
      "Batch 215 loss: 0.748515248298645, accuracy: 0.7460213899612427\n",
      "Batch 216 loss: 0.6645320653915405, accuracy: 0.7460397481918335\n",
      "Batch 217 loss: 0.5846195816993713, accuracy: 0.74612957239151\n",
      "Batch 218 loss: 0.7088961601257324, accuracy: 0.7459332346916199\n",
      "Batch 219 loss: 0.6129323244094849, accuracy: 0.7462357878684998\n",
      "Batch 220 loss: 0.7515923380851746, accuracy: 0.7461114525794983\n",
      "Batch 221 loss: 0.6529670357704163, accuracy: 0.74609375\n",
      "Batch 222 loss: 0.8721994161605835, accuracy: 0.7454806566238403\n",
      "Batch 223 loss: 0.6428492665290833, accuracy: 0.7456752061843872\n",
      "Batch 224 loss: 0.7155387997627258, accuracy: 0.7456198334693909\n",
      "Training epoch: 40, train accuracy: 74.56198120117188, train loss: 0.6837731986575657, valid accuracy: 64.19615173339844, valid loss: 1.1166691636217052 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General training complete!\n"
     ]
    }
   ],
   "source": [
    "# WARNING: this may take quite some time \n",
    "p.train_general_model(\"data/general_model.pt\", learning_rate=1e-3, n_epochs=40, stop_thr=1e-4, use_valid=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to atulapra, we were able to achieve roughly 60-65% validation accuracy. Now, if we are to evaluate the model on the test dataset, we get the following metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'fer13_ef'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e62fd80d18a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/general_model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_general_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Synced/2021_Winter_Classes/EE 435/final_project/iExpressionNet/src/pipeline.py\u001b[0m in \u001b[0;36mevaluate_general_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_general_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# initialize dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFER2013\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PrivateTest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivateTest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfer13_ef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivateTest_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfer13_el\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# get test loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'fer13_ef'"
     ]
    }
   ],
   "source": [
    "p.load_model(\"data/general_model.pt\", mode=\"test\")\n",
    "p.evaluate_general_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our general model has now been trained. The output above shows that the test accuracy of our general model trained on FEER13 is around 63%. We will next move on to optimizing the model for a specific user.\n",
    "## Specific Model\n",
    "We will first need to obtain the user-specific dataset. To do so, we must ingest a user's specific data through a webcam. You can change the number of samples for each emotion and the dataset save location changing the first and second parameters respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make and hold a/an angry expression.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1b06cdd31d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_specific_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/specific_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Synced/2021_Winter_Classes/EE 435/final_project/iExpressionNet/src/pipeline.py\u001b[0m in \u001b[0;36msample_specific_data\u001b[0;34m(self, n_samples, path, prefix)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please make and hold a/an {} expression.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start sampling in 3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p.sample_specific_data(100, \"data/specific_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've collected a small dataset from our user, we can now transfer our general model and train a specific model on the new tiny dataset. But before we do that, let us use the general model from before to test and see the test accuracy of the general model on the user-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9198252830305137, test accuracy: 72.85713958740234\n",
      "Confusion matrix:\n",
      "[[ 6  0  2  2  0  0  0]\n",
      " [ 0  0  0  0  9  1  0]\n",
      " [ 0  0  5  0  0  2  3]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [0.75       0.         0.58823529 0.90909091 0.68965517 0.86956522\n",
      " 0.86956522]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9198252830305137,\n",
       " tensor(72.8571),\n",
       " array([[ 6,  0,  2,  2,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  9,  1,  0],\n",
       "        [ 0,  0,  5,  0,  0,  2,  3],\n",
       "        [ 0,  0,  0, 10,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 10,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 10,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0, 10]]),\n",
       " array([0.75      , 0.        , 0.58823529, 0.90909091, 0.68965517,\n",
       "        0.86956522, 0.86956522]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.load_model(\"data/general_model.pt\", mode=\"test\")\n",
    "p.evaluate_specific_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how exaggerated your expressions were in the user-specific dataset, you can see test accuracies of between 30% to 70%, which is quite a large range. However, we can hopefully improve the accuracy by freezing the convolutional layers and training only the fully-connected layers on the user-specific dataset. You can tune the parameters as you like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 loss: 2.12283992767334, accuracy: 0.4375\n",
      "Batch 1 loss: 1.4868049621582031, accuracy: 0.5\n",
      "Batch 2 loss: 1.0758464336395264, accuracy: 0.53125\n",
      "Batch 3 loss: 1.0222960710525513, accuracy: 0.5625\n",
      "Batch 4 loss: 1.2522144317626953, accuracy: 0.5562499761581421\n",
      "Batch 5 loss: 0.865405261516571, accuracy: 0.578125\n",
      "Batch 6 loss: 0.7112964987754822, accuracy: 0.5892857313156128\n",
      "Batch 7 loss: 0.6685075759887695, accuracy: 0.61328125\n",
      "Batch 8 loss: 0.6280157566070557, accuracy: 0.6319444179534912\n",
      "Batch 9 loss: 0.3177119493484497, accuracy: 0.653124988079071\n",
      "Batch 10 loss: 0.46544164419174194, accuracy: 0.6676136255264282\n",
      "Batch 11 loss: 0.4321470260620117, accuracy: 0.6848958134651184\n",
      "Batch 12 loss: 0.4478530287742615, accuracy: 0.6947115659713745\n",
      "Batch 13 loss: 0.4284808337688446, accuracy: 0.703125\n",
      "Batch 14 loss: 0.34338513016700745, accuracy: 0.7166666388511658\n",
      "Batch 15 loss: 0.17630554735660553, accuracy: 0.734375\n",
      "Batch 16 loss: 0.16502293944358826, accuracy: 0.748161792755127\n",
      "Batch 17 loss: 0.1492702215909958, accuracy: 0.7535714507102966\n",
      "Training epoch: 1, train accuracy: 75.35713958740234, train loss: 0.7088247355487611, valid accuracy: 95.71428680419922, valid loss: 0.09082202054560184 \n",
      "Batch 0 loss: 0.1760646104812622, accuracy: 0.90625\n",
      "Batch 1 loss: 0.17672647535800934, accuracy: 0.9375\n",
      "Batch 2 loss: 0.1799270659685135, accuracy: 0.9479166865348816\n",
      "Batch 3 loss: 0.10342832654714584, accuracy: 0.9609375\n",
      "Batch 4 loss: 0.12499573826789856, accuracy: 0.9624999761581421\n",
      "Batch 5 loss: 0.15363267064094543, accuracy: 0.9583333134651184\n",
      "Batch 6 loss: 0.18593730032444, accuracy: 0.9508928656578064\n",
      "Batch 7 loss: 0.17399145662784576, accuracy: 0.953125\n",
      "Batch 8 loss: 0.1494162231683731, accuracy: 0.9513888955116272\n",
      "Batch 9 loss: 0.09360412508249283, accuracy: 0.953125\n",
      "Batch 10 loss: 0.0956699326634407, accuracy: 0.9545454382896423\n",
      "Batch 11 loss: 0.05940064415335655, accuracy: 0.9583333134651184\n",
      "Batch 12 loss: 0.03535648435354233, accuracy: 0.9615384340286255\n",
      "Batch 13 loss: 0.12802453339099884, accuracy: 0.9620535969734192\n",
      "Batch 14 loss: 0.16643323004245758, accuracy: 0.9624999761581421\n",
      "Batch 15 loss: 0.15866601467132568, accuracy: 0.962890625\n",
      "Batch 16 loss: 0.07598056644201279, accuracy: 0.9632353186607361\n",
      "Batch 17 loss: 0.0881335437297821, accuracy: 0.9624999761581421\n",
      "Training epoch: 2, train accuracy: 96.25, train loss: 0.12918827455076906, valid accuracy: 100.0, valid loss: 0.014934291442235311 \n",
      "Batch 0 loss: 0.020528972148895264, accuracy: 1.0\n",
      "Batch 1 loss: 0.021448520943522453, accuracy: 1.0\n",
      "Batch 2 loss: 0.056041181087493896, accuracy: 1.0\n",
      "Batch 3 loss: 0.08473801612854004, accuracy: 0.984375\n",
      "Batch 4 loss: 0.07763662189245224, accuracy: 0.981249988079071\n",
      "Batch 5 loss: 0.10188116133213043, accuracy: 0.9739583134651184\n",
      "Batch 6 loss: 0.03556738793849945, accuracy: 0.9776785969734192\n",
      "Batch 7 loss: 0.13815809786319733, accuracy: 0.97265625\n",
      "Batch 8 loss: 0.051863133907318115, accuracy: 0.9756944179534912\n",
      "Batch 9 loss: 0.025269415229558945, accuracy: 0.9781249761581421\n",
      "Batch 10 loss: 0.05023704841732979, accuracy: 0.9801136255264282\n",
      "Batch 11 loss: 0.10610268265008926, accuracy: 0.9791666865348816\n",
      "Batch 12 loss: 0.05326634645462036, accuracy: 0.9783653616905212\n",
      "Batch 13 loss: 0.02343077026307583, accuracy: 0.9799107313156128\n",
      "Batch 14 loss: 0.05266740918159485, accuracy: 0.981249988079071\n",
      "Batch 15 loss: 0.0345187671482563, accuracy: 0.982421875\n",
      "Batch 16 loss: 0.02506505697965622, accuracy: 0.9834558963775635\n",
      "Batch 17 loss: 0.008119911886751652, accuracy: 0.9839285612106323\n",
      "Training epoch: 3, train accuracy: 98.39286041259766, train loss: 0.05369669452516569, valid accuracy: 100.0, valid loss: 0.005525080404671219 \n",
      "Batch 0 loss: 0.04859029874205589, accuracy: 0.96875\n",
      "Batch 1 loss: 0.06704962998628616, accuracy: 0.96875\n",
      "Batch 2 loss: 0.035918667912483215, accuracy: 0.9791666865348816\n",
      "Batch 3 loss: 0.054505396634340286, accuracy: 0.9765625\n",
      "Batch 4 loss: 0.017015090212225914, accuracy: 0.981249988079071\n",
      "Batch 5 loss: 0.011020572856068611, accuracy: 0.984375\n",
      "Batch 6 loss: 0.009368953295052052, accuracy: 0.9866071343421936\n",
      "Batch 7 loss: 0.042758725583553314, accuracy: 0.98828125\n",
      "Batch 8 loss: 0.0024942525196820498, accuracy: 0.9895833134651184\n",
      "Batch 9 loss: 0.029350070282816887, accuracy: 0.987500011920929\n",
      "Batch 10 loss: 0.07862482964992523, accuracy: 0.9857954382896423\n",
      "Batch 11 loss: 0.025744758546352386, accuracy: 0.9869791865348816\n",
      "Batch 12 loss: 0.040526922792196274, accuracy: 0.9855769276618958\n",
      "Batch 13 loss: 0.02679411880671978, accuracy: 0.9866071343421936\n",
      "Batch 14 loss: 0.033940285444259644, accuracy: 0.987500011920929\n",
      "Batch 15 loss: 0.014528607949614525, accuracy: 0.98828125\n",
      "Batch 16 loss: 0.0018034476088359952, accuracy: 0.9889705777168274\n",
      "Batch 17 loss: 0.007439774926751852, accuracy: 0.9892857074737549\n",
      "Training epoch: 4, train accuracy: 98.92857360839844, train loss: 0.030415244652734447, valid accuracy: 100.0, valid loss: 0.011525220160062114 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n"
     ]
    }
   ],
   "source": [
    "p.load_model(\"data/general_model.pt\", mode=\"train\")\n",
    "p.train_specific_model(\"data/specific_model.pt\", \n",
    "                       learning_rate=1e-3, \n",
    "                       n_epochs=4, \n",
    "                       stop_thr=1e-5, \n",
    "                       use_valid=True, \n",
    "                       batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by training for only a few epochs, we get near perfect train and validation accuracies. We can now evaluate the new model specifically trained for our user on the user-specific test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.004957175545849526, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.004957175545849526,\n",
       " tensor(100.),\n",
       " array([[10,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 10,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 10,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 10,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 10,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 10,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0, 10]]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.load_model(\"data/specific_model.pt\")\n",
    "p.evaluate_specific_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model performs decently as well on the test data. Now, it should be obvious that by training on a user-specific dataset the model will lose some generality and we expect the running the specific model on the general FER13 dataset will produce lower test accuracies and higher loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.15502933556606, test accuracy: 55.41933822631836\n",
      "Confusion matrix:\n",
      "[[260  17  20   9 117  10  34]\n",
      " [ 16  25   1   0  13   0   1]\n",
      " [ 99   9 117  11 190  35  35]\n",
      " [127   3  12 645  50  29  29]\n",
      " [ 87  17  35  25 409  10  70]\n",
      " [ 23   5  63  12  19 284   9]\n",
      " [ 66   7  37  44 201   3 249]]\n",
      "F1-score: [0.45414847 0.35971223 0.29961588 0.78610603 0.49515738 0.72264631\n",
      " 0.48162476]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.15502933556606,\n",
       " tensor(55.4193),\n",
       " array([[260,  17,  20,   9, 117,  10,  34],\n",
       "        [ 16,  25,   1,   0,  13,   0,   1],\n",
       "        [ 99,   9, 117,  11, 190,  35,  35],\n",
       "        [127,   3,  12, 645,  50,  29,  29],\n",
       "        [ 87,  17,  35,  25, 409,  10,  70],\n",
       "        [ 23,   5,  63,  12,  19, 284,   9],\n",
       "        [ 66,   7,  37,  44, 201,   3, 249]]),\n",
       " array([0.45414847, 0.35971223, 0.29961588, 0.78610603, 0.49515738,\n",
       "        0.72264631, 0.48162476]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.load_model(\"data/specific_model.pt\")\n",
    "p.evaluate_general_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we see that the test accuracy and loss has on the general test set has suffered, but not by that much. The pros of 90+ percent accuracy on user-specific dataset darastically outweights the few lost percents on the general FER13 dataset. Further analysis of this can be seen in Experiment 1. Lastly, we can run our new model on a webcam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video capture starting... Press 'q' to stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peizhiliu/Documents/Synced/2021_Winter_Classes/EE 435/final_project/iExpressionNet/src/face_detection.py:51: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return rects, np.array(crop)\n"
     ]
    }
   ],
   "source": [
    "p.load_model(\"data/specific_model.pt\")\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "## Experiment 1\n",
    "We have seen in the above above pipeline that as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 1.083616288812274, test accuracy: 64.28571319580078\n",
      "Confusion matrix:\n",
      "[[ 5  0  2  3  0  0  0]\n",
      " [ 0  0  0  0  9  1  0]\n",
      " [ 0  0  2  0  0  6  2]\n",
      " [ 0  0  0  8  0  0  2]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [0.66666667 0.         0.28571429 0.76190476 0.68965517 0.74074074\n",
      " 0.83333333]\n",
      "test loss: 1.1766090341153765, test accuracy: 62.914459228515625\n",
      "Confusion matrix:\n",
      "[[251   6  46  22  74  14  54]\n",
      " [ 16  27   3   2   5   1   2]\n",
      " [ 51   0 207  23 126  33  56]\n",
      " [ 17   2  16 748  34  27  51]\n",
      " [ 65   3  76  31 349  13 116]\n",
      " [ 13   0  47  17  15 304  19]\n",
      " [ 42   0  38  47 100   8 372]]\n",
      "F1-score: [0.54446855 0.57446809 0.44564047 0.83809524 0.51474926 0.74601227\n",
      " 0.58261551]\n",
      "Batch 0 loss: 2.180567741394043, accuracy: 0.34375\n",
      "Batch 1 loss: 1.342706561088562, accuracy: 0.46875\n",
      "Batch 2 loss: 1.5619491338729858, accuracy: 0.4166666567325592\n",
      "Batch 3 loss: 0.731502890586853, accuracy: 0.484375\n",
      "Batch 4 loss: 0.8146916031837463, accuracy: 0.5249999761581421\n",
      "Batch 5 loss: 1.0216437578201294, accuracy: 0.5677083134651184\n",
      "Batch 6 loss: 1.1025115251541138, accuracy: 0.5758928656578064\n",
      "Batch 7 loss: 0.6731545925140381, accuracy: 0.59375\n",
      "Batch 8 loss: 0.6021715998649597, accuracy: 0.625\n",
      "Batch 9 loss: 0.5843163728713989, accuracy: 0.6343749761581421\n",
      "Batch 10 loss: 0.5589060187339783, accuracy: 0.6505681872367859\n",
      "Batch 11 loss: 0.3267880082130432, accuracy: 0.671875\n",
      "Batch 12 loss: 0.28842902183532715, accuracy: 0.6850961446762085\n",
      "Batch 13 loss: 0.3915902376174927, accuracy: 0.6986607313156128\n",
      "Batch 14 loss: 0.4641869366168976, accuracy: 0.706250011920929\n",
      "Batch 15 loss: 0.23381371796131134, accuracy: 0.72265625\n",
      "Batch 16 loss: 0.13533666729927063, accuracy: 0.7371323704719543\n",
      "Batch 17 loss: 0.1830020397901535, accuracy: 0.7446428537368774\n",
      "Training epoch: 1, train accuracy: 74.46428680419922, train loss: 0.7331815792454613, valid accuracy: 98.57142639160156, valid loss: 0.055448610335588455 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 0.05994394483132055, test accuracy: 98.57142639160156\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0  9  0  0  1  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1.         1.         0.94736842 1.         1.         0.95238095\n",
      " 1.        ]\n",
      "test loss: 1.4682118031616713, test accuracy: 58.34494400024414\n",
      "Confusion matrix:\n",
      "[[283  17  12  13 112   7  23]\n",
      " [ 13  33   1   1   7   0   1]\n",
      " [ 91  13 134  15 190  30  23]\n",
      " [ 75   2   9 707  60  19  23]\n",
      " [ 88  15  33  31 433   6  47]\n",
      " [ 27   7  46  14  27 285   9]\n",
      " [ 81   7  24  54 217   5 219]]\n",
      "F1-score: [0.50311111 0.44       0.35496689 0.81734104 0.5097116  0.74315515\n",
      " 0.46008403]\n",
      "Batch 0 loss: 1.6055858135223389, accuracy: 0.46875\n",
      "Batch 1 loss: 1.3799139261245728, accuracy: 0.5\n",
      "Batch 2 loss: 1.4363709688186646, accuracy: 0.5104166865348816\n",
      "Batch 3 loss: 1.314817190170288, accuracy: 0.5234375\n",
      "Batch 4 loss: 1.0046004056930542, accuracy: 0.5562499761581421\n",
      "Batch 5 loss: 0.9015867114067078, accuracy: 0.578125\n",
      "Batch 6 loss: 0.7890048027038574, accuracy: 0.5982142686843872\n",
      "Batch 7 loss: 0.5998208522796631, accuracy: 0.625\n",
      "Batch 8 loss: 0.3988730013370514, accuracy: 0.6493055820465088\n",
      "Batch 9 loss: 0.4484882652759552, accuracy: 0.668749988079071\n",
      "Batch 10 loss: 0.4519388675689697, accuracy: 0.6818181872367859\n",
      "Batch 11 loss: 0.5885738134384155, accuracy: 0.6848958134651184\n",
      "Batch 12 loss: 0.43760180473327637, accuracy: 0.6971153616905212\n",
      "Batch 13 loss: 0.27691906690597534, accuracy: 0.7142857313156128\n",
      "Batch 14 loss: 0.21845808625221252, accuracy: 0.7291666865348816\n",
      "Batch 15 loss: 0.3246273100376129, accuracy: 0.73828125\n",
      "Batch 16 loss: 0.13963623344898224, accuracy: 0.751838207244873\n",
      "Batch 17 loss: 0.21371877193450928, accuracy: 0.7571428418159485\n",
      "Training epoch: 1, train accuracy: 75.71428680419922, train loss: 0.6961408828695616, valid accuracy: 98.57142639160156, valid loss: 0.05632231632868449 \n",
      "Batch 0 loss: 0.13718071579933167, accuracy: 0.96875\n",
      "Batch 1 loss: 0.3553628921508789, accuracy: 0.9375\n",
      "Batch 2 loss: 0.13134878873825073, accuracy: 0.9479166865348816\n",
      "Batch 3 loss: 0.16326868534088135, accuracy: 0.9453125\n",
      "Batch 4 loss: 0.21485635638237, accuracy: 0.9375\n",
      "Batch 5 loss: 0.3476804792881012, accuracy: 0.921875\n",
      "Batch 6 loss: 0.2511763572692871, accuracy: 0.9285714030265808\n",
      "Batch 7 loss: 0.1690942943096161, accuracy: 0.9296875\n",
      "Batch 8 loss: 0.1292935013771057, accuracy: 0.9305555820465088\n",
      "Batch 9 loss: 0.12656953930854797, accuracy: 0.9375\n",
      "Batch 10 loss: 0.07444921880960464, accuracy: 0.9431818127632141\n",
      "Batch 11 loss: 0.08424648642539978, accuracy: 0.9453125\n",
      "Batch 12 loss: 0.05029209330677986, accuracy: 0.9471153616905212\n",
      "Batch 13 loss: 0.0865851566195488, accuracy: 0.9486607313156128\n",
      "Batch 14 loss: 0.25694572925567627, accuracy: 0.9437500238418579\n",
      "Batch 15 loss: 0.0877111628651619, accuracy: 0.9453125\n",
      "Batch 16 loss: 0.05768166482448578, accuracy: 0.9466911554336548\n",
      "Batch 17 loss: 0.04996797442436218, accuracy: 0.9482142925262451\n",
      "Training epoch: 2, train accuracy: 94.82142639160156, train loss: 0.15409506091641056, valid accuracy: 100.0, valid loss: 0.06475528391698997 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 0.006029800639870635, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 1.708013275692089, test accuracy: 58.066314697265625\n",
      "Confusion matrix:\n",
      "[[254  14  11  15 138   8  27]\n",
      " [ 15  27   1   1  11   0   1]\n",
      " [ 69   6 124  14 223  38  22]\n",
      " [ 68   2   8 707  69  22  19]\n",
      " [ 64  10  38  27 453   8  53]\n",
      " [ 15   2  54  15  27 294   8]\n",
      " [ 48   5  30  57 238   4 225]]\n",
      "F1-score: [0.508      0.44262295 0.32545932 0.81686886 0.5        0.74524715\n",
      " 0.46777547]\n",
      "Batch 0 loss: 1.5546468496322632, accuracy: 0.59375\n",
      "Batch 1 loss: 1.4619059562683105, accuracy: 0.5625\n",
      "Batch 2 loss: 1.7859805822372437, accuracy: 0.5208333134651184\n",
      "Batch 3 loss: 0.7819513082504272, accuracy: 0.546875\n",
      "Batch 4 loss: 1.3691120147705078, accuracy: 0.550000011920929\n",
      "Batch 5 loss: 1.1365463733673096, accuracy: 0.5572916865348816\n",
      "Batch 6 loss: 0.8060336112976074, accuracy: 0.5669642686843872\n",
      "Batch 7 loss: 0.7232877016067505, accuracy: 0.5859375\n",
      "Batch 8 loss: 0.44594958424568176, accuracy: 0.6145833134651184\n",
      "Batch 9 loss: 0.6911880373954773, accuracy: 0.625\n",
      "Batch 10 loss: 0.37157100439071655, accuracy: 0.6420454382896423\n",
      "Batch 11 loss: 0.4164848029613495, accuracy: 0.6588541865348816\n",
      "Batch 12 loss: 0.5096308588981628, accuracy: 0.6730769276618958\n",
      "Batch 13 loss: 0.2939293384552002, accuracy: 0.6919642686843872\n",
      "Batch 14 loss: 0.24287013709545135, accuracy: 0.7083333134651184\n",
      "Batch 15 loss: 0.22842952609062195, accuracy: 0.724609375\n",
      "Batch 16 loss: 0.25893139839172363, accuracy: 0.7334558963775635\n",
      "Batch 17 loss: 0.5228347182273865, accuracy: 0.7339285612106323\n",
      "Training epoch: 1, train accuracy: 73.39286041259766, train loss: 0.7556268779767884, valid accuracy: 100.0, valid loss: 0.06485114246606827 \n",
      "Batch 0 loss: 0.2185187190771103, accuracy: 0.90625\n",
      "Batch 1 loss: 0.2749989926815033, accuracy: 0.890625\n",
      "Batch 2 loss: 0.25979524850845337, accuracy: 0.8854166865348816\n",
      "Batch 3 loss: 0.21882058680057526, accuracy: 0.8984375\n",
      "Batch 4 loss: 0.1301233470439911, accuracy: 0.90625\n",
      "Batch 5 loss: 0.16914772987365723, accuracy: 0.9166666865348816\n",
      "Batch 6 loss: 0.09837451577186584, accuracy: 0.9285714030265808\n",
      "Batch 7 loss: 0.14563608169555664, accuracy: 0.9296875\n",
      "Batch 8 loss: 0.21016523241996765, accuracy: 0.9305555820465088\n",
      "Batch 9 loss: 0.1653614491224289, accuracy: 0.934374988079071\n",
      "Batch 10 loss: 0.11622067540884018, accuracy: 0.9403409361839294\n",
      "Batch 11 loss: 0.0967424139380455, accuracy: 0.9427083134651184\n",
      "Batch 12 loss: 0.037533897906541824, accuracy: 0.9471153616905212\n",
      "Batch 13 loss: 0.06683097779750824, accuracy: 0.9508928656578064\n",
      "Batch 14 loss: 0.060781944543123245, accuracy: 0.9541666507720947\n",
      "Batch 15 loss: 0.08580663800239563, accuracy: 0.955078125\n",
      "Batch 16 loss: 0.07690171897411346, accuracy: 0.9558823704719543\n",
      "Batch 17 loss: 0.0894317477941513, accuracy: 0.9571428298950195\n",
      "Training epoch: 2, train accuracy: 95.71428680419922, train loss: 0.1400662176311016, valid accuracy: 98.57142639160156, valid loss: 0.021870779106393456 \n",
      "Batch 0 loss: 0.13481271266937256, accuracy: 0.90625\n",
      "Batch 1 loss: 0.14083236455917358, accuracy: 0.921875\n",
      "Batch 2 loss: 0.10726551711559296, accuracy: 0.9375\n",
      "Batch 3 loss: 0.04592517018318176, accuracy: 0.953125\n",
      "Batch 4 loss: 0.045784350484609604, accuracy: 0.9624999761581421\n",
      "Batch 5 loss: 0.062443625181913376, accuracy: 0.9635416865348816\n",
      "Batch 6 loss: 0.057479843497276306, accuracy: 0.96875\n",
      "Batch 7 loss: 0.02560507506132126, accuracy: 0.97265625\n",
      "Batch 8 loss: 0.11321916431188583, accuracy: 0.96875\n",
      "Batch 9 loss: 0.041679468005895615, accuracy: 0.971875011920929\n",
      "Batch 10 loss: 0.06270892173051834, accuracy: 0.9715909361839294\n",
      "Batch 11 loss: 0.023304564878344536, accuracy: 0.9739583134651184\n",
      "Batch 12 loss: 0.01132771372795105, accuracy: 0.9759615659713745\n",
      "Batch 13 loss: 0.02719280682504177, accuracy: 0.9776785969734192\n",
      "Batch 14 loss: 0.04450748488306999, accuracy: 0.9791666865348816\n",
      "Batch 15 loss: 0.08060299605131149, accuracy: 0.978515625\n",
      "Batch 16 loss: 0.04282113537192345, accuracy: 0.9797794222831726\n",
      "Batch 17 loss: 0.01201542466878891, accuracy: 0.9803571701049805\n",
      "Training epoch: 3, train accuracy: 98.03571319580078, train loss: 0.05997379662262069, valid accuracy: 100.0, valid loss: 0.01813028131922086 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 0.0027729452055414833, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 1.9633087830054567, test accuracy: 56.89607238769531\n",
      "Confusion matrix:\n",
      "[[227   7  15  18 161  11  28]\n",
      " [ 13  22   2   1  17   0   1]\n",
      " [ 59   3  93  21 248  45  27]\n",
      " [ 60   1   4 720  64  25  21]\n",
      " [ 40   6  28  33 474  10  62]\n",
      " [ 13   0  40  18  32 301  11]\n",
      " [ 32   4  26  69 265   6 205]]\n",
      "F1-score: [0.49835346 0.44444444 0.26420455 0.81126761 0.49529781 0.7404674\n",
      " 0.42619543]\n",
      "Batch 0 loss: 1.5452032089233398, accuracy: 0.5\n",
      "Batch 1 loss: 2.1123507022857666, accuracy: 0.46875\n",
      "Batch 2 loss: 0.6150090098381042, accuracy: 0.5729166865348816\n",
      "Batch 3 loss: 1.3122875690460205, accuracy: 0.5546875\n",
      "Batch 4 loss: 1.2311367988586426, accuracy: 0.5562499761581421\n",
      "Batch 5 loss: 0.9452255964279175, accuracy: 0.578125\n",
      "Batch 6 loss: 0.8599823117256165, accuracy: 0.5848214030265808\n",
      "Batch 7 loss: 0.5071231126785278, accuracy: 0.6171875\n",
      "Batch 8 loss: 0.6224178671836853, accuracy: 0.6354166865348816\n",
      "Batch 9 loss: 0.48269447684288025, accuracy: 0.65625\n",
      "Batch 10 loss: 0.42432543635368347, accuracy: 0.6732954382896423\n",
      "Batch 11 loss: 0.4292460083961487, accuracy: 0.6875\n",
      "Batch 12 loss: 0.3528725206851959, accuracy: 0.7019230723381042\n",
      "Batch 13 loss: 0.25383785367012024, accuracy: 0.71875\n",
      "Batch 14 loss: 0.23002013564109802, accuracy: 0.731249988079071\n",
      "Batch 15 loss: 0.6223254799842834, accuracy: 0.73828125\n",
      "Batch 16 loss: 0.256930947303772, accuracy: 0.7463235259056091\n",
      "Batch 17 loss: 0.15164296329021454, accuracy: 0.7535714507102966\n",
      "Training epoch: 1, train accuracy: 75.35713958740234, train loss: 0.7197017777297232, valid accuracy: 98.57142639160156, valid loss: 0.07954210291306178 \n",
      "Batch 0 loss: 0.2134382426738739, accuracy: 0.96875\n",
      "Batch 1 loss: 0.19921448826789856, accuracy: 0.9375\n",
      "Batch 2 loss: 0.2759040296077728, accuracy: 0.9166666865348816\n",
      "Batch 3 loss: 0.22725149989128113, accuracy: 0.921875\n",
      "Batch 4 loss: 0.13635437190532684, accuracy: 0.9312499761581421\n",
      "Batch 5 loss: 0.18959744274616241, accuracy: 0.9270833134651184\n",
      "Batch 6 loss: 0.1403094232082367, accuracy: 0.9330357313156128\n",
      "Batch 7 loss: 0.11588471382856369, accuracy: 0.94140625\n",
      "Batch 8 loss: 0.13822807371616364, accuracy: 0.9444444179534912\n",
      "Batch 9 loss: 0.14227943122386932, accuracy: 0.9437500238418579\n",
      "Batch 10 loss: 0.1168477013707161, accuracy: 0.9460227489471436\n",
      "Batch 11 loss: 0.0772937759757042, accuracy: 0.9505208134651184\n",
      "Batch 12 loss: 0.044427014887332916, accuracy: 0.9543269276618958\n",
      "Batch 13 loss: 0.050787437707185745, accuracy: 0.9575892686843872\n",
      "Batch 14 loss: 0.052385762333869934, accuracy: 0.9604166746139526\n",
      "Batch 15 loss: 0.05261120945215225, accuracy: 0.9609375\n",
      "Batch 16 loss: 0.12895901501178741, accuracy: 0.9595588445663452\n",
      "Batch 17 loss: 0.09381523728370667, accuracy: 0.9589285850524902\n",
      "Training epoch: 2, train accuracy: 95.89286041259766, train loss: 0.13308827061620024, valid accuracy: 95.71428680419922, valid loss: 0.06141201468805472 \n",
      "Batch 0 loss: 0.10146144032478333, accuracy: 0.96875\n",
      "Batch 1 loss: 0.1074058786034584, accuracy: 0.953125\n",
      "Batch 2 loss: 0.05663774162530899, accuracy: 0.96875\n",
      "Batch 3 loss: 0.05973608419299126, accuracy: 0.96875\n",
      "Batch 4 loss: 0.03299154341220856, accuracy: 0.9750000238418579\n",
      "Batch 5 loss: 0.07650800049304962, accuracy: 0.9739583134651184\n",
      "Batch 6 loss: 0.07817079871892929, accuracy: 0.9732142686843872\n",
      "Batch 7 loss: 0.048036858439445496, accuracy: 0.9765625\n",
      "Batch 8 loss: 0.08918710052967072, accuracy: 0.9756944179534912\n",
      "Batch 9 loss: 0.02466742694377899, accuracy: 0.9781249761581421\n",
      "Batch 10 loss: 0.0199308842420578, accuracy: 0.9801136255264282\n",
      "Batch 11 loss: 0.03161250799894333, accuracy: 0.9817708134651184\n",
      "Batch 12 loss: 0.045479681342840195, accuracy: 0.9807692170143127\n",
      "Batch 13 loss: 0.013952706940472126, accuracy: 0.9821428656578064\n",
      "Batch 14 loss: 0.02600334957242012, accuracy: 0.9833333492279053\n",
      "Batch 15 loss: 0.03879011049866676, accuracy: 0.984375\n",
      "Batch 16 loss: 0.014437341131269932, accuracy: 0.9852941036224365\n",
      "Batch 17 loss: 0.010657526552677155, accuracy: 0.9857142567634583\n",
      "Training epoch: 3, train accuracy: 98.57142639160156, train loss: 0.04864816564238734, valid accuracy: 100.0, valid loss: 0.0069714662070812965 \n",
      "Batch 0 loss: 0.07697917520999908, accuracy: 0.96875\n",
      "Batch 1 loss: 0.021102743223309517, accuracy: 0.984375\n",
      "Batch 2 loss: 0.008543258532881737, accuracy: 0.9895833134651184\n",
      "Batch 3 loss: 0.012317653745412827, accuracy: 0.9921875\n",
      "Batch 4 loss: 0.08160443603992462, accuracy: 0.981249988079071\n",
      "Batch 5 loss: 0.05284543335437775, accuracy: 0.9791666865348816\n",
      "Batch 6 loss: 0.007315456867218018, accuracy: 0.9821428656578064\n",
      "Batch 7 loss: 0.0028635647613555193, accuracy: 0.984375\n",
      "Batch 8 loss: 0.013788255862891674, accuracy: 0.9861111044883728\n",
      "Batch 9 loss: 0.008296388201415539, accuracy: 0.987500011920929\n",
      "Batch 10 loss: 0.013852812349796295, accuracy: 0.9886363744735718\n",
      "Batch 11 loss: 0.030451355502009392, accuracy: 0.9895833134651184\n",
      "Batch 12 loss: 0.025681888684630394, accuracy: 0.9903846383094788\n",
      "Batch 13 loss: 0.003916461952030659, accuracy: 0.9910714030265808\n",
      "Batch 14 loss: 0.06148095801472664, accuracy: 0.9895833134651184\n",
      "Batch 15 loss: 0.01255718618631363, accuracy: 0.990234375\n",
      "Batch 16 loss: 0.0013365180930122733, accuracy: 0.9908088445663452\n",
      "Batch 17 loss: 0.03203548863530159, accuracy: 0.9910714030265808\n",
      "Training epoch: 4, train accuracy: 99.10713958740234, train loss: 0.0259427241787004, valid accuracy: 100.0, valid loss: 0.004075424900899331 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 0.0030652345716508274, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 2.1062955437484483, test accuracy: 55.78155517578125\n",
      "Confusion matrix:\n",
      "[[223  20  20   7 156  14  27]\n",
      " [  9  28   2   0  16   0   1]\n",
      " [ 64  10 109  11 233  45  24]\n",
      " [ 91   3  10 658  67  30  36]\n",
      " [ 44  13  36  18 467  10  65]\n",
      " [ 12   5  60  10  27 297   4]\n",
      " [ 34   7  38  40 261   7 220]]\n",
      "F1-score: [0.47245763 0.3943662  0.28274968 0.80292862 0.49680851 0.72616137\n",
      " 0.44715447]\n",
      "Batch 0 loss: 1.792549967765808, accuracy: 0.5625\n",
      "Batch 1 loss: 1.2897896766662598, accuracy: 0.53125\n",
      "Batch 2 loss: 1.7244749069213867, accuracy: 0.4895833432674408\n",
      "Batch 3 loss: 1.077372670173645, accuracy: 0.53125\n",
      "Batch 4 loss: 0.9827603101730347, accuracy: 0.5375000238418579\n",
      "Batch 5 loss: 0.5270602107048035, accuracy: 0.59375\n",
      "Batch 6 loss: 0.6696388721466064, accuracy: 0.6294642686843872\n",
      "Batch 7 loss: 0.928210973739624, accuracy: 0.64453125\n",
      "Batch 8 loss: 0.5391293168067932, accuracy: 0.6701388955116272\n",
      "Batch 9 loss: 0.48334836959838867, accuracy: 0.684374988079071\n",
      "Batch 10 loss: 0.40399977564811707, accuracy: 0.7017045617103577\n",
      "Batch 11 loss: 0.3226200342178345, accuracy: 0.71875\n",
      "Batch 12 loss: 0.3959442377090454, accuracy: 0.7307692170143127\n",
      "Batch 13 loss: 0.4886823296546936, accuracy: 0.7388392686843872\n",
      "Batch 14 loss: 0.44463062286376953, accuracy: 0.7479166388511658\n",
      "Batch 15 loss: 0.2545502185821533, accuracy: 0.7578125\n",
      "Batch 16 loss: 0.1773313283920288, accuracy: 0.7702205777168274\n",
      "Batch 17 loss: 0.10418969392776489, accuracy: 0.7767857313156128\n",
      "Training epoch: 1, train accuracy: 77.67857360839844, train loss: 0.7003490842050977, valid accuracy: 100.0, valid loss: 0.04428670865794023 \n",
      "Batch 0 loss: 0.16559728980064392, accuracy: 0.96875\n",
      "Batch 1 loss: 0.11920766532421112, accuracy: 0.984375\n",
      "Batch 2 loss: 0.2924026846885681, accuracy: 0.9583333134651184\n",
      "Batch 3 loss: 0.11226735264062881, accuracy: 0.96875\n",
      "Batch 4 loss: 0.07824339717626572, accuracy: 0.9750000238418579\n",
      "Batch 5 loss: 0.2286076843738556, accuracy: 0.96875\n",
      "Batch 6 loss: 0.173356294631958, accuracy: 0.9642857313156128\n",
      "Batch 7 loss: 0.11702605336904526, accuracy: 0.96875\n",
      "Batch 8 loss: 0.061927251517772675, accuracy: 0.9722222089767456\n",
      "Batch 9 loss: 0.20480652153491974, accuracy: 0.96875\n",
      "Batch 10 loss: 0.06073332205414772, accuracy: 0.9715909361839294\n",
      "Batch 11 loss: 0.09681744128465652, accuracy: 0.9713541865348816\n",
      "Batch 12 loss: 0.09194512665271759, accuracy: 0.9711538553237915\n",
      "Batch 13 loss: 0.12332343310117722, accuracy: 0.9709821343421936\n",
      "Batch 14 loss: 0.06441564857959747, accuracy: 0.9729166626930237\n",
      "Batch 15 loss: 0.04507580026984215, accuracy: 0.974609375\n",
      "Batch 16 loss: 0.06591026484966278, accuracy: 0.9761029481887817\n",
      "Batch 17 loss: 0.05570881441235542, accuracy: 0.9767857193946838\n",
      "Training epoch: 2, train accuracy: 97.67857360839844, train loss: 0.11985400257011254, valid accuracy: 100.0, valid loss: 0.011735365726053715 \n",
      "Batch 0 loss: 0.022900773212313652, accuracy: 1.0\n",
      "Batch 1 loss: 0.09139469265937805, accuracy: 0.984375\n",
      "Batch 2 loss: 0.018424684181809425, accuracy: 0.9895833134651184\n",
      "Batch 3 loss: 0.02954256907105446, accuracy: 0.9921875\n",
      "Batch 4 loss: 0.04450501129031181, accuracy: 0.987500011920929\n",
      "Batch 5 loss: 0.05304119363427162, accuracy: 0.984375\n",
      "Batch 6 loss: 0.05749120935797691, accuracy: 0.9821428656578064\n",
      "Batch 7 loss: 0.06695154309272766, accuracy: 0.984375\n",
      "Batch 8 loss: 0.049004778265953064, accuracy: 0.9826388955116272\n",
      "Batch 9 loss: 0.07319945842027664, accuracy: 0.981249988079071\n",
      "Batch 10 loss: 0.06914863735437393, accuracy: 0.9801136255264282\n",
      "Batch 11 loss: 0.03133925795555115, accuracy: 0.9817708134651184\n",
      "Batch 12 loss: 0.07642090320587158, accuracy: 0.9807692170143127\n",
      "Batch 13 loss: 0.020540717989206314, accuracy: 0.9821428656578064\n",
      "Batch 14 loss: 0.0406670905649662, accuracy: 0.9833333492279053\n",
      "Batch 15 loss: 0.049401625990867615, accuracy: 0.982421875\n",
      "Batch 16 loss: 0.04021375998854637, accuracy: 0.9816176295280457\n",
      "Batch 17 loss: 0.024785473942756653, accuracy: 0.9821428656578064\n",
      "Training epoch: 3, train accuracy: 98.21428680419922, train loss: 0.04772074334323406, valid accuracy: 98.57142639160156, valid loss: 0.045074863669772945 \n",
      "Batch 0 loss: 0.03131488710641861, accuracy: 1.0\n",
      "Batch 1 loss: 0.07814785093069077, accuracy: 0.96875\n",
      "Batch 2 loss: 0.03561237081885338, accuracy: 0.96875\n",
      "Batch 3 loss: 0.02075740322470665, accuracy: 0.9765625\n",
      "Batch 4 loss: 0.0427124910056591, accuracy: 0.981249988079071\n",
      "Batch 5 loss: 0.013377933762967587, accuracy: 0.984375\n",
      "Batch 6 loss: 0.013351277448236942, accuracy: 0.9866071343421936\n",
      "Batch 7 loss: 0.0083558838814497, accuracy: 0.98828125\n",
      "Batch 8 loss: 0.02232472226023674, accuracy: 0.9895833134651184\n",
      "Batch 9 loss: 0.019938530400395393, accuracy: 0.9906250238418579\n",
      "Batch 10 loss: 0.009043795987963676, accuracy: 0.9914772510528564\n",
      "Batch 11 loss: 0.020773325115442276, accuracy: 0.9921875\n",
      "Batch 12 loss: 0.050340570509433746, accuracy: 0.9903846383094788\n",
      "Batch 13 loss: 0.015818165615200996, accuracy: 0.9910714030265808\n",
      "Batch 14 loss: 0.004593086428940296, accuracy: 0.9916666746139526\n",
      "Batch 15 loss: 0.019055508077144623, accuracy: 0.9921875\n",
      "Batch 16 loss: 0.02448083460330963, accuracy: 0.9926470518112183\n",
      "Batch 17 loss: 0.013246054761111736, accuracy: 0.9928571581840515\n",
      "Training epoch: 4, train accuracy: 99.28571319580078, train loss: 0.024624705107675657, valid accuracy: 100.0, valid loss: 0.005112194688990712 \n",
      "Batch 0 loss: 0.0060529280453920364, accuracy: 1.0\n",
      "Batch 1 loss: 0.01039957907050848, accuracy: 1.0\n",
      "Batch 2 loss: 0.03718572109937668, accuracy: 1.0\n",
      "Batch 3 loss: 0.006555063184350729, accuracy: 1.0\n",
      "Batch 4 loss: 0.028049573302268982, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.016950547695159912, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.01222817413508892, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.00511580053716898, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.006153895054012537, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.00826952513307333, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.011114676482975483, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.00932712759822607, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.1065836027264595, accuracy: 0.995192289352417\n",
      "Batch 13 loss: 0.006763987708836794, accuracy: 0.9955357313156128\n",
      "Batch 14 loss: 0.003199265105649829, accuracy: 0.9958333373069763\n",
      "Batch 15 loss: 0.003033856162801385, accuracy: 0.99609375\n",
      "Batch 16 loss: 0.002772358013316989, accuracy: 0.9963235259056091\n",
      "Batch 17 loss: 0.002187904668971896, accuracy: 0.9964285492897034\n",
      "Training epoch: 5, train accuracy: 99.64286041259766, train loss: 0.01566353254020214, valid accuracy: 100.0, valid loss: 0.002730749857922395 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 0.0018803414459315248, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 2.357287938324168, test accuracy: 55.307884216308594\n",
      "Confusion matrix:\n",
      "[[226  19  16  13 145  21  27]\n",
      " [ 13  21   0   1  19   2   0]\n",
      " [ 66  18  81  18 228  62  23]\n",
      " [ 78   1   7 685  67  35  22]\n",
      " [ 56  15  28  28 462  16  48]\n",
      " [ 12   3  35  14  28 317   6]\n",
      " [ 45   6  33  63 258   9 193]]\n",
      "F1-score: [0.46936656 0.30215827 0.23275862 0.79790332 0.49677419 0.72291904\n",
      " 0.41684665]\n",
      "Batch 0 loss: 1.4980255365371704, accuracy: 0.46875\n",
      "Batch 1 loss: 1.9773390293121338, accuracy: 0.421875\n",
      "Batch 2 loss: 1.330773115158081, accuracy: 0.4895833432674408\n",
      "Batch 3 loss: 0.8711487054824829, accuracy: 0.53125\n",
      "Batch 4 loss: 1.3878984451293945, accuracy: 0.550000011920929\n",
      "Batch 5 loss: 1.1454452276229858, accuracy: 0.5520833134651184\n",
      "Batch 6 loss: 0.632560670375824, accuracy: 0.5758928656578064\n",
      "Batch 7 loss: 0.639147162437439, accuracy: 0.59765625\n",
      "Batch 8 loss: 0.6568248867988586, accuracy: 0.6180555820465088\n",
      "Batch 9 loss: 0.528292179107666, accuracy: 0.640625\n",
      "Batch 10 loss: 0.6295813322067261, accuracy: 0.6534090638160706\n",
      "Batch 11 loss: 0.5301885604858398, accuracy: 0.6666666865348816\n",
      "Batch 12 loss: 0.3550757169723511, accuracy: 0.6875\n",
      "Batch 13 loss: 0.1946941763162613, accuracy: 0.7098214030265808\n",
      "Batch 14 loss: 0.20976456999778748, accuracy: 0.7250000238418579\n",
      "Batch 15 loss: 0.18947337567806244, accuracy: 0.7421875\n",
      "Batch 16 loss: 0.14419850707054138, accuracy: 0.7555146813392639\n",
      "Batch 17 loss: 0.2567873001098633, accuracy: 0.7589285969734192\n",
      "Training epoch: 1, train accuracy: 75.89286041259766, train loss: 0.7320676942666372, valid accuracy: 100.0, valid loss: 0.05480329071482023 \n",
      "Batch 0 loss: 0.34591466188430786, accuracy: 0.84375\n",
      "Batch 1 loss: 0.20502440631389618, accuracy: 0.890625\n",
      "Batch 2 loss: 0.28567615151405334, accuracy: 0.90625\n",
      "Batch 3 loss: 0.124679796397686, accuracy: 0.9296875\n",
      "Batch 4 loss: 0.1137414425611496, accuracy: 0.9312499761581421\n",
      "Batch 5 loss: 0.12234285473823547, accuracy: 0.9427083134651184\n",
      "Batch 6 loss: 0.10047131776809692, accuracy: 0.9508928656578064\n",
      "Batch 7 loss: 0.15858694911003113, accuracy: 0.94921875\n",
      "Batch 8 loss: 0.06755204498767853, accuracy: 0.9548611044883728\n",
      "Batch 9 loss: 0.10783480107784271, accuracy: 0.956250011920929\n",
      "Batch 10 loss: 0.1412435621023178, accuracy: 0.9545454382896423\n",
      "Batch 11 loss: 0.11393513530492783, accuracy: 0.9557291865348816\n",
      "Batch 12 loss: 0.028638280928134918, accuracy: 0.9591346383094788\n",
      "Batch 13 loss: 0.10455440729856491, accuracy: 0.9598214030265808\n",
      "Batch 14 loss: 0.07292184978723526, accuracy: 0.9604166746139526\n",
      "Batch 15 loss: 0.1509009748697281, accuracy: 0.958984375\n",
      "Batch 16 loss: 0.1095343604683876, accuracy: 0.9577205777168274\n",
      "Batch 17 loss: 0.0166638046503067, accuracy: 0.9589285850524902\n",
      "Training epoch: 2, train accuracy: 95.89286041259766, train loss: 0.13167871120903227, valid accuracy: 98.57142639160156, valid loss: 0.035268986597657204 \n",
      "Batch 0 loss: 0.03960795700550079, accuracy: 1.0\n",
      "Batch 1 loss: 0.05233922600746155, accuracy: 0.984375\n",
      "Batch 2 loss: 0.11887036263942719, accuracy: 0.9791666865348816\n",
      "Batch 3 loss: 0.05117768794298172, accuracy: 0.984375\n",
      "Batch 4 loss: 0.03762258216738701, accuracy: 0.987500011920929\n",
      "Batch 5 loss: 0.1253615915775299, accuracy: 0.984375\n",
      "Batch 6 loss: 0.08260267227888107, accuracy: 0.9821428656578064\n",
      "Batch 7 loss: 0.019329138100147247, accuracy: 0.984375\n",
      "Batch 8 loss: 0.04304325580596924, accuracy: 0.9861111044883728\n",
      "Batch 9 loss: 0.03446176275610924, accuracy: 0.987500011920929\n",
      "Batch 10 loss: 0.07716818153858185, accuracy: 0.9857954382896423\n",
      "Batch 11 loss: 0.07672639191150665, accuracy: 0.984375\n",
      "Batch 12 loss: 0.04099093750119209, accuracy: 0.9855769276618958\n",
      "Batch 13 loss: 0.03878702595829964, accuracy: 0.9866071343421936\n",
      "Batch 14 loss: 0.08984608203172684, accuracy: 0.9833333492279053\n",
      "Batch 15 loss: 0.04008036479353905, accuracy: 0.984375\n",
      "Batch 16 loss: 0.07285532355308533, accuracy: 0.9834558963775635\n",
      "Batch 17 loss: 0.0188616830855608, accuracy: 0.9839285612106323\n",
      "Training epoch: 3, train accuracy: 98.39286041259766, train loss: 0.058874012591938175, valid accuracy: 100.0, valid loss: 0.019328278334190447 \n",
      "Batch 0 loss: 0.06654047220945358, accuracy: 0.96875\n",
      "Batch 1 loss: 0.06801971793174744, accuracy: 0.96875\n",
      "Batch 2 loss: 0.06339583545923233, accuracy: 0.96875\n",
      "Batch 3 loss: 0.024346964433789253, accuracy: 0.9765625\n",
      "Batch 4 loss: 0.021152351051568985, accuracy: 0.981249988079071\n",
      "Batch 5 loss: 0.07942033559083939, accuracy: 0.9791666865348816\n",
      "Batch 6 loss: 0.007161556743085384, accuracy: 0.9821428656578064\n",
      "Batch 7 loss: 0.026535531505942345, accuracy: 0.984375\n",
      "Batch 8 loss: 0.0626298189163208, accuracy: 0.9826388955116272\n",
      "Batch 9 loss: 0.02660509943962097, accuracy: 0.984375\n",
      "Batch 10 loss: 0.09082521498203278, accuracy: 0.9801136255264282\n",
      "Batch 11 loss: 0.048788826912641525, accuracy: 0.9791666865348816\n",
      "Batch 12 loss: 0.026918375864624977, accuracy: 0.9807692170143127\n",
      "Batch 13 loss: 0.04894334077835083, accuracy: 0.9821428656578064\n",
      "Batch 14 loss: 0.011963363736867905, accuracy: 0.9833333492279053\n",
      "Batch 15 loss: 0.03658608719706535, accuracy: 0.982421875\n",
      "Batch 16 loss: 0.005501714069396257, accuracy: 0.9834558963775635\n",
      "Batch 17 loss: 0.05916475132107735, accuracy: 0.9821428656578064\n",
      "Training epoch: 4, train accuracy: 98.21428680419922, train loss: 0.04302774211909208, valid accuracy: 100.0, valid loss: 0.004741935214648644 \n",
      "Batch 0 loss: 0.0094981100410223, accuracy: 1.0\n",
      "Batch 1 loss: 0.024497754871845245, accuracy: 1.0\n",
      "Batch 2 loss: 0.006281179841607809, accuracy: 1.0\n",
      "Batch 3 loss: 0.026099514216184616, accuracy: 1.0\n",
      "Batch 4 loss: 0.06671182066202164, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.02021838165819645, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.012242124415934086, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.002990184584632516, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.04206813499331474, accuracy: 0.9930555820465088\n",
      "Batch 9 loss: 0.014887839555740356, accuracy: 0.9937499761581421\n",
      "Batch 10 loss: 0.0038335025310516357, accuracy: 0.9943181872367859\n",
      "Batch 11 loss: 0.022380579262971878, accuracy: 0.9947916865348816\n",
      "Batch 12 loss: 0.014502192847430706, accuracy: 0.995192289352417\n",
      "Batch 13 loss: 0.005729420110583305, accuracy: 0.9955357313156128\n",
      "Batch 14 loss: 0.010797579772770405, accuracy: 0.9958333373069763\n",
      "Batch 15 loss: 0.029773250222206116, accuracy: 0.99609375\n",
      "Batch 16 loss: 0.019483068957924843, accuracy: 0.9963235259056091\n",
      "Batch 17 loss: 0.008977068588137627, accuracy: 0.9964285492897034\n",
      "Training epoch: 5, train accuracy: 99.64286041259766, train loss: 0.018942872618532017, valid accuracy: 100.0, valid loss: 0.0018143265430505078 \n",
      "Batch 0 loss: 0.007675541564822197, accuracy: 1.0\n",
      "Batch 1 loss: 0.01690658926963806, accuracy: 1.0\n",
      "Batch 2 loss: 0.005367529112845659, accuracy: 1.0\n",
      "Batch 3 loss: 0.008699536323547363, accuracy: 1.0\n",
      "Batch 4 loss: 0.004770815372467041, accuracy: 1.0\n",
      "Batch 5 loss: 0.005177725106477737, accuracy: 1.0\n",
      "Batch 6 loss: 0.002494723768904805, accuracy: 1.0\n",
      "Batch 7 loss: 0.012769564054906368, accuracy: 1.0\n",
      "Batch 8 loss: 0.016587620601058006, accuracy: 1.0\n",
      "Batch 9 loss: 0.009652726352214813, accuracy: 1.0\n",
      "Batch 10 loss: 0.0028200342785567045, accuracy: 1.0\n",
      "Batch 11 loss: 0.012905443087220192, accuracy: 1.0\n",
      "Batch 12 loss: 0.010301700793206692, accuracy: 1.0\n",
      "Batch 13 loss: 0.004524845629930496, accuracy: 1.0\n",
      "Batch 14 loss: 0.0024430141784250736, accuracy: 1.0\n",
      "Batch 15 loss: 0.006617343984544277, accuracy: 1.0\n",
      "Batch 16 loss: 0.003655840177088976, accuracy: 1.0\n",
      "Batch 17 loss: 0.007429782301187515, accuracy: 1.0\n",
      "Training epoch: 6, train accuracy: 100.0, train loss: 0.007822243108724555, valid accuracy: 100.0, valid loss: 0.0022980292172481618 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 0.00013781616765058403, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 2.3395271733101635, test accuracy: 55.140708923339844\n",
      "Confusion matrix:\n",
      "[[219  15  17  10 159  17  30]\n",
      " [  8  26   2   2  18   0   0]\n",
      " [ 68  15  93  12 234  49  25]\n",
      " [ 82   2   6 676  73  32  24]\n",
      " [ 47  12  31  24 460  14  65]\n",
      " [ 14   3  66  15  26 288   3]\n",
      " [ 33   6  35  48 260   8 217]]\n",
      "F1-score: [0.46695096 0.38518519 0.24932976 0.80380499 0.48858205 0.69987849\n",
      " 0.44696189]\n",
      "Batch 0 loss: 1.8827056884765625, accuracy: 0.4375\n",
      "Batch 1 loss: 1.1877131462097168, accuracy: 0.484375\n",
      "Batch 2 loss: 1.4138624668121338, accuracy: 0.5\n",
      "Batch 3 loss: 1.1220835447311401, accuracy: 0.5078125\n",
      "Batch 4 loss: 0.7571098804473877, accuracy: 0.5625\n",
      "Batch 5 loss: 0.839836597442627, accuracy: 0.5833333134651184\n",
      "Batch 6 loss: 0.7562815546989441, accuracy: 0.5982142686843872\n",
      "Batch 7 loss: 0.5955619215965271, accuracy: 0.6171875\n",
      "Batch 8 loss: 0.5201732516288757, accuracy: 0.6354166865348816\n",
      "Batch 9 loss: 0.5028802156448364, accuracy: 0.65625\n",
      "Batch 10 loss: 0.35839682817459106, accuracy: 0.6761363744735718\n",
      "Batch 11 loss: 0.3677111268043518, accuracy: 0.6901041865348816\n",
      "Batch 12 loss: 0.2619256377220154, accuracy: 0.7067307829856873\n",
      "Batch 13 loss: 0.37129589915275574, accuracy: 0.7209821343421936\n",
      "Batch 14 loss: 0.3694310784339905, accuracy: 0.7270833253860474\n",
      "Batch 15 loss: 0.3585154116153717, accuracy: 0.73828125\n",
      "Batch 16 loss: 0.22655434906482697, accuracy: 0.751838207244873\n",
      "Batch 17 loss: 0.38768115639686584, accuracy: 0.7517856955528259\n",
      "Training epoch: 1, train accuracy: 75.17857360839844, train loss: 0.6822066530585289, valid accuracy: 98.57142639160156, valid loss: 0.10556742238501708 \n",
      "Batch 0 loss: 0.16884300112724304, accuracy: 0.96875\n",
      "Batch 1 loss: 0.2509397268295288, accuracy: 0.921875\n",
      "Batch 2 loss: 0.22742418944835663, accuracy: 0.9270833134651184\n",
      "Batch 3 loss: 0.0991922989487648, accuracy: 0.9453125\n",
      "Batch 4 loss: 0.156142458319664, accuracy: 0.9437500238418579\n",
      "Batch 5 loss: 0.13622315227985382, accuracy: 0.9479166865348816\n",
      "Batch 6 loss: 0.1103847473859787, accuracy: 0.9553571343421936\n",
      "Batch 7 loss: 0.16655471920967102, accuracy: 0.953125\n",
      "Batch 8 loss: 0.15374258160591125, accuracy: 0.9548611044883728\n",
      "Batch 9 loss: 0.13921886682510376, accuracy: 0.953125\n",
      "Batch 10 loss: 0.1153571680188179, accuracy: 0.9545454382896423\n",
      "Batch 11 loss: 0.12334204465150833, accuracy: 0.9557291865348816\n",
      "Batch 12 loss: 0.024800680577754974, accuracy: 0.9591346383094788\n",
      "Batch 13 loss: 0.07071901112794876, accuracy: 0.9620535969734192\n",
      "Batch 14 loss: 0.0949058011174202, accuracy: 0.9624999761581421\n",
      "Batch 15 loss: 0.08335626870393753, accuracy: 0.9609375\n",
      "Batch 16 loss: 0.050435300916433334, accuracy: 0.9632353186607361\n",
      "Batch 17 loss: 0.08735340088605881, accuracy: 0.9624999761581421\n",
      "Training epoch: 2, train accuracy: 96.25, train loss: 0.12549641210999754, valid accuracy: 100.0, valid loss: 0.028486183223625023 \n",
      "Batch 0 loss: 0.055738020688295364, accuracy: 1.0\n",
      "Batch 1 loss: 0.03903382271528244, accuracy: 1.0\n",
      "Batch 2 loss: 0.0377751886844635, accuracy: 1.0\n",
      "Batch 3 loss: 0.03160693123936653, accuracy: 1.0\n",
      "Batch 4 loss: 0.041370365768671036, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.06513211876153946, accuracy: 0.9895833134651184\n",
      "Batch 6 loss: 0.10251259058713913, accuracy: 0.9821428656578064\n",
      "Batch 7 loss: 0.04343591630458832, accuracy: 0.984375\n",
      "Batch 8 loss: 0.04413581266999245, accuracy: 0.9861111044883728\n",
      "Batch 9 loss: 0.028401335701346397, accuracy: 0.987500011920929\n",
      "Batch 10 loss: 0.04656372591853142, accuracy: 0.9857954382896423\n",
      "Batch 11 loss: 0.018096938729286194, accuracy: 0.9869791865348816\n",
      "Batch 12 loss: 0.024281855672597885, accuracy: 0.9879807829856873\n",
      "Batch 13 loss: 0.05465632677078247, accuracy: 0.9866071343421936\n",
      "Batch 14 loss: 0.020657498389482498, accuracy: 0.987500011920929\n",
      "Batch 15 loss: 0.04617390036582947, accuracy: 0.986328125\n",
      "Batch 16 loss: 0.014663414098322392, accuracy: 0.9871323704719543\n",
      "Batch 17 loss: 0.051172398030757904, accuracy: 0.987500011920929\n",
      "Training epoch: 3, train accuracy: 98.75, train loss: 0.04252267561645971, valid accuracy: 100.0, valid loss: 0.008916633552871644 \n",
      "Batch 0 loss: 0.008880174718797207, accuracy: 1.0\n",
      "Batch 1 loss: 0.026398420333862305, accuracy: 1.0\n",
      "Batch 2 loss: 0.06241409853100777, accuracy: 0.9895833134651184\n",
      "Batch 3 loss: 0.013821035623550415, accuracy: 0.9921875\n",
      "Batch 4 loss: 0.021926330402493477, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.0032733511179685593, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.02469760552048683, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.006084169261157513, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.014972391538321972, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.024757105857133865, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.029049787670373917, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.015666108578443527, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.026198381558060646, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.022654607892036438, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.01055929996073246, accuracy: 0.9979166388511658\n",
      "Batch 15 loss: 0.014903401024639606, accuracy: 0.998046875\n",
      "Batch 16 loss: 0.007492589298635721, accuracy: 0.998161792755127\n",
      "Batch 17 loss: 0.08677884936332703, accuracy: 0.9964285492897034\n",
      "Training epoch: 4, train accuracy: 99.64286041259766, train loss: 0.023362650458390515, valid accuracy: 100.0, valid loss: 0.004347328775717567 \n",
      "Batch 0 loss: 0.0115059120580554, accuracy: 1.0\n",
      "Batch 1 loss: 0.012251348234713078, accuracy: 1.0\n",
      "Batch 2 loss: 0.005817371886223555, accuracy: 1.0\n",
      "Batch 3 loss: 0.005477177444845438, accuracy: 1.0\n",
      "Batch 4 loss: 0.004190378822386265, accuracy: 1.0\n",
      "Batch 5 loss: 0.052158258855342865, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.008528575301170349, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.0012021246366202831, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.016983143985271454, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.004732664208859205, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.01711535081267357, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.006392644718289375, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.0028026800137013197, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.06264712661504745, accuracy: 0.9955357313156128\n",
      "Batch 14 loss: 0.012243048287928104, accuracy: 0.9958333373069763\n",
      "Batch 15 loss: 0.02403513714671135, accuracy: 0.99609375\n",
      "Batch 16 loss: 0.01769651658833027, accuracy: 0.9963235259056091\n",
      "Batch 17 loss: 0.10107576847076416, accuracy: 0.9946428537368774\n",
      "Training epoch: 5, train accuracy: 99.46428680419922, train loss: 0.020380846004829638, valid accuracy: 100.0, valid loss: 0.0125271612778306 \n",
      "Batch 0 loss: 0.006484167650341988, accuracy: 1.0\n",
      "Batch 1 loss: 0.022316887974739075, accuracy: 1.0\n",
      "Batch 2 loss: 0.0033377178478986025, accuracy: 1.0\n",
      "Batch 3 loss: 0.0110570527613163, accuracy: 1.0\n",
      "Batch 4 loss: 0.0109282610937953, accuracy: 1.0\n",
      "Batch 5 loss: 0.04046273231506348, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.03084016963839531, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.018010210245847702, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.022908035665750504, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.04888075217604637, accuracy: 0.9937499761581421\n",
      "Batch 10 loss: 0.0044811139814555645, accuracy: 0.9943181872367859\n",
      "Batch 11 loss: 0.028263166546821594, accuracy: 0.9947916865348816\n",
      "Batch 12 loss: 0.007216555532068014, accuracy: 0.995192289352417\n",
      "Batch 13 loss: 0.02444976381957531, accuracy: 0.9955357313156128\n",
      "Batch 14 loss: 0.002475699409842491, accuracy: 0.9958333373069763\n",
      "Batch 15 loss: 0.006926260888576508, accuracy: 0.99609375\n",
      "Batch 16 loss: 0.006363982800394297, accuracy: 0.9963235259056091\n",
      "Batch 17 loss: 0.002573245670646429, accuracy: 0.9964285492897034\n",
      "Training epoch: 6, train accuracy: 99.64286041259766, train loss: 0.01655420977880971, valid accuracy: 100.0, valid loss: 0.00093933439347893 \n",
      "Batch 0 loss: 0.04169763624668121, accuracy: 0.96875\n",
      "Batch 1 loss: 0.0012218779884278774, accuracy: 0.984375\n",
      "Batch 2 loss: 0.0007118115318007767, accuracy: 0.9895833134651184\n",
      "Batch 3 loss: 0.008562147617340088, accuracy: 0.9921875\n",
      "Batch 4 loss: 0.0034716457594186068, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.005366841331124306, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.007564452942460775, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.006573254242539406, accuracy: 0.99609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 loss: 0.006787088233977556, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.011763007380068302, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.007240040227770805, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.010585869662463665, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.008130643516778946, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.011225421912968159, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.044352613389492035, accuracy: 0.9958333373069763\n",
      "Batch 15 loss: 0.00845376867800951, accuracy: 0.99609375\n",
      "Batch 16 loss: 0.011475727893412113, accuracy: 0.9963235259056091\n",
      "Batch 17 loss: 0.0010770750232040882, accuracy: 0.9964285492897034\n",
      "Training epoch: 7, train accuracy: 99.64286041259766, train loss: 0.010903384643218791, valid accuracy: 100.0, valid loss: 0.002882563906799381 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABHKElEQVR4nO3deXxU9bn48c+TjSwkZGPfEoUKiKwRtOAOLW64oYhLhVbppVq09vYW2/tT9NZ7tbVWbdUK7gsqorhUrFuxiguyiMgiApKEsJMQluzL8/vjnAlDyErmZJKZ5/16zStz9u9Mzswz5/t9zvcrqooxxpjwFRHsAhhjjAkuCwTGGBPmLBAYY0yYs0BgjDFhzgKBMcaEOQsExhgT5iwQmFYjIrNF5Plgl8OYtkJEMkRERSQqmOWwQNACIpItIuOCXQ5jGiMiH4nIPhHpEOyymLbHAoExIU5EMoDTAAUmtvKxg/pL1zSNBQIPiEgHEXlARLa7jwd8v8REJF1E/iEihSJSICKfiEiEu+y3IrJNRA6KyAYROaeOfY8WkZ0iEuk37xIRWe0+HyUiy0XkgIjsEpH7GyjnBSKyyi3LZyIyxG9ZtojcJiLr3F+ST4lIrN/yG0Rkk/sa3hSRHn7LThSR991lu0Tkd36HjRGRZ93XuFZEso7xbTZN9xPgC+Bp4Dr/BSLSW0ReE5E9IpIvIn/zW3aDiKx3/1frRGSEO19FpJ/fek+LyB/c52eKSJ57Lu8EnhKRFPec3+OeS/8QkV5+26e659d2d/nr7vw1InKh33rRIrJXRIbXfoFuOS/wm45yjzdCRGJF5Hn39RWKyDIR6VrXGyUiPUTkVXfbLSIy02/ZbBFZICIvu+/JShEZ6rd8oHvlVeie2xP9lsWJyJ9FJEdE9ovIEhGJ8zv01SKS676+39dVNk+pqj2O8QFkA+PqmH8XzgevC9AZ+Az4H3fZ/wF/B6Ldx2mAACcAW4Ee7noZwPH1HHczMN5v+hVglvv8c+Ba93lH4JR69jEc2A2MBiJxviCygQ5+r20N0BtIBT4F/uAuOxvYC4wAOgB/BT52lyUCO4BfA7Hu9Gh32WygFDjPPeb/AV8E+/8Y6g9gE/ALYCRQAXR150cCXwN/ARLc/9dYd9nlwDbgZPf87Af0dZcp0M9v/0/7nRtnApXAve65EQekAZcB8e758Arwut/2bwMvAynuZ+IMd/5/AS/7rXcR8E09r/F24AW/6fOB9e7znwNvucePdN+HpDr2EQGscPcVAxwHfA/82O/8rQAmueX8T2ALhz/Lm4DfudueDRwETnC3fRj4COjpluGH7vuT4b6fc933aihQBgxs1XMk2Cdpe35QfyDYDJznN/1jINt9fhfwhv8HyZ3fD+eLeRwQ3chx/wA86T5PBIr8PqQfA3cC6Y3s41Hc4OQ3b4PfhzAb+A+/ZecBm93nTwB/9FvW0f2AZABTgK/qOeZs4AO/6UFASbD/j6H8AMa6/5t0d/pb4Ffu81OBPUBUHdu9C9xczz4bCwTlQGwDZRoG7HOfdweqgZQ61uvhfpkmudMLgP+qZ5/93HXj3ekXgNvd5z/F+TE2pJH3ajSQW2vebcBT7vPZ+P1wwQkcO3B+zJ0G7AQi/Ja/6G4TAZQAQ+s4Zob7fvbym/clcGVrnidWNeSNHkCO33SOOw/gTzi/HN4Tke9FZBaAqm4CbsE5cXaLyEv+1S21zAMudaubLgVWqqrveD8DfgB8614CX1DPPvoCv3YvYwtFpBDn17//MbfW8xqOeH2qegjIx/m10xsnENZnp9/zYiBWrB7ZS9cB76nqXnd6Hoerh3oDOapaWcd2jf0fG7JHVUt9EyISLyKPudUiB3B+rCS71Zu9gQJV3Vd7J6q6HedK9DIRSQbOxfmCP4r7+VkPXCgi8ThtIfPcxc/hBLaX3OqnP4pIdB276Qv0qPWZ+B3gX41U85lQ1WogD+fz0APY6s7zycH5TKTjXG0153PRsYF1A84CgTe245xUPn3ceajqQVX9taoeh3Oy3ipuW4CqzlPVse62inN5fRRVXYdzkp0LXMXhEx5V3aiqU3Cqpe4FFohIQh272QrcrarJfo94VX3Rb53edb2G2q/P3X8aTlXCVpxLahNkbh30FcAZ4rQr7QR+BQx167a3An3qCcRbgePr2XUxTjWLT7day2t3afxrnKrP0aqaBJzuK6J7nFT3i74uzwDX4FRVfa6q2+pZD5xf4FNwqpDWucEBVa1Q1TtVdRBOlcwFOO0mtW0FttT6TCSq6nl+69R8JsRp2+uF83nYDvR25/n0wflM7MWpEq3v/Qw6CwQtF+02RvkeUTgn5H+LSGcRScepc3weahpo+4mIAPuBKqBaRE4QkbPdX/mlOJeS1XUfEnC+/G/G+VC94pspIteISGf3l0mhO7uu/cwF/kOcxmcRkQQROV9EEv3WuVFEeolIKvB7nHpc3Nc3TUSGueX9X2CpqmYD/wC6i8gt4jSaJ4rI6Ca9kybQLsY5vwbhVMcMAwYCn+B8EX6JU7Vxj/v/jxWRMe62jwP/KSIj3fOjn4j4gv8q4CoRiRSRCcAZjZQjEed8LnTPpTt8C1R1B/AO8Ig4jcrRInK637av47RF3Qw828hxXgJ+BMzA78eRiJwlIie5VyAHcKrK6vpMfAkcFKehO859fYNF5GS/dUaKyKXu5/wWnPr8L4ClOAHyv9zXcCZwIfCS+1l8ErjfbYyOFJFTpS2l8rZmPVSoPXDq0bXW4w84l4EP4XzIdrjPY91tfuVuV4RzWfn/3PlDcE9EoADnC7VHA8fug3Myv11r/vM4bQ2HgLXAxQ3sYwKwDCdg7MAJKIl+r+02YJ27/Bnc+ld3+X/gXOr6yupfxzkY+BDYh3PJ62vIng0877dehvueHVVHbY+AnJ//BP5cx/wr3P9LlHsevY5TtbcXeKjW/3iDey6tAYa787Pcc+sgTrXLixzZRpBX63g9cBpKDwHf4TTe1vzfcZIRngF2uefMa7W2f9z9vHRswmv+EKexupvfvCnu6yhyj/FQfeecW9YX3fdnH86X/Di/83cBzg+ig8BXwAi/bU8E/o3zA28dcInfsjjgAZwrhP041WNxdX0G3Pfq+tY8V8Q9sDFHEJFsnJPxg2CXxYQ3Ebkd+IGqXhPkcszGaSQPajm8YI10xpg2y61K+hlwbbDLEsqsjcAY0yaJyA04DbjvqOrHwS5PKLOqIWOMCXN2RWCMMWGu3bURpKena0ZGRrCLYULUihUr9qpq52Ac285t46WGzu12FwgyMjJYvnx5sIthQpSI5DS+ljfs3DZeaujc9rRqSEQmiNOL5iZfVwq1lv9FnN4vV4nId+4t3cYYY1qRZ1cE7l18DwPjcW6cWiYib6rTPQIAqvorv/V/idMjpjHGmFbk5RXBKGCTqn6vquU4t39f1MD6U3Du6DPGGNOKvGwj6MmRvVfm4XTzehS3D5NM4F/1LJ8OTAfo06dPYEvZTlRUVJCXl0dpaWnjK5tGxcbG0qtXL6Kj6+qE0pjw0lYai68EFqhqVV0LVXUOMAcgKysrLG98yMvLIzExkYyMDJz+6syxUlXy8/PJy8sjMzMz2MUxJui8rBraxpHdGPdy59XlSqxaqEGlpaWkpaVZEAgAESEtLa1ZV1ci8qSI7BaRNX7zUsUZknOj+zfFnS8i8pCbJLFa3CEejWmrvAwEy4D+IpIpIjE4X/Zv1l5JRAbgDFH3uYdlCQkWBALnGN7Lp3F6a/U3C/hQVfvj9Hrpy4w7F+jvPqbjjAZnTJvlWdWQqlaKyE04IwNF4gytuFZE7gKWq6ovKFyJ02d3i6p85i9zmiOuOLl3I2sa03yq+rGIZNSafRFOt8vgdKP8EfBbd/6z7jn9hYgki0h3dfrebzuKC+DrF6GkMNglMYGS1g+GTm72Zp62EajqImBRrXm315qeHYhjvfn1dg6WVVog8EhhYSHz5s3jF7/4RbO2O++885g3bx7Jycn1rnP77bdz+umnM27cuBaWstV19fty38nhIQ3rSpToiTPmwxGCkghRtBc+/xt8ORfKD+EMFGZCQv8ftb1A0Jr6psXzj9Vt6wdXKCksLOSRRx45KhBUVlYSFVX/abRo0aJ6l/ncddddLS5fsKmqikizr2pbNRHi0G747K+w7AmoKIbBl8Lpv4EuAz09rGn7QiYQZKYnsL+kgsLicpLjY4JdnJAza9YsNm/ezLBhw4iOjiY2NpaUlBS+/fZbvvvuOy6++GK2bt1KaWkpN998M9OnTwcOd5tw6NAhzj33XMaOHctnn31Gz549eeONN4iLi2Pq1KlccMEFTJo0iYyMDK677jreeustKioqeOWVVxgwYAB79uzhqquuYvv27Zx66qm8//77rFixgvT09GC+Lbt8VT4i0h1nZDhoXqKE9w7uhE8fguVPQlUZDJ4Ep/8ndD4haEU6ViXlVSzZtJePNuymrLKapNhoEmOjSIqLJqnmrzOvk/u8Y2wUkRHeXvWUVVZxsLSSAyUVHKj5W+E3r4IDJZUcKK2gssq7eD+oRxI3ntWv2duFTCDISHPGZ9+yt4jhfUI7ENz51lrWbT8Q0H0O6pHEHReeWO/ye+65hzVr1rBq1So++ugjzj//fNasWVOTfvnkk0+SmppKSUkJJ598MpdddhlpaWlH7GPjxo28+OKLzJ07lyuuuIJXX32Va645erCn9PR0Vq5cySOPPMJ9993H448/zp133snZZ5/Nbbfdxj//+U+eeOKJgL7+Y/QmcB1wj/v3Db/5N4nISzj3zuwPSvvAge3w6YOw4mmoqoAhk+G0X0N6878ogin/UBkfrt/N++t38cnGPZRWVJPYwfnSP1BSwcGyykb30bFD1BGBIikuisTYo4NHUpzzt7Jaa77UD/p9iR857/CXflllQ8OLQ2SEkBTrHDMmyrscnU7xx3ZfTOgEgvR4AHLyixneJyXIpQl9o0aNOiIH/6GHHmLhwoUAbN26lY0bNx4VCDIzMxk2bBgAI0eOJDs7u859X3rppTXrvPbaawAsWbKkZv8TJkwgJaV1/8ci8iJOw3C6iOThDMB+DzBfRH4G5OCMBQxOu9h5wCacAc2ntWph9+fBkgdg5bOgVTD0Shh7K6Qd36rFaInv9xzi/XW7eH/dLlbk7kMVenSKZXJWb8YP6saozNSaL9SqauVQqftFXe+Xtv+8CnbsL2XDroMcKHGWVzfyIz0mMoKkuCgnYLhXHz06xdXM812RJLpB5vA8J7DEx0S26ay/kAkEvVPjEXGuCEJdQ7/cW0tCQkLN848++ogPPviAzz//nPj4eM4888w6c/Q7dOhQ8zwyMpKSkpI69+1bLzIyksrKxn/ttQZVnVLPonPqWFeBG70tUR0Kc+GT++Gr553pYVfBabdCSkarF6W5qquVr7YWul/+O9m8x/kcD+qexMyz+zN+UFdO7JFU55dpZITQKT76mH8NV1crReWVTjWOGzSiIqXmyiEpNprY6MgWvb62LmQCQYeoSHp0iiMnP/QDQTAkJiZy8ODBOpft37+flJQU4uPj+fbbb/niiy8CfvwxY8Ywf/58fvvb3/Lee++xb9++gB+j3SrYAkvuh1XzQCJgxE9g7C2Q3La7YymtqOLTTXt5f90uPli/m72HyoiKEEYfl8q1p/Rl3KCu9EqJ97wcERFCYmw0ibHR9CDO8+O1RSETCMBpMN6SXxzsYoSktLQ0xowZw+DBg4mLi6Nr1641yyZMmMDf//53Bg4cyAknnMApp5wS8OPfcccdTJkyheeee45TTz2Vbt26kZiYGPDjtCv5m50rgK9fhIgoyPopjLkZOvUKdsnqVVBUzr++3c3763by8Xd7KamoomOHKM44oTM/GtSVM0/oQqc46/+ptbW7MYuzsrK0vsE7fr/wG97+Zgerbv9RK5fKe+vXr2fgwPBN8ysrKyMyMpKoqCg+//xzZsyYwapVq1q0z7reUxFZoapZLdrxMWro3D7C3k3wyX2wej5ERsPIaU4ASOrufSGPQfbeIqfKZ/0ulmcXUK3QLSmWcYO6MH5QN045LpUOUaFd9dIWNHRuh9wVQWGxpZCGotzcXK644gqqq6uJiYlh7ty5wS5S69uzAT7+E6x5FSI7wCkz4IczIbFr49u2soqqah7792beWLWdjbsPATCgWyI3ndWP8YO6Mbhn3fX9JjhCKhD0DaMU0nDTv39/vvrqq2AXIzh2rXMCwNqFEB0PP/wlnPpL6BiUoZUbdaC0ghtfWMknG/dyynGpTBk1iPGDutI71fv6fnNsQioQZFoKqQk127+COWdCTEcY+ys49SZISGt0s2DZVljCT59axuY9h/jjZUOsy5d2IqQCQa+U8EkhNWGi+zA4909w0iSITw12aRr0Td5+fvbMMkrKq3h62ijG9g/qXd+mGUIqEMRGWwqpCTEiMHp6sEvRqA/W7eKXL35FakIMz80YzQndwjyjq50JqUAAlkJqTGt7+tMt3PWPdZzYoxNPXJdFl6TYYBfJNJOXA9MERd+0eLsiaAM6duwIwPbt25k0aVKd65x55pk0li75wAMPUFx8OLCfd955FBYWBqyc5thVVSt3vbWO2W+t4+wBXXn556dYEGinQi4Q+KeQmuDr0aMHCxYsOObtaweCRYsWNTi2gWkdxeWV/MfzK3jy0y1MG5PBY9eOJD4m5CoYwkbIBQJfCmm2VQ8F1KxZs3j44YdrpmfPns0f/vAHzjnnHEaMGMFJJ53EG2+8cdR22dnZDB48GICSkhKuvPJKBg4cyCWXXHJEX0MzZswgKyuLE088kTvuuANwOrLbvn07Z511FmeddRbgdGu9d+9eAO6//34GDx7M4MGDeeCBB2qON3DgQG644QZOPPFEfvSjH9Xbp5E5NrsPlnLlnC/4cP0uZl84iDsuPNHzbp6Nt0IuhPtSSLP3FjGsd3JwC+OVd2bBzm8Cu89uJ8G599S7ePLkydxyyy3ceKPTl9r8+fN59913mTlzJklJSezdu5dTTjmFiRMn1nuj0KOPPkp8fDzr169n9erVjBhxeEz3u+++m9TUVKqqqjjnnHNYvXo1M2fO5P7772fx4sVHjTuwYsUKnnrqKZYuXYqqMnr0aM444wxSUlKa3N21ab7vdh1k2lPLKCgqZ861WYwb1PZuZjPNF3JXBL4U0mxrJwio4cOHs3v3brZv387XX39NSkoK3bp143e/+x1Dhgxh3LhxbNu2jV27dtW7j48//rjmC3nIkCEMGTKkZtn8+fMZMWIEw4cPZ+3ataxbt67B8ixZsoRLLrmEhIQEOnbsyKWXXsonn3wCNL27a9M8n27ay2WPfkZ5VTXzf36qBYEQEnJXBL4U0uxQvpeggV/uXrr88stZsGABO3fuZPLkybzwwgvs2bOHFStWEB0dTUZGRp3dTzdmy5Yt3HfffSxbtoyUlBSmTp16TPvxaWp316bp5i/fyu9e+4bjOifw1LRR9EwOz146Q5WnVwQiMkFENojIJhGZVc86V4jIOhFZKyLzAnHcjPR4SyH1wOTJk3nppZdYsGABl19+Ofv376dLly5ER0ezePFicnJyGtz+9NNPZ94851+8Zs0aVq9eDcCBAwdISEigU6dO7Nq1i3feeadmm/q6vz7ttNN4/fXXKS4upqioiIULF3LaaacF8NUaAFXlvnc38F8LVnPq8WksmPFDCwIhyLMrAhGJBB4GxgN5wDIReVNV1/mt0x+4DRijqvtEpEsgjp2RlsDb39hA9oF24okncvDgQXr27En37t25+uqrufDCCznppJPIyspiwIABDW4/Y8YMpk2bxsCBAxk4cCAjR44EYOjQoQwfPpwBAwbQu3dvxowZU7PN9OnTmTBhAj169GDx4sU180eMGMHUqVMZNWoUANdffz3Dhw+3aqAAKqus4jevrObNr7dz5cm9+Z+LBxMdGXK1yQYPu6EWkVOB2ar6Y3f6NgBV/T+/df4IfKeqjzd1v03pqnfux99z96L1rLp9fMj0Qhru3VB7od12Q90K9hWVM/255SzL3sdvfnwCvzjzeOsttJ1r6Nz2Mrz3BLb6Tee58/z9APiBiHwqIl+IyIS6diQi00VkuYgs37NnT6MHzki3FFJjjlX23iIuffQzvs7bz1+nDOfGs/pZEAhxwb7OiwL64wwKPgWYKyLJtVdS1TmqmqWqWZ07N971bkba4RRSY0zTrcgp4JJHPqWwuJx514/mwqE9gl0k0wq8DATbAP8+aHu58/zlAW+qaoWqbgG+wwkMLeIbyD7UUkjb22hybZm9l0d76+vtTJm7lOT4GBb+YgxZGW27t1MTOF4GgmVAfxHJFJEY4ErgzVrrvI5zNYCIpONUFX3f0gOHYgppbGws+fn59gUWAKpKfn4+sbHWLw4478cjH23ily9+xdBenXhtxg9rqldNePAsa0hVK0XkJuBdIBJ4UlXXishdwHJVfdNd9iMRWQdUAb9R1fxAHD8jPT6k2gh69epFXl4eTWkjMY2LjY2lV6+2O8h7a6moqub/vb6Gl5ZtZeLQHvxx0hBio2384HDj6Q1lqroIWFRr3u1+zxW41X0EVKilkEZHR5OZmRnsYpgQUl2tTH92OYs37OGms/px6/gfEGF9BoWlYDcWeyYjzXohNaYhOQXFLN6wh1vG9ec/f3yCBYEwFrqBwFJIjWmQL5libD8bUjLchW4gSPMNZB86DcbGBFKu+yOpj/tZMeErZAOBL4XUBrI3pm45+cXEx0TSuWOHxlc2IS1kA0EoppAaE0i5BUX0SY23u4ZN6AYCCL0UUmMCKSe/mD6pVi1kQjwQ9E1LCLm7i40JhOpqJbegmL7WPmAI8UCQaSmkphWIyM0issYdU+MWd95sEdkmIqvcx3lBLuYRdh0spayyumaMbxPeQm6EMn/+KaTDQqQ7atO2iMhg4AZgFFAO/FNE/uEu/ouq3he0wjUgx60ytSsCAyF+RWAppKYVDASWqmqxqlYC/wYuDXKZGuVLHe2balcEJsQDgaWQmlawBjhNRNJEJB44j8O97t4kIqtF5EkRSalr4+aOtREoOQVFREUIPZKt4z0T4oHAl0KaY5lDxiOquh64F3gP+CewCqcDxUeB44FhwA7gz/Vs36yxNgIlJ7+YnilxRNnQk4YQDwTgDmRvVwTGQ6r6hKqOVNXTgX04w6/uUtUqVa0G5uK0IbQZuQWWOmoOC/lA0DctwdoIjKdEpIv7tw9O+8A8Eenut8olOFVIbUb23iIyLGPIuEI6awicFNJ9xRXsL66gU3x0sItjQtOrIpIGVAA3qmqhiPxVRIYBCmQDPw9i+Y5QWFzOgdJKyxgyNUI+EPhO9i35RQyLTw5uYUxIUtXT6ph3bTDK0hS+NjOrGjI+IV81lOneS2DVQ8Y4cgp89xBY1ZBxhHwgsBRSY46U6/4osisC4xPygcBSSI05UnZ+MV2TOhAXY2MTG0fIBwJw2gnsisAYR25+sd1RbI7gaSAQkQkiskFENonIrDqWTxWRPX4dc13vRTky0i2F1BifnIIiG5XMHMGzrCERiQQeBsYDecAyEXlTVdfVWvVlVb3Jq3KApZAa41NaUcWuA2X0tfYB48fLK4JRwCZV/V5Vy4GXgIs8PF69fCmkNjaBCXe5BTZOsTmal4GgJ7DVbzrPnVfbZW7HXAtEpHcdy1vcMVdmTXfUFghMeDvc/bS1EZjDgt1Y/BaQoapDgPeBZ+paqaUdc/lSSLP3WuaQCW++trIMuyIwfrwMBNs43B0vQC93Xg1VzVfVMnfycWCkFwWpGcjerghMmMvJLyYpNopkG6jJ+PEyECwD+otIpojEAFcCb/qvUKtjronAeq8K0zct3gKBCXs5BcVWLWSO4lkgcEdrugl4F+cLfr6qrhWRu0RkorvaTHec16+BmcBUr8qTkZ5Att1LYMJcbr6ljpqjedrpnKouAhbVmne73/PbgNu8LINPRlq8pZCasFZZVU3evhLOO6l74yubsBLsxuJW4+t73aqHTLjasb+Uymq17qfNUcInEFgKqQlzvnPf2ghMbWETCPpYCqkJc4fvIbArAnOksAkElkJqwl1uQTExURF0TYwNdlFMGxM2gQAshdSEt5z8IvqkxhMRIcEuimljwioQWAqpCWc5+cXW2ZypU3gFAr8UUmPCiaqSazeTmXqEWSCwzCETnvYcKqO4vMoaik2dwisQWAqpCVO5+db9tKlfWAUCSyE14aomddTaCEwdwioQxEZH0j0p1q4ITNjJKSgmQqBXigUCc7SwCgTgZg5ZIDBhJje/iO6d4oiJCruPvGmCsDsr+qZZCqkJP9n5xWSk29WAqVvYBYLMdEshNeEnt6CYPqmWOmrqFnaBwFJITbg5WFpBQVG5pY6aeoVfILAUUhNmLGPINCbsAoGlkJpwk1tg9xCYhoVdIPClkObYFYEJE4e7n7Y2AlO3sAsE4FQPbbFAYAJERG4WkTXu+Nu3uPNSReR9Edno/k0JVvly8otI7xhDxw6ejkxr2rGwDAR90xJqfiUZ0xIiMhi4ARgFDAUuEJF+wCzgQ1XtD3zoTgdFTn4xfax9wDTA00AgIhNEZIOIbBKRej8IInKZiKiIZHlZHp/M9HgKisrZX2IppOHm0ksv5e2336a6ujpQuxwILFXVYlWtBP4NXApcBDzjrvMMcHGgDthc1uuoaYxngUBEIoGHgXOBQcAUERlUx3qJwM3AUq/KUpvvQ2HtBOHnF7/4BfPmzaN///7MmjWLDRs2tHSXa4DTRCRNROKB84DeQFdV3eGusxPoWtfGIjJdRJaLyPI9e/a0tCxHKausYvv+ErsiMA3y8opgFLBJVb9X1XLgJZxfSbX9D3AvUOphWY6Q6aaQbrE7jMPOuHHjeOGFF1i5ciUZGRmMGzeOH/7whzz11FNUVDT/ClFV1+Ocv+8B/wRWAVW11lFA69l+jqpmqWpW586dm338xuTtK0HVxik2DfMyEPQEtvpN57nzaojICKC3qr7d0I4C/avJ9+vIUkjDU35+Pk8//TSPP/44w4cP5+abb2blypWMHz/+mPanqk+o6khVPR3YB3wH7BKR7gDu390BewHNkGsD1psmCFoagYhEAPcDUxtbV1XnAHMAsrKy6vxl1RzOQPaWQhqOLrnkEjZs2MC1117LW2+9Rffu3QGYPHkyWVnH1kQlIl1UdbeI9MFpHzgFyASuA+5x/74RkBfQTL4bJ62NwDTEy0CwDaeu1KeXO88nERgMfCQiAN2AN0Vkoqou97BcgPPBsBTS8DNz5kzOOuusOpctX74c91xsrldFJA2oAG5U1UIRuQeYLyI/A3KAK461zC2Rk19MQkwkaQkxwTi8aSe8rBpaBvQXkUwRiQGuBN70LVTV/aqarqoZqpoBfAG0ShAA514CSyENP+vWraOwsLBmet++fTzyyCMt2qeqnqaqg1R1qKp+6M7LV9VzVLW/qo5T1YIWHeQY5RYU0yct4VgDnAkTngUCN5XuJuBdYD0wX1XXishdIjLRq+M2laWQhqe5c+eSnJxcM52SksLcuXODVyCP5eQXWR9DplGethGo6iJgUa15t9ez7plelqU2/xTSIb2SW/PQJoiqqqpQ1ZpfyFVVVZSXlwe5VN6oqla2FpQwbmCdmavG1AjLO4vBUkjD1YQJE5g8eTIffvghH374IVOmTGHChAnBLpYndh4opbyq2hqKTaPCtvMRXwqptROEl3vvvZfHHnuMRx99FIDx48dz/fXXB7lU3sipyRiyqiHTsLANBL4UUhu2MrxEREQwY8YMZsyYEeyieM53D4HdVWwaE7aBANzxiy2FNKxs3LiR2267jXXr1lFaevhm9u+//z6IpfJGTkEx0ZFCj+S4YBfFtHFh20YATgpptlUNhZVp06YxY8YMoqKiWLx4MT/5yU+45pprgl0sT+TmF9MrJZ7ICEsdNQ1rUiBw+1tPEscTIrJSRH7kdeG8lpFmKaThpqSkhHPOOQdVpW/fvsyePZu3326wh5N2K6egyKqFTJM09Yrgp6p6APgRkAJci3PrfLvmG7/YupoIHx06dKC6upr+/fvzt7/9jYULF3Lo0KFgFyvgVJWcvcVkWEOxaYKmBgLfteV5wHOqutZvXruVkWYppOHmwQcfpLi4mIceeogVK1bw/PPP88wzzzS+YTuzr7iCg2WV9LHUUdMETW0sXiEi7+F0pHWbO4ZAwEb2CBZfWp2lkIaHqqoqXn75Ze677z46duzIU089FewieaYmddSqhkwTNDUQ/AwYBnyvqsUikgpM86xUrcRSSMNLZGQkS5YsCXYxWkVugXU/bZquqYHgVGCVqhaJyDXACOBB74rVeiyFNLwMHz6ciRMncvnll5OQcLja5NJLLw1iqQLPd5Xb264ITBM0NRA8CgwVkaHAr4HHgWeBM7wqWGvJSE/g3bU7g10M00pKS0tJS0vjX//6V808EQnJQNAtKZbY6MhgF8W0A00NBJWqqiJyEfA3VX3C7We93fNPIe0UFx3s4hiPhXK7gL+c/CKrFjJN1tRAcFBEbsNJGz3NHV0sJL41/VNIrRfS0Ddt2rQ6++Z/8skng1Aa7+QUFHPWCYEfA9mEpqYGgsnAVTj3E+x0h+T7k3fFaj2+FNLs/GILBGHgggsuqHleWlrKwoUL6dGjRxBLFHjF5ZXsOVhmvY6aJmtSIHC//F8AThaRC4AvVfVZb4vWOnyXz5Y5FB4uu+yyI6anTJnC2LFjg1Qab/gyhuyuYtNUTe1i4grgS+BynLFXl4rIJC8L1lpioyPp3inWMofC1MaNG9m9e3ewixFQvowhayMwTdXUqqHfAyer6m4AEekMfAAs8KpgrSkjLcGuCMJEYmLiEW0E3bp149577w1iiQLv8M1kVjVkmqapgSDCFwRc+YRQz6UZ6fG8u3ZXsIthWsHBgweDXQTP5eQXkxwfTaf4kMjnMK2gqV/m/xSRd0VkqohMBd6m1ljE7VlGWoL1QhomFi5cyP79+2umCwsLef3114NXIA/kFhRb1xKmWZoUCFT1N8AcYIj7mKOqv21sOxGZICIbRGSTiMyqY/l/iMg3IrJKRJaIyKDmvoBA8B/I3oS2O++8k06dOtVMJycnc+eddwaxRIGXk19snc2ZZmly9Y6qvqqqt7qPhY2tLyKRwMPAucAgYEodX/TzVPUkVR0G/BG4v+lFDxzfQPY2SE3oq64+uq/EysrKIJTEGxVV1WwrLLErAtMsDQYCETkoIgfqeBwUkQON7HsUsElVv1fVcuAl4CL/FdwxDnwSAD2WF9FSlkIaPrKysrj11lvZvHkzmzdv5tZbb2XkyJHBLlbAbC8soapa6WMZQ6YZGgwEqpqoqkl1PBJVNamRffcEtvpN57nzjiAiN4rIZpwrgpl17UhEpovIchFZvmfPnkYO23yWQho+/vrXvxITE8PkyZO58soriY2N5eGHHw52sQLGd1WbYVVDphmCPni9qj4MPCwiVwH/DVxXxzpzcNooyMrK8uSqwVJIw0NCQgL33NPuB9erV64vddSuCEwzeJkCug3o7Tfdy51Xn5eAiz0sT4My0uNtgJowMH78eAoLC2um9+3bx49//OPgFSjAcvKLiY2OoEtih2AXxbQjXgaCZUB/EckUkRjgSuBN/xVEpL/f5PnARg/L06CMtATyi8o5UGoppKFs7969JCcn10ynpKSE1J3FOQXF9EmNr7NjPWPq41kgUNVK4CbgXWA9MF9V14rIXSIy0V3tJhFZKyKrgFupo1qotdSkkO61q4JQFhERQW5ubs10dnZ2SH1p5uYX08fuKDbN5GkbgaouotaNZ6p6u9/zm708fnP4Uki35BdxUq9Ojaxt2qu7776bsWPHcsYZZ6CqfPLJJ8yZMyfYxQoIVSWnoIix/dODXRTTzoRMNxEt5eup0RqMQ9uECRNYvnw5J5xwAlOmTOHPf/4zcXFxLdqniPzKvbJdIyIvikisiDwtIlvcmyVXiciwwLyC+u0+WEZpRTUZ1lBsminoWUNtRVyMpZCGg8cff5wHH3yQvLw8hg0bxhdffMGpp556xNCVzSEiPXHSngepaomIzMdpDwP4jaq2WseMvmQHu6vYNJddEfjpmxZvVwQh7sEHH2TZsmX07duXxYsX89VXXx3ReHyMooA4EYkC4oHtLd3hsTjc66hdEZjmsUDgJzM9wVJIQ1xsbCyxsbEAlJWVMWDAADZs2HDM+1PVbcB9QC6wA9ivqu+5i+8WkdUi8hcRqTOfM5A3S+YWFBMZIfRMaVlVlwk/Fgj8WApp6OvVqxeFhYVcfPHFjB8/nosuuoi+ffse8/5EJAWn65RMoAeQICLXALcBA4CTgVSgzk4aVXWOqmapalbnzi0bYzgnv5geybFER9rH2jSPtRH48U8htcyh0LRwodNf4uzZsznrrLPYv38/EyZMaMkuxwFbVHUPgIi8BvxQVZ93l5eJyFPAf7bkIE2Rk19kXUuYY2KBwI+lkIaXM844IxC7yQVOEZF4oAQ4B1guIt1VdYc4NylcDKwJxMEaklNQzPkndff6MCYEWSDw40shzbEGY9NEqrpURBYAK4FK4CucfrHecYd0FWAV8B9elmN/SQWFxRXWx5A5JhYI/PhSSLdYCqlpBlW9A7ij1uyzW7MMub7UUbur2BwDa1WqpW+adT5n2p+cAut11Bw7CwS1ZKZbd9Sm/am5mczuITDHwAJBLX0thdS0Qzn5RXRO7EBCB6vtNc1ngaCWDOuF1LRDOfnFdkexOWYWCGrJSHc+TNZgbNqT3IJiG6fYHDMLBLX0TfVdEVggMO1DaUUVOw+U1py7xjSXBYJaLIXUtDd5+4pRtYwhc+wsENTBUkhNe3K4+2kLBObYWCCog6WQmvYk2w0E1s+QOVYWCOpgKaSmPcnNLyKxQxQp8dHBLopppywQ1MFSSE17kuNmDDn92xnTfJ4GAhGZICIbRGSTiMyqY/mtIrLOHbzjQxE59o7hA8iXQmrDVpr2IDe/2BqKTYt4FghEJBJ4GDgXGARMEZFBtVb7CshS1SHAAuCPXpWnOXxpeNZOYNq6qmpl675i62zOtIiXVwSjgE2q+r2qlgMv4YzkVENVF6uqr/7lC6CXh+VpsriYSLolxdY0whnTVu3YX0JFlZJhVwSmBbwMBD2BrX7Tee68+vwMeKeuBYEc17WpMtLjrWrItHmWOmoCoU00FrtjvGYBf6preSDHdW2qjDRLITVtny8Q9LXUUdMCXgaCbUBvv+le7rwjiMg44PfARFUt87A8zZKRbimkpu3LKSgiJjKCbkmxwS6Kace8DATLgP4ikikiMcCVwJv+K4jIcOAxnCCw28OyNJuvztVSSE1blptfTK/UOCIjLHXUHDvPAoGqVgI3Ae8C64H5qrpWRO4SkYnuan8COgKviMgqEXmznt21ugx3IHtrJzBtmXU/bQLB01EsVHURsKjWvNv9no/z8vgtYSmkpq1TVXLyixiVmRrsoph2rk00FrdFlkJq2rr8onKKyqvsZjLTYhYIGmAppKYtO5wxZIHAtIwFggZkpCWQY4HAtFG5Bc65aXcVm5ayQNCAjPQE9h4q56ClkJo2KCe/GBHonRoX7KKYds4CQQNqUkitncC0Qbn5xXRPiqVDVGSwi2LaOQsEDfClkG6xzCHTBmXnF9kdxSYgLBA0oGYge2snMG1QboF1P20CwwJBA3wppFvs7mLTxhwqq2TvoXLrbM4EhAWCRvRNsxRS0/bk+lJHLWPIBIAFgkZkplsKqWl7fKmjVjVkAsECQSMshdQ0RkR+JSJrRWSNiLwoIrFuZ4tL3WFaX3Y7XgwYG4fABJIFgkZYCqlpiIj0BGbiDLk6GIjE6Wn3XuAvqtoP2Icz8FLAZOcXk5oQQ1JsdCB3a8KUBYJGWAqpaYIoIE5EooB4YAdwNs443ADPABcH8oC5BUX0sV5HTYBYIGiEpZCahqjqNuA+IBcnAOwHVgCFblfs0Pgwrc2Wk2+poyZwLBA0wlJITUNEJAW4CMgEegAJwIRmbN/s8bjLK6vZXlhi4xCYgLFA0AQZ6fGsyCngUFll4yubcDMO2KKqe1S1AngNGAMku1VFUM8wrXBs43FvKyyhWqGP3VVsAsQCQRNMP/04tu4r4fpnllFSXhXs4pi2JRc4RUTiRUSAc4B1wGJgkrvOdcAbgTqg776WDKsaMgFigaAJzh7QlfuvGMrSLQX8/PkVlFVaMDAOVV2K0yi8EvgG5zM1B/gtcKuIbALSgCcCdcxcSx01AebpUJWh5KJhPSmtqOK3r37DL+d9xcNXjyA60uKoAVW9A7ij1uzvgVFeHC8nv5j4mEg6d+zgxe5NGLJvsmaYfHIfZl84iPfW7eLX87+mqlqDXSQThnypo05NlDEt52kgEJEJIrLBvbtyVh3LTxeRlSJSKSKT6tpHkxV8D4W5LdpFU0wdk8l/TTiBN7/ezu9e+4ZqCwamleXkF9s9BCagPAsEIhIJPAycCwwCpojIoFqr5QJTgXktOlh1Nbz8E3jqPCcgeOwXZ/Zj5tn9eHn5Vu76xzpULRiY1lFdrdb9tAk4L68IRgGbVPV7VS0HXsLJt66hqtmquhqobtGRIiLgor9BeRE8dT7s3dii3TXFr8b/gOvHZvL0Z9nc+88NFgxMq9h1sJSyymobkMYElJeBoCew1W/6mO+ubNJNNz2GwdR/QHWFc2Wwe/2xHKo5ZeL35w/k6tF9+Pu/N/PXf23y9HjGwOE+r+yKwARSu8gaUtU5OCl5ZGVl1f/Tu+uJMHURPHMhPH0+XPs6dB/iWblEhP+5aDAlFVXc//53xMdEcv1px3l2PGNCbRyCiooK8vLyKC0tDXZRQkZsbCy9evUiOrrpHRJ6GQi2Ab39puu9uzKgOv8Api2CZyY6AeHa16DnSM8OFxEh/PGyIZRVVPOHt9cTGx3JNaf09ex4JrzlFBQRFSH0SI4NdlECIi8vj8TERDIyMiwLKgBUlfz8fPLy8sjMzGzydl5WDS0D+rv9ssfgdM37pofHOyzteCcYxHaCZy+G3KWeHi4qMoK/TB7G2QO68N+vr+HVFXmeHs+Er5z8YnqmxBEVIvewlJaWkpaWZkEgQESEtLS0Zl9heXY2uT0v3gS8C6wH5qvqWhG5S0QmAojIySKSB1wOPCYiawNWgJS+TjBI6AzPXQLZSwK267rEREXwyNUjGNsvnd8s+Jq3V+/w9HgmPOUWhF7qqAWBwDqW99PTnxWqukhVf6Cqx6vq3e6821X1Tff5MlXtpaoJqpqmqicGtACdejnBoFMveH4SbP5XQHdfW2x0JHN+MpKRfVO4+aWv+HD9Lk+PZ8JP9t4iMixjyARYaFxfNiSxG0x926kumnclfPeup4eLj4niyaknM6hHEjOeX8mSjXs9PZ4JH4XF5RworbSMoQArLCzkkUceafZ25513HoWFhQ2uc/vtt/PBBx8cY8laT+gHAoCOneG6t6DLQHjpalj/lqeHS4yN5tmfjuK4zgnc8OxylmUXeHo8Ex5qxikOsaqhYKsvEFRWNtzt/KJFi0hOTm5wnbvuuotx48a1pHitol2kjwZEfCr85A14YRLMvw4umwuDL/PscMnxMTz3s9FMnvM5055axgvXj2Zo72TPjmdCX06B7x6C0KwauvOttazbfiCg+xzUI4k7Lmy4xnnWrFls3ryZYcOGER0dTWxsLCkpKXz77bd89913XHzxxWzdupXS0lJuvvlmpk+fDkBGRgbLly/n0KFDnHvuuYwdO5bPPvuMnj178sYbbxAXF8fUqVO54IILmDRpEhkZGVx33XW89dZbVFRU8MorrzBgwAD27NnDVVddxfbt2zn11FN5//33WbFiBenp6QF9LxoSHlcEPnHJcO1C6D0aXr0eVr3o6eE6J3bghetHk5IQzU+e/JL1OwJ7kpvwkuuOQ2BXBIF1zz33cPzxx7Nq1Sr+9Kc/sXLlSh588EG+++47AJ588klWrFjB8uXLeeihh8jPzz9qHxs3buTGG29k7dq1JCcn8+qrr9Z5rPT0dFauXMmMGTO47777ALjzzjs5++yzWbt2LZMmTSI31/s+02oLnysCnw6JcM0CeHEKvD4Dqsph5HWeHa57pzjmXX8Kl//9c659YikvTT+Vfl06enY8E7qy84vpmtSBuJjIYBfFE439cm8to0aNOiIH/6GHHmLhwoUAbN26lY0bN5KWlnbENpmZmQwbNgyAkSNHkp2dXee+L7300pp1XnvtNQCWLFlSs/8JEyaQkpISyJfTJOF1ReATkwBXvQz9xsFbM+HLuZ4erndqPC/cMBqAax5fWnN3qDHNkZtfHDJ3FLdlCQmH3+OPPvqIDz74gM8//5yvv/6a4cOH15mj36HD4bEhIiMj621f8K3X0DrBEJ6BACA6Dq58AU44Hxb9J3z2N08Pd3znjjx//WhKK6u46vEv2F5Y4unxTOjJKSiyUck8kJiYyMGDB+tctn//flJSUoiPj+fbb7/liy++CPjxx4wZw/z58wF477332LdvX8CP0ZjwDQQAUR3gimdg0MXw3u/h4/s8PdyAbkk8+9NR7C+u4JrHl7LnYJmnxzOho7Siil0Hyuhr7QMBl5aWxpgxYxg8eDC/+c1vjlg2YcIEKisrGThwILNmzeKUU04J+PHvuOMO3nvvPQYPHswrr7xCt27dSExMDPhxGqSq7eoxcuRIDbjKCtUF16vekaT64R9Uq6sDfww/y7bk64D/fkd/dP+/teBQmafHMs0DLNc2eG5v2HlA+/72H/r6V3kBfb3Btm7dumAXIehKS0u1oqJCVVU/++wzHTp0aIv3Wdf72tC5HX6NxXWJjIJL/g5RMfDxH6GqDMbdCR7d+p6Vkcrj12Ux7ell/OTJL3nhhtEkxTa9p0ATfg53P21tBKEmNzeXK664gurqamJiYpg719s2y7pYIPCJiIQL/wqRHeDTB6GyDCbc41kwGNMvnb9fM4KfP7eC8x78hDN+0JlRmamMzkyjW6fQ6FnSBE6OmzqaYW0EIad///589dVXQS2DBQJ/ERFw/p+dtoMvHnGCwfn3O/M9cPaArsz9SRZPfZrNG6u288JSJ3+4T2o8J2ekMjozlVGZqfRNs4HKw11OfjFJsVEkx8cEuygmBFkgqE0Efvy/TjBY8hfnPoOJf3WuGDxw5gldOPOELlRWVfPtzoMs3VLAl1vyWbxhN6+udLqz7pLYgZMzDweGH3RJJCLCAkM4ySkotmoh4xkLBHURgXPugKhY+Oj/nCuDSx5z2hI8EhUZweCenRjcsxM/G5uJqrJ5zyE3MDgPX9fWneKiOTkjhVGZqYzKTOPEHklEh0j/9KZuuflFnNizU7CLYUKUBYL6iMCZsyAyBj68Ewo2ww8mQN8x0OtkiPa2Hl9E6NclkX5dErl6dF9Ulbx9JTVB4cvsAj5YvxuA+JhIRvTxBYZUhvVOJjY6NO8+DUeVVdXk7SvhvJO6B7soJkRZIGjMabdCfBosmwsf3QOoExx6ZkHGGCcw9B7l3K3sIRGhd2o8vVPjuWxkLwB2Hyxl2ZZ9fLkln6VbCvjLB9+hCjGREQzp1YlRmakM6pFEfEwksdHOI859+J53iI6gQ1SEtUG0YTv2l1JZrdb9dBvRsWNHDh06xPbt25k5cyYLFiw4ap0zzzyT++67j6ysrHr388ADDzB9+nTi453/63nnnce8efMa7dHUCxYImmLkdc6jZB/kfuGMdpbzKXzyZ/j4TxARBT1GuIFhLPQZ7fRp5LEuibGcP6Q75w9xfinuL65geY5zxbB0SwFzPv6eymptdD8iHBEcYqMj/J6782MiiY2KcP76rduxQyTxMVEkdIgkoUPU4ecxUe50pAWaFsp2M4asjaBt6dGjR51BoKkeeOABrrnmmppAsGjRokAVrdksEDRHXAqccK7zACg9AFuXHg4Mn/3VaWCWSOg+1C8wnOL0fOqxTvHRnDOwK+cM7ApAcXkluQXFlFZUU1pRRUlFFaXlVZRWVlFSfnhemfu3pKKK0orqI+YVl1dSUFROaUVVzfq+9ZoqKkKIj4msCQwd/QOG77m7PMENLImxUXSKiyY5PobkuGiS46NJio0Oy0byw/cQhPgVwTuzYOc3gd1nt5Pg3HsaXGXWrFn07t2bG2+8EYDZs2cTFRXF4sWL2bdvHxUVFfzhD3/goosuOmK77OxsLrjgAtasWUNJSQnTpk3j66+/ZsCAAZSUHO5CZsaMGSxbtoySkhImTZrEnXfeyUMPPcT27ds566yzSE9PZ/HixTXdWqenp3P//ffz5JNPAnD99ddzyy23kJ2dXW931y1lgaAlYpOg/3jnAVBe5AaGT53AsPQxJzhIhHNC9h3rBIc+pzrjI3gsPiaKAd2SAr/jyjL0wHbKSks4FN+T4qpoisorKSqrpKi8iuKySg6VVVJcXnV4fpkTVIrKnHnFZVVsLyx15pVXUeSu3xARp6E8OS6aTvExpMRHu0Eixg0a0aTEx9DJb35KfDSJsdFEtuMAkltQTExUBF0T7f4SL0yePJlbbrmlJhDMnz+fd999l5kzZ5KUlMTevXs55ZRTmDhxYr1Xto8++ijx8fGsX7+e1atXM2LEiJpld999N6mpqVRVVXHOOeewevVqZs6cyf3338/ixYuPGndgxYoVPPXUUyxduhRVZfTo0ZxxxhmkpKSwceNGXnzxRebOncsVV1zBq6++yjXXXNPi98ACQSDFJMDxZzsPgIoSyFt2ODAsfwK+eBgQ6Hqi076QMcYJErHJ0CHJ08ykRlVXQdEeOLAdDu6Egzv8HjvhgPu8pAABYoFYBDr1hrTjIK0fpB7vDAvaox8k94HIpt8xXV2tlFQ4QeFgWSWFxRXsLymnsLjCfZRTWOI+L6mgoKic7/cU1QzhWB8RSIp1AoUvQPxl8jBSE9pHTn5OfhF9UuND/2qokV/uXhk+fDi7d+9m+/bt7Nmzh5SUFLp168avfvUrPv74YyIiIti2bRu7du2iW7dude7j448/ZubMmQAMGTKEIUOG1CybP38+c+bMobKykh07drBu3bojlte2ZMkSLrnkkppeUC+99FI++eQTJk6c2OTurpvL028dEZkAPAhEAo+r6j21lncAngVGAvnAZFXN9rJMrSo6DjJPdx7gpKFuW+EGhiXw1XPw5WO1tol32hc6JDlXHEc87+RMxybVWt7p8LwOic5x/X+5qDrtGw19uR/cCYd2gdb6VS4R0LGrM/ZzSoZTzZXYHZK6Q0Q07NsC+ZsgfzOsfgXK9vttGwkpfY8MEGnHO8879Trq3oyICHGrh6Lo0sy3uqpaOVBSwT43WOwvrqCwpJx9RRXutDN/nxtQoiPbz5dqTn6xdTbnscsvv5wFCxawc+dOJk+ezAsvvMCePXtYsWIF0dHRZGRk1Nn9dGO2bNnCfffdx7Jly0hJSWHq1KnHtB+f2t1d+1dBtYRngUBEIoGHgfFAHrBMRN5U1XV+q/0M2Keq/UTkSuBeYLJXZQq6qA7Q94fOg99AZTnsWAV7N0LZAafNoeyA3/ODzvMDO9z5B6H8UOPHiYg6HChUnS/4yjpOvrhU50s9sRt0HXT4eWIP92936Nil6TfTqUJxvhMUCjYfDhD5m512lAq/cRgiYyAl0wkSace5gaKfEygSuzfctUd1lRNUq8qhqgKqyomsKiOlqoKUqnKILIf4Cogpg44V7nq1HpFZQMv7dxKRE4CX/WYdB9wOJAM3AHvc+b9T1Wa3BqoquQXF/PD41hu2MBxNnjyZG264gb179/Lvf/+b+fPn06VLF6Kjo1m8eDE5OTkNbn/66aczb948zj77bNasWcPq1asBOHDgAAkJCXTq1Ildu3bxzjvvcOaZZwKHu7+uXTV02mmnMXXqVGbNmoWqsnDhQp577jlPXrePl1cEo4BNqvo9gIi8BFwE+AeCi4DZ7vMFwN9ERNye8kJfVIyTetp7VNO3qa46HBRqAofv+f4jA0jpAUCP/HJPcv927Bb4eyFEICHdefQZfeQyVeeqI3+TGyQ2Hw4Ymz5wOvrziY53gkF1Zc0X/REPbXpDdb1OOD8gr19VNwDDoObHzzZgITAN+Iuqtqhv8z2Hyigurwr9huIgO/HEEzl48CA9e/ake/fuXH311Vx44YWcdNJJZGVlMWDAgAa3nzFjBtOmTWPgwIEMHDiQkSNHAjB06FCGDx/OgAED6N27N2PGjKnZZvr06UyYMIEePXqwePHimvkjRoxg6tSpjBrlfC9cf/31DB8+PGDVQHURr75zRWQSMEFVr3enrwVGq+pNfuuscdfJc6c3u+vsrbWv6cB0gD59+oxsLDqbdqa6CvbnHRkgDu10rhoio52OAGuexzhXVr7n/o8o/+la20V1OHJ+QnqdVzoiskJV60/+boCI/Ai4Q1XHiMhs4FBzAkFWVpYuX778iHnbCkv430Xr+emYDEb29T7BoLWtX7+egQMHBrsYIaeu97Whc7tdNBar6hxgDjgfliAXxwRahNuWkNL3cEN7+3Ql8KLf9E0i8hNgOfBrVT1q6KlaP3KO2mHP5DgevmrEUfONCSQvO6jZBvT2m+7lzqtzHRGJAjrhNBob066ISAwwEXjFnfUocDxOtdEO4M91baeqc1Q1S1WzOnfu3BpFNeYoXgaCZUB/Ecl0PyRXAm/WWudN4Dr3+STgX2HTPmBCzbnASlXdBaCqu1S1SlWrgbk4bWamDvaRD6xjeT89CwSqWgncBLwLrAfmq+paEblLRCa6qz0BpInIJuBWYJZX5THGY1PwqxYSEf8e4i4B1rR6idqB2NhY8vPzLRgEiKqSn59PbGzzEiE8bSNw0+UW1Zp3u9/zUuByL8tgjNdEJAEnTfrnfrP/KCLDAAWyay0zrl69epGXl8eePXsaX9k0SWxsLL169WrWNu2isdiYtkxVi4C0WvOuDVJx2pXo6GgyMzODXYywZ6OZGGNMmLNAYIwxYc4CgTHGhDnP7iz2iojsAeq7tTgd2FvPsrbMyt26Gip3X1UNSkK/ndttSiiWu95zu90FgoaIyPJj7R4gmKzcras9lrs9lhms3K3tWMttVUPGGBPmLBAYY0yYC7VAMCfYBThGVu7W1R7L3R7LDFbu1nZM5Q6pNgJjjDHNF2pXBMYYY5rJAoExxoS5kAgEIjJBRDaIyCYRaTc9mIpIbxFZLCLrRGStiNwc7DI1h4hEishXIvKPYJelqUQkWUQWiMi3IrJeRE4Ndpka0h7PbTuvg6Ml53a7byNwx4n9Dqf3xzyccRCmqOq6BjdsA9yuirur6koRSQRWABe3h7IDiMitQBaQpKoXBLs8TSEizwCfqOrj7jgZ8apaGORi1am9ntt2XgdHS87tULgiGAVsUtXvVbUceAm4KMhlahJV3aGqK93nB3HGbegZ3FI1jYj0As4HHg92WZpKRDoBp+OMg4GqlrfVIOBql+e2ndetr6XndigEgp7AVr/pPNrJSedPRDKA4cDSIBelqR4A/guoDnI5miMT2AM85V76P+6OJdBWtftz287rVtOiczsUAkG7JyIdgVeBW1T1QLDL0xgRuQDYraorgl2WZooCRgCPqupwoAgbFc8zdl63qhad26EQCLYBvf2me7nz2gURicb5sLygqq8FuzxNNAaYKCLZONUVZ4vI88EtUpPkAXmq6vt1ugDnw9NWtdtz287rVteiczsUAsEyoL+IZLoNJFcCbwa5TE0iIoJTp7deVe8PdnmaSlVvU9VeqpqB837/S1WvCXKxGqWqO4GtInKCO+scoC03YLbLc9vO69bX0nO73Q9VqaqVInIT8C4QCTypqmuDXKymGgNcC3wjIqvceb9zx3o23vgl8IL7xfo9MC3I5alXOz637bwOjmM+t9t9+qgxxpiWCYWqIWOMMS1ggcAYY8KcBQJjjAlzFgiMMSbMWSAwxpgwZ4HAICJntreeFo1pjJ3XTWeBwBhjwpwFgnZERK4RkS9FZJWIPOb2m35IRP7i9vv+oYh0dtcdJiJfiMhqEVkoIinu/H4i8oGIfC0iK0XkeHf3Hf36Mn/BvTvUGM/ZeR18FgjaCREZCEwGxqjqMKAKuBpIAJar6onAv4E73E2eBX6rqkOAb/zmvwA8rKpDgR8CO9z5w4FbgEHAcTh3hxrjKTuv24Z238VEGDkHGAksc3/UxAG7cbrLfdld53ngNbdv8mRV/bc7/xngFXeQkJ6quhBAVUsB3P19qap57vQqIANY4vmrMuHOzus2wAJB+yHAM6p62xEzRf5frfWOtc+QMr/nVdi5YVqHnddtgFUNtR8fApNEpAuAiKSKSF+c/+Ekd52rgCWquh/YJyKnufOvBf7tjhaVJyIXu/voICLxrfkijKnFzus2wKJjO6Gq60Tkv4H3RCQCqABuxBmAYpS7bDdOfSvAdcDf3Q+Ef0+E1wKPichd7j4ub8WXYcwR7LxuG6z30XZORA6pasdgl8OYQLLzunVZ1ZAxxoQ5uyIwxpgwZ1cExhgT5iwQGGNMmLNAYIwxYc4CgTHGhDkLBMYYE+b+P6wh1wlsVnyzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 4.5289337335775923e-05, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 2.4674300843763897, test accuracy: 54.47199630737305\n",
      "Confusion matrix:\n",
      "[[214  26  18  12 138  19  40]\n",
      " [  8  27   0   1  16   2   2]\n",
      " [ 74  20  86  15 206  57  38]\n",
      " [105   1   7 652  62  34  34]\n",
      " [ 44  19  35  22 428  15  90]\n",
      " [ 13   7  55  14  22 297   7]\n",
      " [ 36  11  36  46 218   9 251]]\n",
      "F1-score: [0.44536941 0.32335329 0.23465211 0.78696439 0.49110729 0.7004717\n",
      " 0.46959775]\n",
      "Batch 0 loss: 1.3010798692703247, accuracy: 0.59375\n",
      "Batch 1 loss: 1.7222615480422974, accuracy: 0.59375\n",
      "Batch 2 loss: 1.4255387783050537, accuracy: 0.5729166865348816\n",
      "Batch 3 loss: 1.196258544921875, accuracy: 0.578125\n",
      "Batch 4 loss: 0.8246676921844482, accuracy: 0.5874999761581421\n",
      "Batch 5 loss: 0.972290575504303, accuracy: 0.5885416865348816\n",
      "Batch 6 loss: 0.46572139859199524, accuracy: 0.625\n",
      "Batch 7 loss: 0.6797217726707458, accuracy: 0.65234375\n",
      "Batch 8 loss: 0.5584380626678467, accuracy: 0.6701388955116272\n",
      "Batch 9 loss: 0.6573435664176941, accuracy: 0.6781250238418579\n",
      "Batch 10 loss: 0.5103768110275269, accuracy: 0.6931818127632141\n",
      "Batch 11 loss: 0.2764466106891632, accuracy: 0.7135416865348816\n",
      "Batch 12 loss: 0.4215591549873352, accuracy: 0.7211538553237915\n",
      "Batch 13 loss: 0.2560299038887024, accuracy: 0.7366071343421936\n",
      "Batch 14 loss: 0.2859930396080017, accuracy: 0.75\n",
      "Batch 15 loss: 0.2343548834323883, accuracy: 0.76171875\n",
      "Batch 16 loss: 0.23403526842594147, accuracy: 0.7738970518112183\n",
      "Batch 17 loss: 0.4018250107765198, accuracy: 0.7767857313156128\n",
      "Training epoch: 1, train accuracy: 77.67857360839844, train loss: 0.6902190273006757, valid accuracy: 97.14286041259766, valid loss: 0.1116775597135226 \n",
      "Batch 0 loss: 0.1391049474477768, accuracy: 0.96875\n",
      "Batch 1 loss: 0.27075597643852234, accuracy: 0.921875\n",
      "Batch 2 loss: 0.16627518832683563, accuracy: 0.9270833134651184\n",
      "Batch 3 loss: 0.19410118460655212, accuracy: 0.921875\n",
      "Batch 4 loss: 0.16639725863933563, accuracy: 0.918749988079071\n",
      "Batch 5 loss: 0.08316207677125931, accuracy: 0.9322916865348816\n",
      "Batch 6 loss: 0.05578248202800751, accuracy: 0.9419642686843872\n",
      "Batch 7 loss: 0.28520485758781433, accuracy: 0.94140625\n",
      "Batch 8 loss: 0.03700029104948044, accuracy: 0.9479166865348816\n",
      "Batch 9 loss: 0.05508367344737053, accuracy: 0.949999988079071\n",
      "Batch 10 loss: 0.14476439356803894, accuracy: 0.9488636255264282\n",
      "Batch 11 loss: 0.17205995321273804, accuracy: 0.9479166865348816\n",
      "Batch 12 loss: 0.10709600150585175, accuracy: 0.9471153616905212\n",
      "Batch 13 loss: 0.07854781299829483, accuracy: 0.9508928656578064\n",
      "Batch 14 loss: 0.10987761616706848, accuracy: 0.949999988079071\n",
      "Batch 15 loss: 0.039793629199266434, accuracy: 0.953125\n",
      "Batch 16 loss: 0.08741766959428787, accuracy: 0.9522058963775635\n",
      "Batch 17 loss: 0.19820575416088104, accuracy: 0.9517857432365417\n",
      "Training epoch: 2, train accuracy: 95.17857360839844, train loss: 0.13281282037496567, valid accuracy: 100.0, valid loss: 0.018700469906131428 \n",
      "Batch 0 loss: 0.12822231650352478, accuracy: 0.9375\n",
      "Batch 1 loss: 0.042642805725336075, accuracy: 0.96875\n",
      "Batch 2 loss: 0.04029904678463936, accuracy: 0.9791666865348816\n",
      "Batch 3 loss: 0.16347636282444, accuracy: 0.96875\n",
      "Batch 4 loss: 0.07402870804071426, accuracy: 0.96875\n",
      "Batch 5 loss: 0.12854650616645813, accuracy: 0.9583333134651184\n",
      "Batch 6 loss: 0.2145034372806549, accuracy: 0.9598214030265808\n",
      "Batch 7 loss: 0.0411752387881279, accuracy: 0.9609375\n",
      "Batch 8 loss: 0.053373195230960846, accuracy: 0.9618055820465088\n",
      "Batch 9 loss: 0.028414877131581306, accuracy: 0.965624988079071\n",
      "Batch 10 loss: 0.02837824635207653, accuracy: 0.96875\n",
      "Batch 11 loss: 0.13017822802066803, accuracy: 0.9661458134651184\n",
      "Batch 12 loss: 0.02928726188838482, accuracy: 0.96875\n",
      "Batch 13 loss: 0.05633891001343727, accuracy: 0.96875\n",
      "Batch 14 loss: 0.04218250513076782, accuracy: 0.9708333611488342\n",
      "Batch 15 loss: 0.060418520122766495, accuracy: 0.970703125\n",
      "Batch 16 loss: 0.01367841474711895, accuracy: 0.9724264740943909\n",
      "Batch 17 loss: 0.09621662646532059, accuracy: 0.9714285731315613\n",
      "Training epoch: 3, train accuracy: 97.14286041259766, train loss: 0.07618673373427656, valid accuracy: 100.0, valid loss: 0.017848508122066658 \n",
      "Batch 0 loss: 0.04148632287979126, accuracy: 1.0\n",
      "Batch 1 loss: 0.07822446525096893, accuracy: 0.984375\n",
      "Batch 2 loss: 0.04149940609931946, accuracy: 0.9791666865348816\n",
      "Batch 3 loss: 0.009320501238107681, accuracy: 0.984375\n",
      "Batch 4 loss: 0.05020309239625931, accuracy: 0.981249988079071\n",
      "Batch 5 loss: 0.009541588835418224, accuracy: 0.984375\n",
      "Batch 6 loss: 0.013099800795316696, accuracy: 0.9866071343421936\n",
      "Batch 7 loss: 0.006450321990996599, accuracy: 0.98828125\n",
      "Batch 8 loss: 0.05776632949709892, accuracy: 0.9861111044883728\n",
      "Batch 9 loss: 0.020421260967850685, accuracy: 0.987500011920929\n",
      "Batch 10 loss: 0.011746092699468136, accuracy: 0.9886363744735718\n",
      "Batch 11 loss: 0.0035844515077769756, accuracy: 0.9895833134651184\n",
      "Batch 12 loss: 0.006564147770404816, accuracy: 0.9903846383094788\n",
      "Batch 13 loss: 0.01850663125514984, accuracy: 0.9910714030265808\n",
      "Batch 14 loss: 0.02474307268857956, accuracy: 0.9895833134651184\n",
      "Batch 15 loss: 0.04486560449004173, accuracy: 0.990234375\n",
      "Batch 16 loss: 0.06932400912046432, accuracy: 0.9889705777168274\n",
      "Batch 17 loss: 0.03268864005804062, accuracy: 0.9892857074737549\n",
      "Training epoch: 4, train accuracy: 98.92857360839844, train loss: 0.030001985530058544, valid accuracy: 100.0, valid loss: 0.009776837051807282 \n",
      "Batch 0 loss: 0.03805961459875107, accuracy: 0.96875\n",
      "Batch 1 loss: 0.023863481357693672, accuracy: 0.984375\n",
      "Batch 2 loss: 0.021236613392829895, accuracy: 0.9895833134651184\n",
      "Batch 3 loss: 0.016071828082203865, accuracy: 0.9921875\n",
      "Batch 4 loss: 0.013200514018535614, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.011278520338237286, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.016698991879820824, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.011358334682881832, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.013005618937313557, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.006029034964740276, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.00562973553314805, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.031538158655166626, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.003994580823928118, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.0483708418905735, accuracy: 0.9955357313156128\n",
      "Batch 14 loss: 0.11614122241735458, accuracy: 0.9916666746139526\n",
      "Batch 15 loss: 0.01108899898827076, accuracy: 0.9921875\n",
      "Batch 16 loss: 0.030317887663841248, accuracy: 0.9926470518112183\n",
      "Batch 17 loss: 0.007324005477130413, accuracy: 0.9928571581840515\n",
      "Training epoch: 5, train accuracy: 99.28571319580078, train loss: 0.023622665761245623, valid accuracy: 100.0, valid loss: 0.001394451401817302 \n",
      "Batch 0 loss: 0.012853912077844143, accuracy: 1.0\n",
      "Batch 1 loss: 0.00560257863253355, accuracy: 1.0\n",
      "Batch 2 loss: 0.0021882178261876106, accuracy: 1.0\n",
      "Batch 3 loss: 0.04460682347416878, accuracy: 1.0\n",
      "Batch 4 loss: 0.04371529445052147, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.0014193619135767221, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.006861693225800991, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.010709981434047222, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.042410533875226974, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.005600910168141127, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.002625617664307356, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.006301975809037685, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.021326210349798203, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.019446298480033875, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.061285458505153656, accuracy: 0.9958333373069763\n",
      "Batch 15 loss: 0.017078332602977753, accuracy: 0.99609375\n",
      "Batch 16 loss: 0.007542313542217016, accuracy: 0.9963235259056091\n",
      "Batch 17 loss: 0.000604010361712426, accuracy: 0.9964285492897034\n",
      "Training epoch: 6, train accuracy: 99.64286041259766, train loss: 0.01734330691073814, valid accuracy: 100.0, valid loss: 0.0014735621189174708 \n",
      "Batch 0 loss: 0.026873910799622536, accuracy: 0.96875\n",
      "Batch 1 loss: 0.0012843299191445112, accuracy: 0.984375\n",
      "Batch 2 loss: 0.0014772196300327778, accuracy: 0.9895833134651184\n",
      "Batch 3 loss: 0.011407848447561264, accuracy: 0.9921875\n",
      "Batch 4 loss: 0.0028558066114783287, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.02945660799741745, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.002820775378495455, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.002989347092807293, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.001141371438279748, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.02838030830025673, accuracy: 0.9937499761581421\n",
      "Batch 10 loss: 0.003659679088741541, accuracy: 0.9943181872367859\n",
      "Batch 11 loss: 0.004205780103802681, accuracy: 0.9947916865348816\n",
      "Batch 12 loss: 0.0031406946945935488, accuracy: 0.995192289352417\n",
      "Batch 13 loss: 0.026424353942275047, accuracy: 0.9933035969734192\n",
      "Batch 14 loss: 0.0043891980312764645, accuracy: 0.9937499761581421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 loss: 0.017926376312971115, accuracy: 0.994140625\n",
      "Batch 16 loss: 0.0011090121697634459, accuracy: 0.9944853186607361\n",
      "Batch 17 loss: 0.0020919733215123415, accuracy: 0.9946428537368774\n",
      "Training epoch: 7, train accuracy: 99.46428680419922, train loss: 0.009535255182224015, valid accuracy: 100.0, valid loss: 0.0007644951450250422 \n",
      "Batch 0 loss: 0.005240738857537508, accuracy: 1.0\n",
      "Batch 1 loss: 0.026724833995103836, accuracy: 1.0\n",
      "Batch 2 loss: 0.009206422604620457, accuracy: 1.0\n",
      "Batch 3 loss: 0.022231008857488632, accuracy: 1.0\n",
      "Batch 4 loss: 0.0006980009493418038, accuracy: 1.0\n",
      "Batch 5 loss: 0.0010400189785286784, accuracy: 1.0\n",
      "Batch 6 loss: 0.0028781434521079063, accuracy: 1.0\n",
      "Batch 7 loss: 0.001555531402118504, accuracy: 1.0\n",
      "Batch 8 loss: 0.0008698892779648304, accuracy: 1.0\n",
      "Batch 9 loss: 0.07003835588693619, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.002737651811912656, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.0010486934334039688, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.006537049543112516, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.0115651311352849, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.001897665555588901, accuracy: 0.9979166388511658\n",
      "Batch 15 loss: 0.004402697551995516, accuracy: 0.998046875\n",
      "Batch 16 loss: 0.010558906011283398, accuracy: 0.998161792755127\n",
      "Batch 17 loss: 0.001412663608789444, accuracy: 0.9982143044471741\n",
      "Training epoch: 8, train accuracy: 99.82142639160156, train loss: 0.010035744606284425, valid accuracy: 100.0, valid loss: 0.0006018336086223522 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABHsklEQVR4nO3deXxU9dX48c/JvkKGhDUBg4KyyRoRRVzRokVURFGrFarSohaXPs8jts9P0dY+2lqrtmpFRa2KiiiKLdYVF+pSFhHZFFAkISwhCyQkIdv5/XFvwhAmG8lkJjPn/XrNKzP33rn3zOTOnLnfVVQVY4wx4Ssi0AEYY4wJLEsExhgT5iwRGGNMmLNEYIwxYc4SgTHGhDlLBMYYE+YsEZh2IyJzROT5QMdhTLAQkUwRURGJCmQclghaQUS2isj4QMdhTFNE5EMRKRSR2EDHYoKPJQJjQpyIZALjAAUmtfOxA/pL1zSPJQI/EJFYEXlQRHLd24O1v8REJE1E/iEiRSJSICKfiEiEu+42EdkuIsUi8o2InOVj3yeKyE4RifRadpGIrHHvjxaRFSKyT0R2icgDjcQ5UURWu7F8KiJDvdZtFZHbRWS9+0vyaRGJ81p/nYhsdl/DYhHp5bVusIi8667bJSK/9jpsjIj83X2N60Qk6wjfZtN8PwU+B54BrvZeISK9ReQ1EckTkXwR+avXuutEZIP7v1ovIiPd5Soi/by2e0ZEfufeP11EctxzeSfwtIh43HM+zz2X/iEiGV7P7+KeX7nu+tfd5WtF5Hyv7aJFZI+IjKj/At04J3o9jnKPN1JE4kTkeff1FYnIchHp7uuNEpFeIvKq+9zvRWSW17o5IrJQRF5235NVIjLMa/1A98qryD23J3mtixeRP4nIDyKyV0SWiUi816F/IiLb3Nf3G1+x+ZWq2u0Ib8BWYLyP5XfjfPC6AV2BT4Hfuuv+D/gbEO3exgECHAdkA73c7TKBYxo47hbgbK/HrwCz3fufAVe595OAMQ3sYwSwGzgRiMT5gtgKxHq9trVAb6AL8G/gd+66M4E9wEggFvgL8LG7LhnYAfwKiHMfn+iumwOUA+e5x/w/4PNA/x9D/QZsBq4HRgGVQHd3eSTwFfBnINH9f53irrsE2A6c4J6f/YCj3HUK9PPa/zNe58bpQBVwn3tuxAOpwMVAgns+vAK87vX8fwIvAx73M3Gau/x/gJe9trsA+LqB13gH8ILX4x8DG9z7PwfedI8f6b4PnXzsIwJY6e4rBjga+A74kdf5WwlMceP8L+B7Dn6WNwO/dp97JlAMHOc+9xHgQyDdjeFk9/3JdN/PJ9z3ahhwABjYrudIoE/Sjnyj4USwBTjP6/GPgK3u/buBN7w/SO7yfjhfzOOB6CaO+ztgnns/Gdjv9SH9GLgLSGtiH4/hJievZd94fQi3Ar/wWncesMW9/xTwB691Se4HJBO4HPiygWPOAd7zejwIKAv0/zGUb8Ap7v8mzX28EbjFvX8SkAdE+Xje28BNDeyzqURQAcQ1EtNwoNC93xOoATw+tuvlfpl2ch8vBP6ngX32c7dNcB+/ANzh3v8Zzo+xoU28VycC2+otux142r0/B68fLjiJYwfOj7lxwE4gwmv9i+5zIoAyYJiPY2a672eG17L/AJe153liRUP+0Qv4wevxD+4ygD/i/HJ4R0S+E5HZAKq6GbgZ58TZLSIveRe31DMfmOwWN00GVqlq7fGuAY4FNrqXwBMb2MdRwK/cy9giESnC+fXvfczsBl7DIa9PVUuAfJxfO71xEmFDdnrdLwXixMqR/elq4B1V3eM+ns/B4qHewA+qWuXjeU39HxuTp6rltQ9EJEFEHneLRfbh/FhJcYs3ewMFqlpYfyeqmotzJXqxiKQA5+J8wR/G/fxsAM4XkQScupD57urncBLbS27x0x9EJNrHbo4CetX7TPwa8C5GqvtMqGoNkIPzeegFZLvLav2A85lIw7naasnnIqmRbducJQL/yMU5qWr1cZehqsWq+itVPRrnZL1V3LoAVZ2vqqe4z1Wcy+vDqOp6nJPsXOAKDp7wqOomVb0cp1jqPmChiCT62E02cI+qpnjdElT1Ra9tevt6DfVfn7v/VJyihGycS2oTYG4Z9KXAaeLUK+0EbgGGuWXb2UCfBhJxNnBMA7suxSlmqdWj3vr6Qxr/Cqfo80RV7QScWhuie5wu7he9L88CV+IUVX2mqtsb2A6cX+CX4xQhrXeTA6paqap3qeognCKZiTj1JvVlA9/X+0wkq+p5XtvUfSbEqdvLwPk85AK93WW1+uB8JvbgFIk29H4GnCWC1ot2K6Nqb1E4J+T/ikhXEUnDKXN8HuoqaPuJiAB7gWqgRkSOE5Ez3V/55TiXkjW+Dwk4X/434XyoXqldKCJXikhX95dJkbvY136eAH4hTuWziEiiiPxYRJK9trlBRDJEpAvwG5xyXNzXN11Ehrvx/h74QlW3Av8AeorIzeJUmieLyInNeidNW7sQ5/wahFMcMxwYCHyC80X4H5yijXvd/3+ciIx1n/sk8F8iMso9P/qJSG3yXw1cISKRIjIBOK2JOJJxzuci91y6s3aFqu4A3gIeFadSOVpETvV67us4dVE3AX9v4jgvAecAM/H6cSQiZ4jI8e4VyD6cojJfn4n/AMXiVHTHu69viIic4LXNKBGZ7H7Ob8Ypz/8c+AInQf6P+xpOB84HXnI/i/OAB9zK6EgROUmCqSlve5ZDhdoNpxxd691+h3MZ+DDOh2yHez/Ofc4t7vP241xW/j93+VDcExEowPlC7dXIsfvgnMz/rLf8eZy6hhJgHXBhI/uYACzHSRg7cBJKstdrux1Y765/Frf81V3/C5xL3dpYvcs4hwDvA4U4l7y1FdlzgOe9tst037PDyqjt1ibn57+AP/lYfqn7f4lyz6PXcYr29gAP1/sff+OeS2uBEe7yLPfcKsYpdnmRQ+sIcuodrxdORWkJ8C1O5W3d/x2nMcKzwC73nHmt3vOfdD8vSc14ze/jVFb38Fp2ufs69rvHeLihc86N9UX3/SnE+ZIf73X+LsT5QVQMfAmM9HruYOAjnB9464GLvNbFAw/iXCHsxSkei/f1GXDfq2vb81wR98DGHEJEtuKcjO8FOhYT3kTkDuBYVb0ywHHMwakkD2gc/mCVdMaYoOUWJV0DXBXoWEKZ1REYY4KSiFyHU4H7lqp+HOh4QpkVDRljTJizKwJjjAlzHa6OIC0tTTMzMwMdhglRK1eu3KOqXQNxbDu3jT81dm53uESQmZnJihUrAh2GCVEi8kPTW/mHndvGnxo7t61oyBhjwpwlAmOMCXN+TQQiMkGccfU31w6uVm/9n8UZD3+1iHzrDvJkjDGmHfmtjsAd1+MR4GycoRSWi8hidQZMA0BVb/Ha/pc4Y+QbHyorK8nJyaG8vLzpjU2T4uLiyMjIIDra1yCUxoQXf1YWjwY2q+p3ACLyEu6ogA1sfzleg1GZQ+Xk5JCcnExmZibOeHXmSKkq+fn55OTk0Ldv30CHY0zA+bNoKJ1Dx7PPcZcdxh3VsC/wQQPrZ4gz/eKKvLy8Ng+0IygvLyc1NdWSQBsQEVJTU1t0dSUi80Rkt4is9VrWRZwpOTe5fz3uchGRh90i0TXiTvFoTLAKlsriy4CFqlrta6WqzlXVLFXN6to1IE28g4IlgbZzBO/lMzijtXqbDbyvqv1xRr2srQc7F+jv3mbgzAZnTNDyZ9HQdg6d2CTDXebLZcANrTnYguXZIHBpVu+mNzamhVT1YxHJrLf4Apxhl8EZRvlD4DZ3+d/VGb/lcxFJEZGe6oy9H1j782Hl01B1INCRGH9I7QfDprb4af5MBMuB/iLSFycBXIYzm9YhRGQAzqTVn7XmYG98tZ3SimpLBH5SVFTE/Pnzuf7661v0vPPOO4/58+eTkpLS4DZ33HEHp556KuPHj29llO2uu9eX+04OTmnYULHoYYlARGbgXDXQp08f/0UKUF0JL/8Etn2GMzmYCTXa/xwkmBKBqlaJyI04c4VG4ky2vk5E7gZWqOpid9PLcGbxadXodxkpCXzwze7WBW0aVFRUxKOPPnpYIqiqqiIqquHTaMmSJU3u++677251fIGmqioiLT6HVXUuMBcgKyvLvyNAvnuHkwQufgqOn+LXQxn/2ltayea8YjbvLjl4yythmKTw1yPYn1+HmFDVJcCSesvuqPd4TlscK90TT17xAcorq4mLjmyLXRovs2fPZsuWLQwfPpzo6Gji4uLweDxs3LiRb7/9lgsvvJDs7GzKy8u56aabmDFjBnBw2ISSkhLOPfdcTjnlFD799FPS09N54403iI+PZ9q0aUycOJEpU6aQmZnJ1VdfzZtvvkllZSWvvPIKAwYMIC8vjyuuuILc3FxOOukk3n33XVauXElaWlog35ZdtUU+ItITZ2Y4aFmxaPv4eiF8/iicONOSwBEoLq9kS95+dhSV0Sk+Gk9CDKlJMXgSYoiJ8k9Vq6qya98B94u+mM15tV/6+9lTcrBoLyYqgqPTEhmWkcJJx6Qe0bE63FhDDcnwxAOQW1TG0V2TAhyNf9315jrW5+5r030O6tWJO88f3OD6e++9l7Vr17J69Wo+/PBDfvzjH7N27dq65pfz5s2jS5culJWVccIJJ3DxxReTmnroSblp0yZefPFFnnjiCS699FJeffVVrrzy8Mme0tLSWLVqFY8++ij3338/Tz75JHfddRdnnnkmt99+O//617946qmn2vT1H6HFwNXAve7fN7yW3+g2mT4R2BvQ+oHdG2DxL6H3GDjntwELI9ipKntKKup+XW/x+rW9c1/DLcySY6PwJMbQxdctwfnrSYwh1f3bKS7qkMYK1TXKtoLSw37df7e7hOIDVQePExdFv25JnHFcV/p1S6q7ZXgSiIxoXVFfCCWCBAByCkM/EQSD0aNHH9IG/+GHH2bRokUAZGdns2nTpsMSQd++fRk+fDgAo0aNYuvWrT73PXny5LptXnvtNQCWLVtWt/8JEybg8Xja8uU0SURexKkYThORHJw+L/cCC0TkGuAHnLmAwbkKPg/YjDOh+fR2DdZb+V54+UqITYZLn4VI60BXU6NsLyo77It38+4S9pZV1m2XGBPJMd2SOPmYVI5xv3TTU+LZf6CKgv0VFJRWUFDi/C3cX0H+/gp2F5ezccc+8vdXcKCqxufxoyKkLjGowvd79lNRfXDb7p1i6dctickj0+nXLanu2F2TYv3WcjBkEkG6e0WQU1gW4Ej8r7Ff7u0lMTGx7v6HH37Ie++9x2effUZCQgKnn366zzb6sbGxdfcjIyMpK/P9v6rdLjIykqqqKp/btDdVvbyBVWf52FZpZSu4NqEKr18PBd/DtH9Aco9AR9SuKqpq2Jq//9Av/N0lfLenhPLKg1+8qYkxHNMtiR8P7Um/rgd/affsHNeqL97SCjdh+LgVllaQX1JBjcLpA7rWHfeYbkl0imv/ZB0yiaB7cixREcL2otJAhxKSkpOTKS4u9rlu7969eDweEhIS2LhxI59//nmbH3/s2LEsWLCA2267jXfeeYfCwsI2P0bI+feDsPEf8KP/g6NODnQ0flNyoOpgMY77y37L7hJ+KCiluuZg/Xt6Sjz9uiVx0jGpB4tWuibhSYzxS1wJMVEkxETVlVYEs5BJBFGREfRMiQuLK4JASE1NZezYsQwZMoT4+Hi6d+9et27ChAn87W9/Y+DAgRx33HGMGTOmzY9/5513cvnll/Pcc89x0kkn0aNHD5KTk9v8OCHjuw/h/bth8GQYMzPQ0bSaqpK/v+KQX/Zb3C/9HXsPXn1GRQiZaYkc2z2Z847vWfeFf3TXRBJiQubrrs11uDmLs7KytKHJOy6b+xmV1cqrM0Pv18+GDRsYOHBgoMMImAMHDhAZGUlUVBSfffYZM2fOZPXq1a3ap6/3VERWqmpWq3Z8hBo7t1tkbw48fiokdoVr34fYjlNnVld+X6+ydnNeCUWlB8vvE2Ii637RH9MtiWPcopWjUhOIjgyWAROCS2PndkilyAxPAss27Ql0GMYPtm3bxqWXXkpNTQ0xMTE88cQTgQ4pOFUdgAU/haoKmPp8h0oCm3YV84vnV7Ilb3/dstry+/OOb9vye3OoEEsE8ewqLudAVTWxUdaXIJT079+fL7/8MtBhBL9/zYbtK50kkNY/0NE027/W7uRXC1YTHxPFby8cwoAeyX4tvzeHCqlEkJ4SjyrsKConMy2x6ScYE0q+fAFWzIOxN8PA8wMdTbPU1Ch/fu9b/vLBZob1TuFvV46kZ+f4QIcVdkIqEdTWzm8vKrNEYMLLjq/gn7dC31PhzP8X6GiaZW9ZJTe/9CVLv8nj0qwM7r5giI0KECAhlghq+xJYE1ITRkoL4OWrICEVLp4HkcH/sd60q5gZz60ku6CU314wmCvHHGVl/gEU/GdMC/ToHEeEhEenMmMAqKmB12bAvlz42b8gKfjn6/CuD3hxxhhOyOwS6JDCXki1s4qOjKBn53i2WyIIuKQkp7VKbm4uU6b4HuTs9NNPp6nmkg8++CClpQev8M477zyKioraLM4O7+M/wOZ34dz7ICMgrV6braZG+dM73/CL51fSr3syb/5yrCWBIBFSiQCcoSbsiiB49OrVi4ULFx7x8+sngiVLljQ6t0FY+fYd+PBeGHYFZP0s0NE0am9ZJdf+fQV/+WAzl2Zl8PKMMVYpHERCLhFkpMRbHYEfzJ49m0ceeaTu8Zw5c/jd737HWWedxciRIzn++ON54403Dnve1q1bGTJkCABlZWVcdtllDBw4kIsuuuiQsYZmzpxJVlYWgwcP5s477wScgexyc3M544wzOOOMMwBnWOs9e5y+Ig888ABDhgxhyJAhPPjgg3XHGzhwINdddx2DBw/mnHPOaXBMow6t4Ht47VroMQQmPgBBXL6+aVcxFz7ybz7+No/fXjCY+y4eapXCQSak6gjAqTDeua+cyuqa0O1h+NZs2Pl12+6zx/Fw7r0Nrp46dSo333wzN9zgjKW2YMEC3n77bWbNmkWnTp3Ys2cPY8aMYdKkSQ1W+j322GMkJCSwYcMG1qxZw8iRB+d0v+eee+jSpQvV1dWcddZZrFmzhlmzZvHAAw+wdOnSw+YdWLlyJU8//TRffPEFqsqJJ57IaaedhsfjafZw1x1WZRksuMq5f+lzEB28v6zfXreTW19eTXxMJPOvG8PovlYUFIxC7psyw5NAjcLOvQ2PH25absSIEezevZvc3Fy++uorPB4PPXr04Ne//jVDhw5l/PjxbN++nV27djW4j48//rjuC3no0KEMHTq0bt2CBQsYOXIkI0aMYN26daxfv77ReJYtW8ZFF11EYmIiSUlJTJ48mU8++QRo/nDXHZIq/ONW54fA5CehS9+mnxMANTXKA+98w8+fW0m/bkm8+ctTLAkEsZC7Iqgdjjq7sJTeXYJ/1L8j0sgvd3+65JJLWLhwITt37mTq1Km88MIL5OXlsXLlSqKjo8nMzPQ5/HRTvv/+e+6//36WL1+Ox+Nh2rRpR7SfWs0d7rpDWvk0fDUfTpsNx54T6Gh82ltWyS0vr+aDjbu5ZFQGv73Q+gcEuxC8InASgbUcantTp07lpZdeYuHChVxyySXs3buXbt26ER0dzdKlS/nhhx8aff6pp57K/PnzAVi7di1r1qwBYN++fSQmJtK5c2d27drFW2+9Vfechoa/HjduHK+//jqlpaXs37+fRYsWMW7cuDZ8tUEoZyW8dRv0OxtOuy3Q0fhUvz7gD1OsPqAjCLkrgp6d4xHrS+AXgwcPpri4mPT0dHr27MlPfvITzj//fI4//niysrIYMGBAo8+fOXMm06dPZ+DAgQwcOJBRo0YBMGzYMEaMGMGAAQPo3bs3Y8eOrXvOjBkzmDBhAr169WLp0qV1y0eOHMm0adMYPXo0ANdeey0jRowIrWIgb/v3OIPJJfeAyXMhIvh+w1l9QMfl12GoRWQC8BAQCTypqoeVaYjIpcAcQIGvVPWKxvbZnKF6x/z+fcb2S+NPlw470tCDTrgPQ+0PHWYY6ppqeO4iyP4CrnkHegbXeV1Tozz43rc8/MFmhmV05m9XjbKmoUEoIMNQi0gk8AhwNpADLBeRxaq63mub/sDtwFhVLRSRbm1x7AxPvM1UZkLHB7+D7z+CCx4NuiSwr7ySW15azftWH9Ch+bNoaDSwWVW/AxCRl4ALAO/mINcBj6hqIYCq7m6LA2d44lnxg01laELA9lWw7AEYNR1G/CTQ0Rxie1EZ0+b9h+/37Lfxgjo4fxY0pgPZXo9z3GXejgWOFZF/i8jnblHSYURkhoisEJEVeXl5TR/YE8+OveVUVdc0uW1H0tFmkwtmHea97DUCpsxzhpAIIuty93LRI/9m575y/n7NaK46KdOSQAcW6BqnKKA/cDpwOfCEiKTU30hV56pqlqpmde3a9KBaGZ4EqmuUXcUH2jjcwImLiyM/P7/jfIEFMVUlPz+fuLi4QIfSNBEYcjFExTa9bTv5+Ns8Lv3bZ0RFCAt/cTInH5PW9JNMUPNn0dB2oLfX4wx3mbcc4AtVrQS+F5FvcRLD8tYcuG446oJS0lNCo9IqIyODnJwcmnNFZJoWFxdHRkZGoMPocF5Zkc3tr31Nv25JPDN9ND06d4Bkaprkz0SwHOgvIn1xEsBlQP0WQa/jXAk8LSJpOEVF37X2wLVf/jmFZZzY2p0FiejoaPr2Dc5epCb0qSoPvb+JB9/bxLj+aTz6k5Ekx0UHOizTRvyWCFS1SkRuBN7GaT46T1XXicjdwApVXeyuO0dE1gPVwH+ran5rj93LTQTbi6wvgTGtVVldw28Wfc2CFTlcPDKDey8+PnTH8QpTfu1QpqpLgCX1lt3hdV+BW91bm4mLjqRbcqyNQmpMK5UcqOL6F1bx8bd5zDqrP7eM72+VwiEo5HoW17J5CYxpnd37ypn+zHI27izmvouPZ+oJfQIdkvGTkE0EGZ4E1uQUBToMYzqkzbuLuXrecgpLK3jy6izOOK5N+nqaIBWyBX0Znnhyi8qorrHmlsa0xBff5TP50U85UFXDyzNOsiQQBkI2EaSnxFNZrewutnkJjGmuN7/K5aqn/kPX5FgWXX8yx2d0DnRIph2EbCKw4aiNaT5VZe7HW/jli18yvHcKr848OXTn8zCHCeFE4JzEVmFsTOOqa5Q5i9fx+yUb+fHQnvz9mtGkJMQEOizTjkK2svhgpzJrQmpMQ8oqqrnppS95Z/0urhvXl9vPHUhEhDUPDTchmwjiYyJJS4qxTmXGNCC/5ADX/n0Fq7OLmHP+IKaNtZ7r4SpkEwFAuifBioaM8eGH/P1cPe8/7NhbzmM/GcWEIT0CHZIJoJCtIwDISLFOZcb/ROQmEVkrIutE5GZ32RwR2S4iq93beQEOs87q7CImP/ope8sqmX/dGEsCJsQTgSee7UVl1FhfAuMnIjIEZ4Kl0cAwYKKI9HNX/1lVh7u3JQ3upB2tz93HZXM/IzE2ildnnsyoozyBDskEgZBPBBVVNewpCZ15CUzQGYgzlHqpqlYBHwGTAxxTgz7ZlEd5ZQ0v/3wMR3dNCnQ4JkiEdCJId/sSZFvxkPGftcA4EUkVkQTgPA7Ow3GjiKwRkXkiEhQ/vbMLS+kcH22Ty5tDhHQiqO1LYC2HjL+o6gbgPuAd4F/Aapwh1R8DjgGGAzuAP/l6fkunYW2tnMKyus6WxtQK6URgfQlMe1DVp1R1lKqeChQC36rqLlWtVtUa4AmcOgRfz23RNKytlV1QSm+P9Rg2hwrpRJAYG4UnIdpaDhm/EpFu7t8+OPUD80Wkp9cmF+EUIQWUqpJTWEbvLnZFYA4V0v0IwCkesvGGjJ+9KiKpQCVwg6oWichfRGQ4oMBW4OcBjA+AvJIDHKiqqSsyNaZWGCSCeL7dVRzoMEwIU9VxPpZdFYhYGpNd4PwgsisCU19IFw2BU0+wvagMZ1ZMY8JXbV2Z1RGY+kI+EWR44imvrCF/f0WgQzEmoGrrytKt1ZCpx6+JQEQmiMg3IrJZRGb7WD9NRPK8uuFf29Yx2HDUxjiyC0pJS4ohISbkS4RNC/ktEYhIJPAIcC4wCLhcRAb52PRlr274T7Z1HLW/fqwJqQl32YWlVlFsfPLnFcFoYLOqfqeqFcBLwAV+PJ5P6TZTmTGAdSYzDfNnIkgHsr0e57jL6rvY7Ya/UER6+1jfqt6XneKi6RxvfQlMeKuuUXKLymz6SeNToCuL3wQyVXUo8C7wrK+NWtv7Mj0l3oqGTFjbta+cymq1FkPGJ38mgu0cHHwLIMNdVkdV81W1dmjQJ4FR/gikdjhqY8JVdoHzQ8iKhowv/kwEy4H+ItJXRGKAy4DF3hvU64Y/Cdjgj0Ay3JnKrC+BCVe1I/Ba0ZDxxW/tyFS1SkRuBN4GIoF5qrpORO4GVqjqYmCWiEwCqoACYJo/Ykn3xFNaUU1haSVdEmP8cQhjglpOYSki0CslLtChmCDk1wbF7qxMS+otu8Pr/u3A7f6MAQ5eDm8vLLNEYMJSdkEZ3ZPjiI2KDHQoJggFurK4XWRYXwIT5rILS22MIdOg8EgEKda72IS37YVl1mLINCgsEkGn+CiSY6Os5ZAJS5XVNezYa53JTMPCIhGICOke60tgwlNuURk1ChnWYsg0ICwSATj1BFY0ZMJR7XlvVwSmIWGUCJyZyqwvgQk3tZ3JrI7ANCSMEkE8xQeq2FdWFehQjGlX2YWlREYIPTtbHwLjW9gkgvQU57I42+oJTJjJKSyjZ+c4oiLD5uNuWihszozacdit5ZAJN9kFpVYsZBoVRomgtlOZJQITXnIKy6wzmWlU2CSClIRoEmIirQmpCSvlldXsLj5gM5OZRoVNIhARZzhquyIwYSSnbtRRuyIwDQubRAAHh6M2JlzUXgFbHYFpTFglApupzISb7LrOZJYITMPCKhFkeOLZV17FvvLKQIdiTLvIKSglJiqCbsmxgQ7FBLEwSwRuE1IrHjJhIqewjIyUeCIiJNChmCAWVokg3ZqQmjCTXVhad94b05CwSgQHZyqzegITHrILSm2eYtOksEoEqYkxxEVH2BVBGJs8eTL//Oc/qampCXQofldyoIrC0kprMWSa5NdEICITROQbEdksIrMb2e5iEVERyfJzPG7LIUsE4er6669n/vz59O/fn9mzZ/PNN98EOiS/qW0hZ8NPm6b4LRGISCTwCHAuMAi4XEQG+dguGbgJ+MJfsXjL8CTYeENhbPz48bzwwgusWrWKzMxMxo8fz8knn8zTTz9NZWVotSbLLqjtTGZXBKZx/rwiGA1sVtXvVLUCeAm4wMd2vwXuA8r9GEudDJupLOzl5+fzzDPP8OSTTzJixAhuuukmVq1axdlnnx3o0NrUwc5kdkVgGufPRJAOZHs9znGX1RGRkUBvVf1nYzsSkRkiskJEVuTl5bUuKE88haWV7D9g8xKEo4suuohx48ZRWlrKm2++yeLFi5k6dSp/+ctfKCkpCXR4bSq7oIz46Ei6JMYEOhQT5KICdWARiQAeAKY1ta2qzgXmAmRlZbVqijHv4aiP7Z7cml2ZDmjWrFmcccYZPtetWLECkdBpb59TWErvLvEh9ZqMf/jzimA70NvrcYa7rFYyMAT4UES2AmOAxf6uMD44HLUVD4Wj9evXU1RUVPe4sLCQRx99NHAB+VF2YZkNLWGaxZ+JYDnQX0T6ikgMcBmwuHalqu5V1TRVzVTVTOBzYJKqrvBjTGSkWKeycPbEE0+QkpJS99jj8fDEE0+0ap8icpOIrBWRdSJys7usi4i8KyKb3L+eVh2khVSVnIJSqx8wzeK3RKCqVcCNwNvABmCBqq4TkbtFZJK/jtuUtKRYYqIibJiJMFVdXY2qHvK4oqLiiPcnIkOA63AaRwwDJopIP2A28L6q9gfedx+3m31lVRQfqLIWQ6ZZ/FpHoKpLgCX1lt3RwLan+zOWWhERQob1JQhbEyZMYOrUqfz85z8H4PHHH2fChAmt2eVA4AtVLQUQkY+AyTgt5E53t3kW+BC4rTUHaols60NgWiBglcWBlG5NSMPWfffdx+OPP85jjz0GwNlnn821117bml2uBe4RkVSgDDgPWAF0V9Ud7jY7ge6+niwiM4AZAH369GlNHIfILqhNBHZFYJoWlokgwxPPuzv2BToMEwARERHMnDmTmTNntsn+VHWDiNwHvAPsB1YD1fW2URHx2dqtLVvEeTs4M5klAtO0sBprqFaGJ4E9JRWUVVQ3vbEJKZs2bWLKlCkMGjSIo48+uu7WGqr6lKqOUtVTgULgW2CXiPQEcP/ubnXwLZBdWEpyXBSd46Pb87CmgwrLRJDuthzaXmTFQ+Fm+vTpzJw5k6ioKJYuXcpPf/pTrrzyylbtU0S6uX/74NQPzMdpIXe1u8nVwButOkgLZReU2mBzptmalQjc5nGdxPGUiKwSkXP8HZy/ZNi8BGGrrKyMs846C1XlqKOOYs6cOfzzn412bG+OV0VkPfAmcIOqFgH3AmeLyCZgvPu43eQUltmE9abZmltH8DNVfUhEfgR4gKuA53DKRTuc2go0SwThJzY2lpqaGvr3789f//pX0tPTWz20hKqO87EsHzirVTs+QqpKTmEZpx7bNRCHNx1Qc4uGavuonwc8p6rrvJZ1ON2SY4mOFEsEYeihhx6itLSUhx9+mJUrV/L888/z7LPPBjqsNrWnpIKyymrrTGaarblXBCtF5B2gL3C7O3R0h53ZIyJC6JUSb8NRh5nq6mpefvll7r//fpKSknj66acDHZJf1I06ai2GTDM1NxFcAwwHvlPVUhHpAkz3W1TtwIajDj+RkZEsW7Ys0GH4XbZ7pWt9CExzNTcRnASsVtX9InIlMBJ4yH9h+V96SjxLv2ndkNam4xkxYgSTJk3ikksuITExsW755MmTAxhV27KZyUxLNTcRPAYME5FhwK+AJ4G/A6f5KzB/y/AkkFd8gPLKauKiIwMdjmkn5eXlpKam8sEHH9QtE5GQSgTZBWWkJsaQGBuW/UXNEWjumVLl9o68APirqj4lItf4MzB/q/21lFtUxtFdkwIcjWkvoVov4C2nsNSuBkyLNDcRFIvI7TjNRse5k8p06C6L6V7DUVsiCB/Tp0/3OVHLvHnzAhCNf+QUljGoV6dAh2E6kOYmgqnAFTj9CXa6PSj/6L+w/C+jy8GZykz4mDhxYt398vJyFi1aRK9evQIYUduqqVG2F5bxo8E9Ah2K6UCalQjcL/8XgBNEZCLwH1X9u39D86/uybFERYi1HAozF1988SGPL7/8ck455ZQARdP2dhWXU1FdY0VDpkWaO8TEpcB/gEuAS4EvRGSKPwPzt6jICHp0jrNOZWFu06ZN7N7druPB+ZWNOmqORHOLhn4DnKCquwFEpCvwHrDQX4G1hwxPvM1UFmaSk5MPqSPo0aMH9913XwAjalsH5yGwKwLTfM1NBBG1ScCVTwiMXJrhSWDZpj2BDsO0o+Li4kCH4FfZBc4Pm9rGEMY0R3O/zP8lIm+LyDQRmQb8k3pTUHZE6SnxTplqVYcdLcO00KJFi9i7d2/d46KiIl5//fXABdTGcgpL6d4p1vrGmBZpViJQ1f/GmUVpqHubq6rtNv+qv2R44lGFHXuteChc3HXXXXTu3LnucUpKCnfddVcAI2pb2YWlNrSEabFmF++o6quqeqt7W9Sc54jIBBH5RkQ2i8hsH+t/ISJfi8hqEVkmIoNaEnxr2XDU4aem5vCrv6qqqgBE4h85hWU26qhpsUYTgYgUi8g+H7diEWl00l8RiQQeAc4FBgGX+/iin6+qx6vqcOAPwANH/lJa7uAENdaENFxkZWVx6623smXLFrZs2cKtt97KqFGjAh1Wm6iqrmHH3nJrMWRarNFEoKrJqtrJxy1ZVZvqujga2Kyq36lqBfAScEG9/Xsnk0SgzSbvbo4eneOIEKzlUBj5y1/+QkxMDFOnTuWyyy4jLi6ORx55JNBhtYkde8uprlFrMWRazJ+jUqUD2V6Pc4AT628kIjcAtwIxwJm+diQiM4AZAH369GmzAKMjI+jZOd6KhsJIYmIi997brrNGtpvs2nkIrI7AtFDAm4Cq6iOqegxwG/C/DWwzV1WzVDWra9e2nX4vPcUSQTg5++yzKSoqqntcWFjIj370o8AF1IZyCqwzmTky/kwE24HeXo8z3GUNeQm40I/x+GQT1ISXPXv2kJKSUvfY4/GETM/i7MJSIsQp8jSmJfyZCJYD/UWkr4jEAJcBi703EJH+Xg9/DGzyYzw+ZXji2bmvnMpq60sQDiIiIti2bVvd461bt/ocjbQjyikso2fneKIjA36hbzoYv9URqGqViNwIvA1EAvNUdZ2I3A2sUNXFwI0iMh6oBAqBq/0VT0PSPfHUKOy01hZh4Z577uGUU07htNNOQ1X55JNPmDt3bqDDahPZBaX07mIVxabl/DqFkaouoV4PZFW9w+v+Tf48fnPU9iXILiy1RBAGJkyYwIoVK5g7dy4jRozgwgsvJD4+NL48swtLGde/bevQTHgI+7nsapvaWRPS8PDkk0/y0EMPkZOTw/Dhw/n888856aSTDpm6siM6UFXNrn0HrMWQOSJhX5jYs3M8Ita7OFw89NBDLF++nKOOOoqlS5fy5ZdfHlJ53FFtrxt+OjSubkz7CvtEEBMVQfdkm5cgXMTFxREX57SqOXDgAAMGDOCbb74JcFStl+2evzbOkDkSYV80BO68BEXWhDQcZGRkUFRUxIUXXsjZZ5+Nx+PhqKOOCnRYrVbbBNquCMyRsESA03Jo5Q+FgQ7DtINFi5zxEufMmcMZZ5zB3r17mTBhQoCjar3sgjKiI4XuydaHwLScJQKcK4J/rNlBVXUNUdYGO2ycdtppgQ6hzeQUlpKeEk9ERGj0iTDty771cMpVq2uUXcUHAh2KMUcku7DMmj+bI2aJgIPT+uUUWD2B6ZhyCkpt1FFzxCwR4D0vgbUcMh1PaUUV+fsrrMWQOWKWCIBe7hXB9iJLBKbjySm0UUdN61giAOKiI+maHGujkJoOKdst0rSiIXOkLBG4nOGo7YrAdDx1VwRWNGSOkCUCV4YnwYqGzBERkVtEZJ2IrBWRF0UkTkSeEZHvRWS1exvur+NnF5QSFx1BWlKMvw5hQpwlAld6Sjy5RWVU17TrtMmmgxORdGAWkKWqQ3CGXL/MXf3fqjrcva32VwzZhaVkeBJCZl4F0/4sEbgyPPFUViu7i8sDHYrpeKKAeBGJAhKA3PY8eE5hGb2tfsC0giUClw1HbY6Eqm4H7ge2ATuAvar6jrv6HhFZIyJ/FpFYX88XkRkiskJEVuTl5R1RDM6ENFY/YI6cJQKX9SUwR0JEPMAFQF+gF5AoIlcCtwMDgBOALsBtvp6vqnNVNUtVs7p2bfmkMnvLKtlXXmUthkyrWCJwpac4v6isCalpofHA96qap6qVwGvAyaq6Qx0HgKeB0f44eN2oo9ZiyLSCJQJXfEwkaUkx1nLItNQ2YIyIJIhTW3sWsEFEegK4yy4E1vrj4NkF1pnMtJ6NPuolPcX6EpiWUdUvRGQhsAqoAr4E5gJviUhXQIDVwC/8cfzaKwIrGjKt4ddEICITgIdwmtQ9qar31lt/K3AtzgcoD/iZqv7gz5gak+FJYP2OfYE6vOmgVPVO4M56i89sj2PnFJaRHBtF5/jo9jicCVF+KxoSkUjgEeBcYBBwuYgMqrfZlzjtr4cCC4E/+Cue5nBmKiujxvoSmA4iu6CUjC7Wh8C0jj/rCEYDm1X1O1WtAF7CaV1RR1WXqmpt7eznQIYf42lSuieeiqoa9pTYvASmY8gpLLNiIdNq/kwE6UC21+Mcd1lDrgHe8rWiLdpaN0ftByrb6glMB6CqZBeWWosh02pB0WrIbXedBfzR1/rWtrVurtrx3K3lkOkICvZXUFpRbVcEptX8WVm8Hejt9TjDXXYIERkP/AY4zW1zHTB1M5VZXwLTAdg8BKat+POKYDnQX0T6ikgMzkBci703EJERwOPAJFXd7cdYmiUxNgpPQrQ1ITUdQnZtZ7IudkVgWsdviUBVq4AbgbeBDcACVV0nIneLyCR3sz8CScAr7lC9ixvYXbvJ8CTYeEOmQ6jtTGZTVJrW8ms/AlVdAiypt+wOr/vj/Xn8I5GeEs+m3cWBDsOYJuUUluJJiCYp1vqFmtYJisriYFLbl0DV+hKY4JZdWGb1A6ZNWCKoJ8MTT3llDfn7KwIdijGNyikotRZDpk1YIqgn3VM7CqnVE5jgVVOj5BSVWR8C0yYsEdRzcF4Ca0JqgldeyQEqqmrIsKIh0wYsEdSTbjOVmQ7ARh01bckSQT2d4qLpFBdlRUMmqNXNQ2BFQ6YNWCLwIcOTYEVDJqhlF9gVgWk7lgh8qG1Cakywyikso2tyLHHRkYEOxYQASwQ+pHucmcqsL4EJVs6oo3Y1YNqGJQIfMjwJlFZUU1haGehQjPEpu7DUOpOZNmOJwIcMazlkglhVdQ07isqtfsC0GUsEPthw1CaY7dxXTlWNWosh02YsEfhQ+wFbs31vgCMx5nB1TUetaMi0ERu20IdO8VGcemxXHvtwCxECvzr7OCIibHJwExysM5lpa3ZF4IOI8ORPs7h8dG8eWbqFGc+toLjcKo5NcMguLCNCoGdnSwSmbVgiaEBMVAS/v+h47po0mKXf5HHxY5+yLd/qDEzg5RSU0qNTHDFR9vE1bcPOpEaICFefnMnffzaaXfsOMOmRZXy6ZU+gwzJhLqewzAabM23KEkEzjO2XxuIbx5KWFMtVT/2H5z7bap3NTMA4ncksEZi2ExqJQBU+vBc++oPfDnFUaiKLrj+Z047tyv97Yx2/eX0tFVU1fjueMb5UVNWwc5/1ITBty6+JQEQmiMg3IrJZRGb7WH+qiKwSkSoRmdKqgxV8D0t/D99/3KrdNCY5LponfprFzNOPYf4X27jqqS/ILzngt+MZU19uURmq1nTUtC2/JQIRiQQeAc4FBgGXi8igepttA6YB81t5MPjxnyD1GHj1OijJa9XuGhMZIdw2YQAPTh3Ol9lFXPDIv9mwY5/fjmeMt2y36aiNM2Takj+vCEYDm1X1O1WtAF4CLvDeQFW3quoaoPVlLLFJcMkzUFYIi34ONf4ttrlwRDqv/PwkKqtruPixT/nX2p1+PZ4xcHAKVassNm3Jn4kgHcj2epzjLmsxEZkhIitEZEVeXiO/9nscDxP+D7a8D58+dCSHapFhvVNYfOMp9O+ezC+eX8lf3t9klcjGr7ILSomOFHp0igt0KCaEdIiexao6F5gLkJWV1fg3bdbPnHqC938LfU6GPif6NbbuneJ4ecYYbn/ta/707rds3FnMHy8ZSkJMh3hrTQeTXVhGr5R4IkOkp3tlZSU5OTmUl5cHOpSQERcXR0ZGBtHR0c1+jj+/rbYDvb0eZ7jL/EsEJj0MuV/Cq9fAzz+GhC5+PWRcdCQPXDqMAT2SufdfG9mav5+5P82qG7zOmLaSU1gaUi2GcnJySE5OJjMzE5HQSG6BpKrk5+eTk5ND3759m/08fxYNLQf6i0hfEYkBLgMW+/F4B8V1hkuehuKd8MaNTvNSPxMRfn7aMcy7+gS25ZdywV+XsfKHAr8f14SX7IKykOpDUF5eTmpqqiWBNiIipKamtvgKy2+JQFWrgBuBt4ENwAJVXScid4vIJAAROUFEcoBLgMdFZF2bBZA+Cs6+C775J3zxeJvttilnDOjGohtOJik2isvmfs6C5dlNP8mYZiirqGZPyYGQazpqSaBtHcn76dd+BKq6RFWPVdVjVPUed9kdqrrYvb9cVTNUNVFVU1V1cJsGMOZ6OHYCvPO/sH1Vm+66Mf26JfP6DWM5sW8q//PqGu5+cz1V1db5LFSJyC0isk5E1orIiyIS514Jf+H2oXnZvSpule1FNuqo8Y/Q6FncEBG48DFI6gYLfwbl7dfePyUhhmemn8C0kzOZ9+/vmf7Mcut8FoJEJB2YBWSp6hAgEqcY9D7gz6raDygErmntsWrnIcgIoaKhYFBUVMSjjz7a4uedd955FBUVNbrNHXfcwXvvvXeEkbWf0E4E4FQUX/wUFG2DN29ql/qCWlGREcyZNJj7Lj6eL74r4Jw/f8ySr3e02/FNu4kC4kUkCkgAdgBnAgvd9c8CF7b2IHWdybrYFUFbaigRVFVVNfq8JUuWkJKS0ug2d999N+PHj29NeO0iPNo4HnUSnPFr+OC3cPRpMGpaux5+6gl9GN7bw3+98hXXv7CKiUN7cvcFQ+iS2OrSAhNgqrpdRO7H6SVfBrwDrASK3HoyaKQPjYjMAGYA9OnTp9Fj5RSWERsVQdek2DaKPrjc9eY61ue27VX7oF6duPP8xkucZ8+ezZYtWxg+fDjR0dHExcXh8XjYuHEj3377LRdeeCHZ2dmUl5dz0003MWPGDAAyMzNZsWIFJSUlnHvuuZxyyil8+umnpKen88YbbxAfH8+0adOYOHEiU6ZMITMzk6uvvpo333yTyspKXnnlFQYMGEBeXh5XXHEFubm5nHTSSbz77rusXLmStLS0Nn0vGhP6VwS1TrkVjj4D3roNdrVdnXRzHdcjmdeuP5n/OudY3l63k3P+/JH1Rg4BIuLB6THfF+gFJAITmvt8VZ2rqlmqmtW1a9dGt80ucJqOWuVq27r33ns55phjWL16NX/84x9ZtWoVDz30EN9++y0A8+bNY+XKlaxYsYKHH36Y/Pz8w/axadMmbrjhBtatW0dKSgqvvvqqz2OlpaWxatUqZs6cyf333w/AXXfdxZlnnsm6deuYMmUK27Zt89+LbUB4XBEARETA5Lnw2Fh4ZRrM+BBiEts1hOjICG48sz9nDezOf73yFb94fiWThvXirkmD8djVQUc1HvheVfMAROQ1YCyQIiJR7lVBm/ShySksC7kWQ96a+uXeXkaPHn1IG/yHH36YRYsWAZCdnc2mTZtITU095Dl9+/Zl+PDhAIwaNYqtW7f63PfkyZPrtnnttdcAWLZsWd3+J0yYgMfjacuX0yzhc0UATqXxxU/Ank2w5L8DFsbAnp14/Yax3DL+WJZ8vYOz//wx76yzq4MOahswRkQSxPmpfhawHlgK1I6oezXwRmsPlB1incmCVWLiwR+IH374Ie+99x6fffYZX331FSNGjPDZRj829mBxXWRkZIP1C7XbNbZNIIRXIgA4+nQ49b9h9Qvw1UsBCyM6MoKbxvfnjRvH0jU5lhnPreTml76kqLQiYDGZllPVL3AqhVcBX+N8puYCtwG3ishmIBV4qjXHKS6vpKi0MqQ6kwWL5ORkiouLfa7bu3cvHo+HhIQENm7cyOeff97mxx87diwLFiwA4J133qGwsLDNj9GU8Cka8nbabfDDv+Eftzodz9L6ByyUwb0688YNY3lk6WYeWbqZf2/J5/8uOp7xg7oHLCbTMqp6J3BnvcXf4YzA2ybqRh21RNDmUlNTGTt2LEOGDCE+Pp7u3Q9+9iZMmMDf/vY3Bg4cyHHHHceYMWPa/Ph33nknl19+Oc899xwnnXQSPXr0IDk5uc2P0xjpaKNlZmVl6YoVK1q/o325Tn1Bp15w7XsQHfhL7rXb9/Jfr3zFxp3FTB6Zzp0TB9M5ofkDR5nWE5GVqpoViGM3dm6/s24nM55byeIbxzI0I6V9A/OjDRs2MHDgwECHEVAHDhwgMjKSqKgoPvvsM2bOnMnq1atbtU9f72tj53b4FQ3V6tQLLnocdq2Ft38T6GgAGJLemcU3nsKsM/vxxupcznnwI5Zu3B3osEwQyHavCKxoKPRs27aNE044gWHDhjFr1iyeeOKJdo8hPIuGah17Dpz8S/j0L9B3HAy+KNARERMVwa3nHMfZg3rwq1dWM/2Z5VwyKoP/nTiIzvF2dRCucgpLSYyJJMWuEENO//79+fLLLwMaQ/heEdQ68w5Iz4LFs5x5j4PE8RmdefOXp3DDGcfw2pfb+dGfP+bDb+zqIFxlFzhNR60PgfEHSwRRMTBlHiDOeERVwdNqJzYqkv/+0QBem3kyyXFRTHt6ObctXMO+8spAh2bamTMPgRULGf+wRADgOQou+CvkroL37wp0NIcZ1juFN395CjNPP4ZXVmYzwR2zaEteCfvKK216zBCnquQUllkfAuM34V1H4G3QJBg9Az77K2SeAsedG+iIDhEXHcltEwZwzqDudWMW1Ypxx5/pmhxLmvu3a3IsXZNi6u7XLrcpNDueotJKSg5UhXSvYhNY9q3g7ezfwrbP4PWZ8Itl0Dkj0BEdZkQfD/+cNY5V2wrJKz5w8Fbi/M0pLGV1diH5+yt8DrSaGBNJWnIsXZMOTRppSbGkuYmjdnlcdGT7v0BzmLpRR+2KICgkJSVRUlJCbm4us2bNYuHChYdtc/rpp3P//feTldVwS+QHH3yQGTNmkJDgJPjzzjuP+fPnNzmiqT9YIvAWHQdTnoG5p8HCa+DqxRAZ48xrEETioiM5+ZjGRyasqq6hoLSCvOID7CmpOCRp7HGTxua8Ej77Lp+9Zb7rHJJjo0hLPjRBpCX5ThyWNPzHOpMFp169evlMAs314IMPcuWVV9YlgiVLlrRVaC1miaC+tH4w8UF47Vr4XTdnmUSAREJEpNffCOdvRNThy2ofR0QdXBYZA0ndoXNv50qj7tYbEtPaPNlERUbQLTmObslxTW57oKqa/JKKugSxp8QreZQcYE/xAb7ZWcyy4j3sK/c9PkpybNTBBJEcQ/dOcfTqHE/PlDh6do6nV4oTS2REcCXVjiC7IEzmIXhrNuz8um332eN4OPfeRjeZPXs2vXv35oYbbgBgzpw5REVFsXTpUgoLC6msrOR3v/sdF1xwwSHP27p1KxMnTmTt2rWUlZUxffp0vvrqKwYMGEBZWVnddjNnzmT58uWUlZUxZcoU7rrrLh5++GFyc3M544wzSEtLY+nSpXXDWqelpfHAAw8wb948AK699lpuvvlmtm7d2uBw161licCXoZdAZLQzOF1NFWg11FR7/a1xlrdkWfUByNsIm96FqrJDjxcVB53SDyaG+omic7pfez7HRkXSKyWeXilNH6M5SWPjzmI++iaP/RXVhzw3MkLonhxLz5R4enaOo5f7tzZR9OwcT2piDBGWLA6RU1hGSkI0yXHWh8Afpk6dys0331yXCBYsWMDbb7/NrFmz6NSpE3v27GHMmDFMmjSpwea7jz32GAkJCWzYsIE1a9YwcuTIunX33HMPXbp0obq6mrPOOos1a9Ywa9YsHnjgAZYuXXrYvAMrV67k6aef5osvvkBVOfHEEznttNPweDxs2rSJF198kSeeeIJLL72UV199lSuvvLLV74ElgoYMvtA/+1WFskLYmw17c9yb1/0t70PxTqBeAX9CWr3kkAEpvZ37KX0gIbVdirCamzRUlX3lVezYW8aOonJy6/1du30v76zfRUXVoXM5x0RG0KNz3CGJokdn50qiRp391tSoc989To06j2tUUYWaGkWhbnn9bVCYdVZ/EmM7xukfNqOONvHL3V9GjBjB7t27yc3NJS8vD4/HQ48ePbjlllv4+OOPiYiIYPv27ezatYsePXr43MfHH3/MrFmzABg6dChDhw6tW7dgwQLmzp1LVVUVO3bsYP369Yesr2/ZsmVcdNFFdaOgTp48mU8++YRJkyY1e7jrlvLrJ0FEJgAP4czj+qSq3ltvfSzwd2AUkA9MVdWt/owp4ESc6TMTukDPYb63qaqA4lzfiSJ/M2xZCpX7D31OdIKbFHp7/e1z8HFyD6eIqp2ICJ3jo+kcH82AHp18bqOqFOyvYMfecnKLypy/bqLYsbeM/3xfwK595VTVtK55bIRAhAgRIiDO42vHHd1xEkFBKcd2b99ByMLNJZdcwsKFC9m5cydTp07lhRdeIC8vj5UrVxIdHU1mZqbP4aeb8v3333P//fezfPlyPB4P06ZNO6L91Ko/3LV3EVRr+O2TICKRwCPA2ThT9S0XkcWqut5rs2uAQlXtJyK1E35P9VdMHUZUDHgynZsv3lcVRdlef7c5f3O/hNJ6syhFRDnFTyl96iUMN1nEew4/RkMOW+drW3EmAzqkfsW9LxEQEYGIkJoUS2pSLEPSO/s8VHWNkyxUFRHx+aVe+1jq7lO3bUfviVvbh+CsgTYarT9NnTqV6667jj179vDRRx+xYMECunXrRnR0NEuXLuWHH35o9Pmnnnoq8+fP58wzz2Tt2rWsWbMGgH379pGYmEjnzp3ZtWsXb731FqeffjpwcPjr+kVD48aNY9q0acyePRtVZdGiRTz33HN+ed21/PmTaDSwWVW/AxCRl3Cm9PNOBBcAc9z7C4G/ioio9ZBqXHOuKir2eyWJbYcmje8+hOId+P4Cb0dSr4LdTRDeySNSIugqkV7FXk4CqLsP9daJj3U+tvvZv5z3L8jllRzgQFVNeBQNBdDgwYMpLi4mPT2dnj178pOf/ITzzz+f448/nqysLAYMGNDo82fOnMn06dMZOHAgAwcOZNSoUQAMGzaMESNGMGDAAHr37s3YsWPrnjNjxgwmTJhAr169WLp0ad3ykSNHMm3aNEaPdkYxv/baaxkxYkSbFQP54rdhqEVkCjBBVa91H18FnKiqN3pts9bdJsd9vMXdZk+9fXlP8D2qqexsmqGqAvblHEwO5fvq1THU+yV92C/rJtZrjVtZ7lae193XQyvTD7nf0HPcegRV6pJX3Xmrja/zuR0w6WGIO/wqJNiGoc4tKuP3SzYwfWwmo44K/sTVUjYMtX+0dBjqDlFIqqpzcWZ9Iisry64W2kJUDHQ52rmZoNUrJZ6/XjGy6Q2NaQV/jjW0Hejt9djXBN5124hIFNAZp9LYGGNMO/FnIlgO9BeRviISA1wGLK63zWKcib3Bmej7A6sfMCa82Ee+bR3J++m3RKCqVcCNwNvABmCBqq4TkbtFZJK72VNAqjvB963AbH/FY4wJPnFxceTn51syaCOqSn5+PnFxTY8o4M2vdQSqugRYUm/ZHV73y4FL/BmDMSZ4ZWRkkJOTQ15eXqBDCRlxcXFkZLRswMwOUVlsjAlN0dHR9O3bN9BhhD2bmMYYY8KcJQJjjAlzlgiMMSbM+a1nsb+ISB7QUNfiNGBPA+uCQTDHF8yxQfvFd5Sqdm2H4xymA5/bwRwbBHd87Rlbg+d2h0sEjRGRFYEaHqA5gjm+YI4Ngj8+fwvm1x/MsUFwxxcssVnRkDHGhDlLBMYYE+ZCLRHMDXQATQjm+II5Ngj++PwtmF9/MMcGwR1fUMQWUnUExhhjWi7UrgiMMca0kCUCY4wJcyGRCERkgoh8IyKbRSSoRjAVkd4islRE1ovIOhG5KdAx1ScikSLypYj8I9Cx1CciKSKyUEQ2isgGETkp0DG1Jzu3W8fO7WbG0tHrCEQkEvgWOBvIwZkH4XJVXd/oE9uJiPQEeqrqKhFJBlYCFwZLfAAiciuQBXRS1YmBjsebiDwLfKKqT7rzWiSoalGAw2oXdm63np3bzRMKVwSjgc2q+p2qVgAvARcEOKY6qrpDVVe594tx5mZID2xUB4lIBvBj4MlAx1KfiHQGTsWZtwJVrQiXJOCyc7sV7NxuvlBIBOlAttfjHILoZPQmIpnACOCLAIfi7UHgf4CaAMfhS18gD3javbx/UkQSAx1UO7Jzu3UexM7tZgmFRNAhiEgS8Cpws6ruC3Q8ACIyEditqisDHUsDooCRwGOqOgLYj81iF3Ts3D4iQXVuh0Ii2A709nqc4S4LGiISjfNBeUFVXwt0PF7GApNEZCtOscOZIvJ8YEM6RA6Qo6q1vzIX4nx4woWd20fOzu0WCIVEsBzoLyJ93QqXy4DFAY6pjogITjngBlV9INDxeFPV21U1Q1Uzcd63D1T1ygCHVUdVdwLZInKcu+gsIGgqItuBndtHyM7tlunwU1WqapWI3Ai8DUQC81R1XYDD8jYWuAr4WkRWu8t+7c7nbJr2S+AF94vwO2B6gONpN3Zuh7ygObc7fPNRY4wxrRMKRUPGGGNawRKBMcaEOUsExhgT5iwRGGNMmLNEYIwxYc4SgUFETg/G0RmNaQ07r5vPEoExxoQ5SwQdiIhcKSL/EZHVIvK4O9Z6iYj82R0P/n0R6epuO1xEPheRNSKySEQ87vJ+IvKeiHwlIqtE5Bh390leY6O/4PYaNcbv7LwOPEsEHYSIDASmAmNVdThQDfwESARWqOpg4CPgTvcpfwduU9WhwNdey18AHlHVYcDJwA53+QjgZmAQcDROr1Fj/MrO6+DQ4YeYCCNnAaOA5e6PmnhgN84Quy+72zwPvOaOdZ6iqh+5y58FXnEnD0lX1UUAqloO4O7vP6qa4z5eDWQCy/z+qky4s/M6CFgi6DgEeFZVbz9kocj/q7fdkY4ZcsDrfjV2bpj2Yed1ELCioY7jfWCKiHQDEJEuInIUzv9wirvNFcAyVd0LFIrIOHf5VcBH7ixSOSJyobuPWBFJaM8XYUw9dl4HAcuOHYSqrheR/wXeEZEIoBK4AWdCi9Huut045a0AVwN/cz8Q3iMbXgU8LiJ3u/u4pB1fhjGHsPM6ONjoox2ciJSoalKg4zCmLdl53b6saMgYY8KcXREYY0yYsysCY4wJc5YIjDEmzFkiMMaYMGeJwBhjwpwlAmOMCXP/H2Klf4ih7I/bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 9.478568723326118e-05, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 2.5688761956411534, test accuracy: 53.942604064941406\n",
      "Confusion matrix:\n",
      "[[222  24  16   6 143  20  36]\n",
      " [ 10  23   0   0  20   2   1]\n",
      " [ 78  14  83  13 223  58  27]\n",
      " [118   2   7 629  74  34  31]\n",
      " [ 58  14  30  18 447  21  65]\n",
      " [ 17   5  44  10  23 309   7]\n",
      " [ 40   9  31  40 254  10 223]]\n",
      "F1-score: [0.43960396 0.31292517 0.23479491 0.78088144 0.48666304 0.71116226\n",
      " 0.44734203]\n",
      "Batch 0 loss: 0.9606376886367798, accuracy: 0.5\n",
      "Batch 1 loss: 1.0244331359863281, accuracy: 0.578125\n",
      "Batch 2 loss: 1.4113876819610596, accuracy: 0.5833333134651184\n",
      "Batch 3 loss: 0.7803808450698853, accuracy: 0.609375\n",
      "Batch 4 loss: 1.5015952587127686, accuracy: 0.612500011920929\n",
      "Batch 5 loss: 0.8516018986701965, accuracy: 0.6458333134651184\n",
      "Batch 6 loss: 0.5354365110397339, accuracy: 0.6696428656578064\n",
      "Batch 7 loss: 0.6905971169471741, accuracy: 0.69140625\n",
      "Batch 8 loss: 0.5883004665374756, accuracy: 0.7048611044883728\n",
      "Batch 9 loss: 0.7468917965888977, accuracy: 0.7093750238418579\n",
      "Batch 10 loss: 0.5811499953269958, accuracy: 0.71875\n",
      "Batch 11 loss: 0.3663233816623688, accuracy: 0.7317708134651184\n",
      "Batch 12 loss: 0.42213985323905945, accuracy: 0.7403846383094788\n",
      "Batch 13 loss: 0.5085956454277039, accuracy: 0.7455357313156128\n",
      "Batch 14 loss: 0.369088351726532, accuracy: 0.7541666626930237\n",
      "Batch 15 loss: 0.31452158093452454, accuracy: 0.759765625\n",
      "Batch 16 loss: 0.33545687794685364, accuracy: 0.7647058963775635\n",
      "Batch 17 loss: 0.3135867714881897, accuracy: 0.7696428298950195\n",
      "Training epoch: 1, train accuracy: 76.96428680419922, train loss: 0.6834513809945848, valid accuracy: 98.57142639160156, valid loss: 0.05186437132457892 \n",
      "Batch 0 loss: 0.5144134163856506, accuracy: 0.8125\n",
      "Batch 1 loss: 0.1533544510602951, accuracy: 0.90625\n",
      "Batch 2 loss: 0.2129078209400177, accuracy: 0.90625\n",
      "Batch 3 loss: 0.10768089443445206, accuracy: 0.9296875\n",
      "Batch 4 loss: 0.14055153727531433, accuracy: 0.9312499761581421\n",
      "Batch 5 loss: 0.10813760757446289, accuracy: 0.9427083134651184\n",
      "Batch 6 loss: 0.1408233344554901, accuracy: 0.9464285969734192\n",
      "Batch 7 loss: 0.07380599528551102, accuracy: 0.953125\n",
      "Batch 8 loss: 0.09003167599439621, accuracy: 0.9583333134651184\n",
      "Batch 9 loss: 0.1732916533946991, accuracy: 0.9593750238418579\n",
      "Batch 10 loss: 0.1570608764886856, accuracy: 0.9602272510528564\n",
      "Batch 11 loss: 0.12266828864812851, accuracy: 0.9609375\n",
      "Batch 12 loss: 0.1452307552099228, accuracy: 0.9615384340286255\n",
      "Batch 13 loss: 0.15950825810432434, accuracy: 0.9598214030265808\n",
      "Batch 14 loss: 0.028971342369914055, accuracy: 0.9624999761581421\n",
      "Batch 15 loss: 0.07474426180124283, accuracy: 0.962890625\n",
      "Batch 16 loss: 0.027682214975357056, accuracy: 0.9650735259056091\n",
      "Batch 17 loss: 0.21402758359909058, accuracy: 0.9607142806053162\n",
      "Training epoch: 2, train accuracy: 96.07142639160156, train loss: 0.1469384426664975, valid accuracy: 98.57142639160156, valid loss: 0.022383310676862795 \n",
      "Batch 0 loss: 0.04049587994813919, accuracy: 1.0\n",
      "Batch 1 loss: 0.10350050032138824, accuracy: 0.96875\n",
      "Batch 2 loss: 0.08280866593122482, accuracy: 0.96875\n",
      "Batch 3 loss: 0.05970458313822746, accuracy: 0.96875\n",
      "Batch 4 loss: 0.06455077230930328, accuracy: 0.96875\n",
      "Batch 5 loss: 0.04920659959316254, accuracy: 0.9739583134651184\n",
      "Batch 6 loss: 0.1043735146522522, accuracy: 0.9732142686843872\n",
      "Batch 7 loss: 0.05495375022292137, accuracy: 0.97265625\n",
      "Batch 8 loss: 0.006112154573202133, accuracy: 0.9756944179534912\n",
      "Batch 9 loss: 0.049573831260204315, accuracy: 0.9750000238418579\n",
      "Batch 10 loss: 0.026896892115473747, accuracy: 0.9772727489471436\n",
      "Batch 11 loss: 0.14251825213432312, accuracy: 0.9739583134651184\n",
      "Batch 12 loss: 0.03691727668046951, accuracy: 0.9759615659713745\n",
      "Batch 13 loss: 0.056532829999923706, accuracy: 0.9732142686843872\n",
      "Batch 14 loss: 0.06916673481464386, accuracy: 0.9708333611488342\n",
      "Batch 15 loss: 0.026267053559422493, accuracy: 0.97265625\n",
      "Batch 16 loss: 0.014407205395400524, accuracy: 0.9742646813392639\n",
      "Batch 17 loss: 0.06598564982414246, accuracy: 0.9750000238418579\n",
      "Training epoch: 3, train accuracy: 97.5, train loss: 0.05855400813743472, valid accuracy: 98.57142639160156, valid loss: 0.036212172359228134 \n",
      "Batch 0 loss: 0.03138294443488121, accuracy: 1.0\n",
      "Batch 1 loss: 0.04457489773631096, accuracy: 0.984375\n",
      "Batch 2 loss: 0.01234239712357521, accuracy: 0.9895833134651184\n",
      "Batch 3 loss: 0.018804753199219704, accuracy: 0.9921875\n",
      "Batch 4 loss: 0.034143902361392975, accuracy: 0.9937499761581421\n",
      "Batch 5 loss: 0.049895986914634705, accuracy: 0.9895833134651184\n",
      "Batch 6 loss: 0.05016380548477173, accuracy: 0.9866071343421936\n",
      "Batch 7 loss: 0.02174275927245617, accuracy: 0.98828125\n",
      "Batch 8 loss: 0.0329497829079628, accuracy: 0.9895833134651184\n",
      "Batch 9 loss: 0.03590819984674454, accuracy: 0.987500011920929\n",
      "Batch 10 loss: 0.03488298878073692, accuracy: 0.9886363744735718\n",
      "Batch 11 loss: 0.006574789993464947, accuracy: 0.9895833134651184\n",
      "Batch 12 loss: 0.07004974037408829, accuracy: 0.9879807829856873\n",
      "Batch 13 loss: 0.03287028521299362, accuracy: 0.9888392686843872\n",
      "Batch 14 loss: 0.004490496590733528, accuracy: 0.9895833134651184\n",
      "Batch 15 loss: 0.022184958681464195, accuracy: 0.990234375\n",
      "Batch 16 loss: 0.008137437514960766, accuracy: 0.9908088445663452\n",
      "Batch 17 loss: 0.019511951133608818, accuracy: 0.9910714030265808\n",
      "Training epoch: 4, train accuracy: 99.10713958740234, train loss: 0.029478448753555615, valid accuracy: 100.0, valid loss: 0.004194950975943357 \n",
      "Batch 0 loss: 0.01865294575691223, accuracy: 1.0\n",
      "Batch 1 loss: 0.0041828700341284275, accuracy: 1.0\n",
      "Batch 2 loss: 0.0013362705940380692, accuracy: 1.0\n",
      "Batch 3 loss: 0.018548892810940742, accuracy: 1.0\n",
      "Batch 4 loss: 0.002809995086863637, accuracy: 1.0\n",
      "Batch 5 loss: 0.04275831952691078, accuracy: 1.0\n",
      "Batch 6 loss: 0.008346738293766975, accuracy: 1.0\n",
      "Batch 7 loss: 0.00591543922200799, accuracy: 1.0\n",
      "Batch 8 loss: 0.003367641009390354, accuracy: 1.0\n",
      "Batch 9 loss: 0.004863898269832134, accuracy: 1.0\n",
      "Batch 10 loss: 0.006503099109977484, accuracy: 1.0\n",
      "Batch 11 loss: 0.00408758781850338, accuracy: 1.0\n",
      "Batch 12 loss: 0.041555698961019516, accuracy: 1.0\n",
      "Batch 13 loss: 0.030077502131462097, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.04106650501489639, accuracy: 0.9958333373069763\n",
      "Batch 15 loss: 0.004519019275903702, accuracy: 0.99609375\n",
      "Batch 16 loss: 0.012451456859707832, accuracy: 0.9963235259056091\n",
      "Batch 17 loss: 0.02451007068157196, accuracy: 0.9964285492897034\n",
      "Training epoch: 5, train accuracy: 99.64286041259766, train loss: 0.015308552803212984, valid accuracy: 100.0, valid loss: 0.003836939256871119 \n",
      "Batch 0 loss: 0.006393172778189182, accuracy: 1.0\n",
      "Batch 1 loss: 0.007525158580392599, accuracy: 1.0\n",
      "Batch 2 loss: 0.0048266383819282055, accuracy: 1.0\n",
      "Batch 3 loss: 0.0025886367075145245, accuracy: 1.0\n",
      "Batch 4 loss: 0.0022462629713118076, accuracy: 1.0\n",
      "Batch 5 loss: 0.06105998903512955, accuracy: 0.9947916865348816\n",
      "Batch 6 loss: 0.009083284065127373, accuracy: 0.9955357313156128\n",
      "Batch 7 loss: 0.0024815623182803392, accuracy: 0.99609375\n",
      "Batch 8 loss: 0.01161144208163023, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.004098633769899607, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.00915472861379385, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.007605833001434803, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.016244232654571533, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.00825459510087967, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.0050812335684895515, accuracy: 0.9979166388511658\n",
      "Batch 15 loss: 0.014005716890096664, accuracy: 0.998046875\n",
      "Batch 16 loss: 0.004479496739804745, accuracy: 0.998161792755127\n",
      "Batch 17 loss: 0.005914163310080767, accuracy: 0.9982143044471741\n",
      "Training epoch: 6, train accuracy: 99.82142639160156, train loss: 0.010147487809364166, valid accuracy: 100.0, valid loss: 0.0022016972931548176 \n",
      "Batch 0 loss: 0.0035449471324682236, accuracy: 1.0\n",
      "Batch 1 loss: 0.002784686628729105, accuracy: 1.0\n",
      "Batch 2 loss: 0.07340428978204727, accuracy: 0.9791666865348816\n",
      "Batch 3 loss: 0.010237196460366249, accuracy: 0.984375\n",
      "Batch 4 loss: 0.00743599608540535, accuracy: 0.987500011920929\n",
      "Batch 5 loss: 0.0012928571086376905, accuracy: 0.9895833134651184\n",
      "Batch 6 loss: 0.008413917385041714, accuracy: 0.9910714030265808\n",
      "Batch 7 loss: 0.000831221230328083, accuracy: 0.9921875\n",
      "Batch 8 loss: 0.0016561371739953756, accuracy: 0.9930555820465088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 loss: 0.11465355008840561, accuracy: 0.9906250238418579\n",
      "Batch 10 loss: 0.0018959484295919538, accuracy: 0.9914772510528564\n",
      "Batch 11 loss: 0.0011821419466286898, accuracy: 0.9921875\n",
      "Batch 12 loss: 0.011315418407320976, accuracy: 0.9927884340286255\n",
      "Batch 13 loss: 0.003150918986648321, accuracy: 0.9933035969734192\n",
      "Batch 14 loss: 0.00238386495038867, accuracy: 0.9937499761581421\n",
      "Batch 15 loss: 0.005989123601466417, accuracy: 0.994140625\n",
      "Batch 16 loss: 0.011895633302628994, accuracy: 0.9944853186607361\n",
      "Batch 17 loss: 0.0011546931928023696, accuracy: 0.9946428537368774\n",
      "Training epoch: 7, train accuracy: 99.46428680419922, train loss: 0.014623474549605615, valid accuracy: 100.0, valid loss: 0.0021893372177146375 \n",
      "Batch 0 loss: 0.03199249878525734, accuracy: 1.0\n",
      "Batch 1 loss: 0.028534604236483574, accuracy: 1.0\n",
      "Batch 2 loss: 0.03758024424314499, accuracy: 1.0\n",
      "Batch 3 loss: 0.003984218463301659, accuracy: 1.0\n",
      "Batch 4 loss: 0.010119151324033737, accuracy: 1.0\n",
      "Batch 5 loss: 0.001110216835513711, accuracy: 1.0\n",
      "Batch 6 loss: 0.0012913745595142245, accuracy: 1.0\n",
      "Batch 7 loss: 0.0095451008528471, accuracy: 1.0\n",
      "Batch 8 loss: 0.003564372891560197, accuracy: 1.0\n",
      "Batch 9 loss: 0.0036206990480422974, accuracy: 1.0\n",
      "Batch 10 loss: 0.004813524894416332, accuracy: 1.0\n",
      "Batch 11 loss: 0.0010427669622004032, accuracy: 1.0\n",
      "Batch 12 loss: 0.0034647907596081495, accuracy: 1.0\n",
      "Batch 13 loss: 0.0009286951390095055, accuracy: 1.0\n",
      "Batch 14 loss: 0.0021383175626397133, accuracy: 1.0\n",
      "Batch 15 loss: 0.0068659125827252865, accuracy: 1.0\n",
      "Batch 16 loss: 0.002713023452088237, accuracy: 1.0\n",
      "Batch 17 loss: 0.012541063129901886, accuracy: 1.0\n",
      "Training epoch: 8, train accuracy: 100.0, train loss: 0.009213920873460464, valid accuracy: 100.0, valid loss: 0.0007018100280523262 \n",
      "Batch 0 loss: 0.001109548145905137, accuracy: 1.0\n",
      "Batch 1 loss: 0.003317882539704442, accuracy: 1.0\n",
      "Batch 2 loss: 0.0007594448979943991, accuracy: 1.0\n",
      "Batch 3 loss: 0.0020843299571424723, accuracy: 1.0\n",
      "Batch 4 loss: 0.012417781166732311, accuracy: 1.0\n",
      "Batch 5 loss: 0.0013951613800600171, accuracy: 1.0\n",
      "Batch 6 loss: 0.008562803268432617, accuracy: 1.0\n",
      "Batch 7 loss: 0.004070554859936237, accuracy: 1.0\n",
      "Batch 8 loss: 0.001344919903203845, accuracy: 1.0\n",
      "Batch 9 loss: 0.0007258722907863557, accuracy: 1.0\n",
      "Batch 10 loss: 0.015356571413576603, accuracy: 1.0\n",
      "Batch 11 loss: 0.0027726050466299057, accuracy: 1.0\n",
      "Batch 12 loss: 0.006998589262366295, accuracy: 1.0\n",
      "Batch 13 loss: 0.04029407724738121, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.00186056992970407, accuracy: 0.9979166388511658\n",
      "Batch 15 loss: 0.0018957629799842834, accuracy: 0.998046875\n",
      "Batch 16 loss: 0.0015189074911177158, accuracy: 0.998161792755127\n",
      "Batch 17 loss: 0.00015106782666407526, accuracy: 0.9982143044471741\n",
      "Training epoch: 9, train accuracy: 99.82142639160156, train loss: 0.005924247200406778, valid accuracy: 100.0, valid loss: 0.00020535409551788084 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 9.667919408918075e-05, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 2.620966122710391, test accuracy: 55.558650970458984\n",
      "Confusion matrix:\n",
      "[[215  16  14  17 142  24  39]\n",
      " [ 13  17   0   2  21   2   1]\n",
      " [ 78  12  58  28 223  68  29]\n",
      " [ 79   1   2 705  52  34  22]\n",
      " [ 52  15  16  37 445  23  65]\n",
      " [ 15   5  23  17  23 323   9]\n",
      " [ 33   9  20  77 221  16 231]]\n",
      "F1-score: [0.45168067 0.25954198 0.18441971 0.79302587 0.5        0.71381215\n",
      " 0.46061815]\n",
      "Batch 0 loss: 1.8590325117111206, accuracy: 0.46875\n",
      "Batch 1 loss: 1.1890735626220703, accuracy: 0.59375\n",
      "Batch 2 loss: 1.295372486114502, accuracy: 0.59375\n",
      "Batch 3 loss: 1.310857892036438, accuracy: 0.546875\n",
      "Batch 4 loss: 1.0053904056549072, accuracy: 0.5874999761581421\n",
      "Batch 5 loss: 0.7813305854797363, accuracy: 0.625\n",
      "Batch 6 loss: 0.9468461871147156, accuracy: 0.625\n",
      "Batch 7 loss: 0.6972969770431519, accuracy: 0.64453125\n",
      "Batch 8 loss: 0.617617130279541, accuracy: 0.65625\n",
      "Batch 9 loss: 0.561449408531189, accuracy: 0.6625000238418579\n",
      "Batch 10 loss: 0.3696635663509369, accuracy: 0.6818181872367859\n",
      "Batch 11 loss: 0.23155219852924347, accuracy: 0.7083333134651184\n",
      "Batch 12 loss: 0.4216805398464203, accuracy: 0.71875\n",
      "Batch 13 loss: 0.19306908547878265, accuracy: 0.7366071343421936\n",
      "Batch 14 loss: 0.19309169054031372, accuracy: 0.7520833611488342\n",
      "Batch 15 loss: 0.19330665469169617, accuracy: 0.763671875\n",
      "Batch 16 loss: 0.29351702332496643, accuracy: 0.7702205777168274\n",
      "Batch 17 loss: 0.24869388341903687, accuracy: 0.7749999761581421\n",
      "Training epoch: 1, train accuracy: 77.5, train loss: 0.6893800993760427, valid accuracy: 95.71428680419922, valid loss: 0.14863631625970206 \n",
      "Batch 0 loss: 0.17580214142799377, accuracy: 0.9375\n",
      "Batch 1 loss: 0.16324761509895325, accuracy: 0.953125\n",
      "Batch 2 loss: 0.14456291496753693, accuracy: 0.9479166865348816\n",
      "Batch 3 loss: 0.1924198865890503, accuracy: 0.953125\n",
      "Batch 4 loss: 0.29028236865997314, accuracy: 0.9375\n",
      "Batch 5 loss: 0.15768012404441833, accuracy: 0.9375\n",
      "Batch 6 loss: 0.056194424629211426, accuracy: 0.9464285969734192\n",
      "Batch 7 loss: 0.17232118546962738, accuracy: 0.94921875\n",
      "Batch 8 loss: 0.11635741591453552, accuracy: 0.9479166865348816\n",
      "Batch 9 loss: 0.10693413019180298, accuracy: 0.949999988079071\n",
      "Batch 10 loss: 0.08613603562116623, accuracy: 0.9545454382896423\n",
      "Batch 11 loss: 0.1893911212682724, accuracy: 0.953125\n",
      "Batch 12 loss: 0.12741807103157043, accuracy: 0.9543269276618958\n",
      "Batch 13 loss: 0.03524482250213623, accuracy: 0.9575892686843872\n",
      "Batch 14 loss: 0.07396616786718369, accuracy: 0.9583333134651184\n",
      "Batch 15 loss: 0.08731026202440262, accuracy: 0.958984375\n",
      "Batch 16 loss: 0.03349101543426514, accuracy: 0.9613970518112183\n",
      "Batch 17 loss: 0.07303343713283539, accuracy: 0.9624999761581421\n",
      "Training epoch: 2, train accuracy: 96.25, train loss: 0.1267662855486075, valid accuracy: 100.0, valid loss: 0.020401388096312683 \n",
      "Batch 0 loss: 0.03538033366203308, accuracy: 1.0\n",
      "Batch 1 loss: 0.031788259744644165, accuracy: 1.0\n",
      "Batch 2 loss: 0.07511565089225769, accuracy: 0.9791666865348816\n",
      "Batch 3 loss: 0.19958406686782837, accuracy: 0.96875\n",
      "Batch 4 loss: 0.08997397869825363, accuracy: 0.96875\n",
      "Batch 5 loss: 0.08411165326833725, accuracy: 0.9739583134651184\n",
      "Batch 6 loss: 0.02985534816980362, accuracy: 0.9776785969734192\n",
      "Batch 7 loss: 0.15707238018512726, accuracy: 0.97265625\n",
      "Batch 8 loss: 0.052222780883312225, accuracy: 0.9722222089767456\n",
      "Batch 9 loss: 0.027497049421072006, accuracy: 0.9750000238418579\n",
      "Batch 10 loss: 0.008918427862226963, accuracy: 0.9772727489471436\n",
      "Batch 11 loss: 0.05511213093996048, accuracy: 0.9791666865348816\n",
      "Batch 12 loss: 0.05272504314780235, accuracy: 0.9783653616905212\n",
      "Batch 13 loss: 0.013415446504950523, accuracy: 0.9799107313156128\n",
      "Batch 14 loss: 0.05291343480348587, accuracy: 0.981249988079071\n",
      "Batch 15 loss: 0.04065612331032753, accuracy: 0.98046875\n",
      "Batch 16 loss: 0.036505136638879776, accuracy: 0.9816176295280457\n",
      "Batch 17 loss: 0.007428458891808987, accuracy: 0.9821428656578064\n",
      "Training epoch: 3, train accuracy: 98.21428680419922, train loss: 0.05834865021622843, valid accuracy: 95.71428680419922, valid loss: 0.05460327221468712 \n",
      "Batch 0 loss: 0.007705662399530411, accuracy: 1.0\n",
      "Batch 1 loss: 0.03982434794306755, accuracy: 0.984375\n",
      "Batch 2 loss: 0.08732368797063828, accuracy: 0.9791666865348816\n",
      "Batch 3 loss: 0.014850078150629997, accuracy: 0.984375\n",
      "Batch 4 loss: 0.026516608893871307, accuracy: 0.987500011920929\n",
      "Batch 5 loss: 0.04076264053583145, accuracy: 0.984375\n",
      "Batch 6 loss: 0.0550769567489624, accuracy: 0.9821428656578064\n",
      "Batch 7 loss: 0.06389825791120529, accuracy: 0.98046875\n",
      "Batch 8 loss: 0.05737876892089844, accuracy: 0.9791666865348816\n",
      "Batch 9 loss: 0.054525185376405716, accuracy: 0.9781249761581421\n",
      "Batch 10 loss: 0.02235601842403412, accuracy: 0.9801136255264282\n",
      "Batch 11 loss: 0.004518747795373201, accuracy: 0.9817708134651184\n",
      "Batch 12 loss: 0.008160903118550777, accuracy: 0.9831730723381042\n",
      "Batch 13 loss: 0.015309905633330345, accuracy: 0.984375\n",
      "Batch 14 loss: 0.028352277353405952, accuracy: 0.9854166507720947\n",
      "Batch 15 loss: 0.01835303194820881, accuracy: 0.986328125\n",
      "Batch 16 loss: 0.09546126425266266, accuracy: 0.9834558963775635\n",
      "Batch 17 loss: 0.041267868131399155, accuracy: 0.9839285612106323\n",
      "Training epoch: 4, train accuracy: 98.39286041259766, train loss: 0.03786901175044477, valid accuracy: 100.0, valid loss: 0.010452712381569048 \n",
      "Batch 0 loss: 0.017603637650609016, accuracy: 1.0\n",
      "Batch 1 loss: 0.03897174820303917, accuracy: 1.0\n",
      "Batch 2 loss: 0.006942653562873602, accuracy: 1.0\n",
      "Batch 3 loss: 0.010371385142207146, accuracy: 1.0\n",
      "Batch 4 loss: 0.023223521187901497, accuracy: 1.0\n",
      "Batch 5 loss: 0.006547950673848391, accuracy: 1.0\n",
      "Batch 6 loss: 0.0031806507613509893, accuracy: 1.0\n",
      "Batch 7 loss: 0.007935190573334694, accuracy: 1.0\n",
      "Batch 8 loss: 0.0329325869679451, accuracy: 0.9965277910232544\n",
      "Batch 9 loss: 0.0062540797516703606, accuracy: 0.996874988079071\n",
      "Batch 10 loss: 0.005458538420498371, accuracy: 0.9971590638160706\n",
      "Batch 11 loss: 0.0013137654168531299, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.0022135963663458824, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.01413080096244812, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.00716412253677845, accuracy: 0.9979166388511658\n",
      "Batch 15 loss: 0.009815070778131485, accuracy: 0.998046875\n",
      "Batch 16 loss: 0.016986994072794914, accuracy: 0.998161792755127\n",
      "Batch 17 loss: 0.0026585538871586323, accuracy: 0.9982143044471741\n",
      "Training epoch: 5, train accuracy: 99.82142639160156, train loss: 0.011872491495321609, valid accuracy: 100.0, valid loss: 0.002685638920714458 \n",
      "Batch 0 loss: 0.011285601183772087, accuracy: 1.0\n",
      "Batch 1 loss: 0.007466008886694908, accuracy: 1.0\n",
      "Batch 2 loss: 0.0022697565145790577, accuracy: 1.0\n",
      "Batch 3 loss: 0.004007647279649973, accuracy: 1.0\n",
      "Batch 4 loss: 0.002934382064267993, accuracy: 1.0\n",
      "Batch 5 loss: 0.00541719188913703, accuracy: 1.0\n",
      "Batch 6 loss: 0.0028313561342656612, accuracy: 1.0\n",
      "Batch 7 loss: 0.0075754825957119465, accuracy: 1.0\n",
      "Batch 8 loss: 0.0018735183402895927, accuracy: 1.0\n",
      "Batch 9 loss: 0.01109764538705349, accuracy: 1.0\n",
      "Batch 10 loss: 0.018394023180007935, accuracy: 1.0\n",
      "Batch 11 loss: 0.041008010506629944, accuracy: 0.9973958134651184\n",
      "Batch 12 loss: 0.014694211073219776, accuracy: 0.9975961446762085\n",
      "Batch 13 loss: 0.005856761243194342, accuracy: 0.9977678656578064\n",
      "Batch 14 loss: 0.0016015339642763138, accuracy: 0.9979166388511658\n",
      "Batch 15 loss: 0.0023022450041025877, accuracy: 0.998046875\n",
      "Batch 16 loss: 0.0063018049113452435, accuracy: 0.998161792755127\n",
      "Batch 17 loss: 0.12005060911178589, accuracy: 0.9964285492897034\n",
      "Training epoch: 6, train accuracy: 99.64286041259766, train loss: 0.014831543848332431, valid accuracy: 100.0, valid loss: 0.004194556355893535 \n",
      "Batch 0 loss: 0.010846303775906563, accuracy: 1.0\n",
      "Batch 1 loss: 0.0217237900942564, accuracy: 1.0\n",
      "Batch 2 loss: 0.005753001663833857, accuracy: 1.0\n",
      "Batch 3 loss: 0.00517963757738471, accuracy: 1.0\n",
      "Batch 4 loss: 0.012834670953452587, accuracy: 1.0\n",
      "Batch 5 loss: 0.017311248928308487, accuracy: 1.0\n",
      "Batch 6 loss: 0.0028736067470163107, accuracy: 1.0\n",
      "Batch 7 loss: 0.002622057683765888, accuracy: 1.0\n",
      "Batch 8 loss: 0.02232634834945202, accuracy: 1.0\n",
      "Batch 9 loss: 0.023464065045118332, accuracy: 1.0\n",
      "Batch 10 loss: 0.009398733265697956, accuracy: 1.0\n",
      "Batch 11 loss: 0.010402470827102661, accuracy: 1.0\n",
      "Batch 12 loss: 0.003893244778737426, accuracy: 1.0\n",
      "Batch 13 loss: 0.005759125109761953, accuracy: 1.0\n",
      "Batch 14 loss: 0.005677001550793648, accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15 loss: 0.00248223845846951, accuracy: 1.0\n",
      "Batch 16 loss: 0.002511201659217477, accuracy: 1.0\n",
      "Batch 17 loss: 0.015146130695939064, accuracy: 1.0\n",
      "Training epoch: 7, train accuracy: 100.0, train loss: 0.010011382064678602, valid accuracy: 100.0, valid loss: 0.00102950776151071 \n",
      "Batch 0 loss: 0.0019025658257305622, accuracy: 1.0\n",
      "Batch 1 loss: 0.0026026898995041847, accuracy: 1.0\n",
      "Batch 2 loss: 0.0008711335249245167, accuracy: 1.0\n",
      "Batch 3 loss: 0.0035186712630093098, accuracy: 1.0\n",
      "Batch 4 loss: 0.0019895657896995544, accuracy: 1.0\n",
      "Batch 5 loss: 0.006513426546007395, accuracy: 1.0\n",
      "Batch 6 loss: 0.007918165065348148, accuracy: 1.0\n",
      "Batch 7 loss: 0.010428161360323429, accuracy: 1.0\n",
      "Batch 8 loss: 0.017950616776943207, accuracy: 1.0\n",
      "Batch 9 loss: 0.00260399398393929, accuracy: 1.0\n",
      "Batch 10 loss: 0.0022288025356829166, accuracy: 1.0\n",
      "Batch 11 loss: 0.001289132866077125, accuracy: 1.0\n",
      "Batch 12 loss: 0.15728545188903809, accuracy: 0.9927884340286255\n",
      "Batch 13 loss: 0.0034800937864929438, accuracy: 0.9933035969734192\n",
      "Batch 14 loss: 0.008595529943704605, accuracy: 0.9937499761581421\n",
      "Batch 15 loss: 0.0012301448732614517, accuracy: 0.994140625\n",
      "Batch 16 loss: 0.0009208760457113385, accuracy: 0.9944853186607361\n",
      "Batch 17 loss: 0.007628889754414558, accuracy: 0.9946428537368774\n",
      "Training epoch: 8, train accuracy: 99.46428680419922, train loss: 0.013275439540545145, valid accuracy: 100.0, valid loss: 0.002317090518772602 \n",
      "Batch 0 loss: 0.0014000211376696825, accuracy: 1.0\n",
      "Batch 1 loss: 0.0026207726914435625, accuracy: 1.0\n",
      "Batch 2 loss: 0.0038333465345203876, accuracy: 1.0\n",
      "Batch 3 loss: 0.01406722143292427, accuracy: 1.0\n",
      "Batch 4 loss: 0.0010273718507960439, accuracy: 1.0\n",
      "Batch 5 loss: 0.0012393412180244923, accuracy: 1.0\n",
      "Batch 6 loss: 0.00042554072570055723, accuracy: 1.0\n",
      "Batch 7 loss: 0.005214645527303219, accuracy: 1.0\n",
      "Batch 8 loss: 0.015202167443931103, accuracy: 1.0\n",
      "Batch 9 loss: 0.007116387598216534, accuracy: 1.0\n",
      "Batch 10 loss: 0.000749661005102098, accuracy: 1.0\n",
      "Batch 11 loss: 0.0024691508151590824, accuracy: 1.0\n",
      "Batch 12 loss: 0.0059716650284826756, accuracy: 1.0\n",
      "Batch 13 loss: 0.006821453105658293, accuracy: 1.0\n",
      "Batch 14 loss: 0.00041020853677764535, accuracy: 1.0\n",
      "Batch 15 loss: 0.0024292627349495888, accuracy: 1.0\n",
      "Batch 16 loss: 0.0018908195197582245, accuracy: 1.0\n",
      "Batch 17 loss: 0.004238918889313936, accuracy: 1.0\n",
      "Training epoch: 9, train accuracy: 100.0, train loss: 0.004284886433096189, valid accuracy: 100.0, valid loss: 0.0033025064428026476 \n",
      "Batch 0 loss: 0.00044375110883265734, accuracy: 1.0\n",
      "Batch 1 loss: 0.004974290728569031, accuracy: 1.0\n",
      "Batch 2 loss: 0.00434311805292964, accuracy: 1.0\n",
      "Batch 3 loss: 0.0025406167842447758, accuracy: 1.0\n",
      "Batch 4 loss: 0.0006788845639675856, accuracy: 1.0\n",
      "Batch 5 loss: 0.0007656067027710378, accuracy: 1.0\n",
      "Batch 6 loss: 0.0019168010912835598, accuracy: 1.0\n",
      "Batch 7 loss: 0.0012080378364771605, accuracy: 1.0\n",
      "Batch 8 loss: 0.0014302496565505862, accuracy: 1.0\n",
      "Batch 9 loss: 0.0005933907814323902, accuracy: 1.0\n",
      "Batch 10 loss: 0.0017446945421397686, accuracy: 1.0\n",
      "Batch 11 loss: 0.0008910796605050564, accuracy: 1.0\n",
      "Batch 12 loss: 0.0037491696421056986, accuracy: 1.0\n",
      "Batch 13 loss: 0.0005662977928295732, accuracy: 1.0\n",
      "Batch 14 loss: 0.00061831995844841, accuracy: 1.0\n",
      "Batch 15 loss: 0.0002887304872274399, accuracy: 1.0\n",
      "Batch 16 loss: 0.0011383029632270336, accuracy: 1.0\n",
      "Batch 17 loss: 0.0021016858518123627, accuracy: 1.0\n",
      "Training epoch: 10, train accuracy: 100.0, train loss: 0.001666279344741876, valid accuracy: 100.0, valid loss: 0.0003893939679831722 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific training complete!\n",
      "test loss: 9.730319224153001e-05, test accuracy: 100.0\n",
      "Confusion matrix:\n",
      "[[10  0  0  0  0  0  0]\n",
      " [ 0 10  0  0  0  0  0]\n",
      " [ 0  0 10  0  0  0  0]\n",
      " [ 0  0  0 10  0  0  0]\n",
      " [ 0  0  0  0 10  0  0]\n",
      " [ 0  0  0  0  0 10  0]\n",
      " [ 0  0  0  0  0  0 10]]\n",
      "F1-score: [1. 1. 1. 1. 1. 1. 1.]\n",
      "test loss: 2.7482731675073344, test accuracy: 54.109779357910156\n",
      "Confusion matrix:\n",
      "[[184  16  19  16 158  38  36]\n",
      " [ 10  15   0   1  27   2   1]\n",
      " [ 62  14  73  12 224  80  31]\n",
      " [ 82   2   7 664  65  46  29]\n",
      " [ 41  10  26  23 443  29  81]\n",
      " [ 14   2  44   5  23 319   8]\n",
      " [ 20   5  34  49 236  19 244]]\n",
      "F1-score: [0.41818182 0.25       0.20886981 0.7975976  0.48441771 0.67299578\n",
      " 0.47058824]\n"
     ]
    }
   ],
   "source": [
    "pex1 = Pipeline()\n",
    "pex1.ingest_fer13_data(\"data/icml_face_data.csv\")\n",
    "pex1.ingest_specific_data(\"data/specific_dataset\", train_ratio=0.8)\n",
    "\n",
    "specific_losses = []\n",
    "specific_accuracies = []\n",
    "fer13_losses = []\n",
    "fer13_accuracies = []\n",
    "\n",
    "epochs_list = np.arange(11)\n",
    "\n",
    "\n",
    "\n",
    "for epochs in epochs_list:\n",
    "    # load the previous model\n",
    "    pex1.load_model(\"data/general_model.pt\", mode=\"train\")\n",
    "    \n",
    "    pex1.train_specific_model(\"data/temp.pt\", \n",
    "                           learning_rate=1e-3, \n",
    "                           n_epochs=epochs, \n",
    "                           stop_thr=1e-5, \n",
    "                           use_valid=True, \n",
    "                           batch_size=32)\n",
    "    \n",
    "    sp_loss, sp_acc, _, _ = pex1.evaluate_specific_model()\n",
    "    fer_loss, fer_acc, _, _ = pex1.evaluate_general_model()\n",
    "    \n",
    "    specific_accuracies.append(sp_acc)\n",
    "    specific_losses.append(sp_loss)\n",
    "    fer13_accuracies.append(fer_acc)\n",
    "    fer13_losses.append(fer_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now plot the training epochs vs specific and FER13 losses and accuracies\n",
    "plt.clf()\n",
    "\n",
    "plt.title(\"Test Loss vs. Epochs Trained \\n While Training on Specific Dataset\")\n",
    "plt.xlabel(\"Epochs Trained\")\n",
    "plt.ylabel(\"Test Loss\")\n",
    "plt.plot(epochs_list, fer13_losses, label=\"FER13 test loss\")\n",
    "plt.plot(epochs_list, specific_losses, label=\"Specific Dataset test loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.title(\"Test Accuracy vs. Epochs Trained \\n While Training on Specific Dataset\")\n",
    "plt.xlabel(\"Epochs Trained\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.plot(epochs_list, fer13_accuracies, label=\"FER13 test accuracy\")\n",
    "plt.plot(epochs_list, specific_accuracies, label=\"Specific Dataset test accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_venv",
   "language": "python",
   "name": "final_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
